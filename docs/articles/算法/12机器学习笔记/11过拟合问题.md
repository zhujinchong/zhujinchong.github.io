过拟合：在训练数据上表现很好，在新数据上表现很差

原因：

1. 样本少，或者样本里面错误较多
2. 模型复杂度过高 
3. 神经网络迭代次数过度

解决方法总结：

1. 增加样本
2. 减小模型复杂度

解决方法：

1. 增加更多样本，样本增强处理
2. 网络结构，从简单开始
3. 正则化，添加惩罚函数。
4. 学习率衰减
5. 提前终止迭代early stopping
6. batch normalization / layer normalizetion
7. dropout
8. 模型融合
9. 增加噪声

### 增加噪声

**输入添加噪声**

> 在输入中加高斯噪声，会在输出中生成 的干扰项。训练时，减小误差，同时也会对噪声产生的干扰项进行惩罚，达到减小权值的平方的目的，达到与 L2 regularization 类似的效果（对比公式）。 ——Hinton

**权值初始化、权值添加噪声**

在初始化网络的时候，用0均值的高斯分布作为初始化。 

**对网络的响应加噪声**

如在前向传播过程中，让默写神经元的输出变为 binary 或 random。显然，这种有点乱来的做法会打乱网络的训练过程，让训练更慢，但据 Hinton 说，在测试集上效果会有显著提升 （But it doessignificantly better on the test set!）。

### batch normalization

问题：数据不止需要在输入标准化，可能出现内部方差漂移。

BN带来许多好处：

1. 不仅仅极大提升了训练速度，收敛过程大大加快；
2. 还能增加分类效果，一种解释是这是类似于Dropout的一种防止过拟合的正则化表达方式，所以不用Dropout也能达到相当的效果；
3. 另外调参过程也简单多了，对于初始化要求没那么高，而且可以使用大的学习率

### 正则化

L1正则化：$minL(w) = minf(w) + \frac{\lambda}{n}\sum|w_i| $

L2正则化：$minL(w) = minf(w) + \frac{\lambda}{2n}\sum w_i^2$ 

1. L1是模型各个参数的绝对值之和；L2是模型各个参数的平方和。
2. L1得到稀疏解，L2得到较小解。

**优化角度**

L1正则化等价于：$minf(w) \quad s.t.\sum|w_i|<C$ 

L2正则化等价于：$minf(w) \quad s.t.\sum w_i^2<C$ 

表现在图像上会有很多角出现， 多数情况下，菱形的角尖是在“限制活动区”内所能到达的解空间上最低的地方。因为角尖这些点都在坐标轴上，所以与该轴垂直的其他轴所代表的的参数都为0。就会造成最优值出现在坐标轴上，因此就会导致某一维的权重为0 ，产生稀疏权重矩阵，进而防止过拟合。

和L1相比，图像上的棱角被圆滑了很多。最优解以“平等的概率”出现在圆的任何一个点上。在最小化正则项时，参数不断趋向于小数，最后得到很小的参数。 

**梯度视角**

L1梯度：$w_i^{'} = w_i - \eta f' -\eta \frac{\lambda}{n}sign(w_i)$ 

L2梯度：$w_i^{'} = w_i - \eta f' -\eta \frac{\lambda}{n}w_i$

L1会减去$sign(w_i)$倍的$n\frac{\lambda}{n}$，L2会减去$w_i$倍的$n\frac{\lambda}{n}$。

当w>=1时，L2减小的更快；当0<w<1时，L1减小的更快，且匀速下降直至为0。

**概率角度**

加入正则化，相当于f(w)的参数加入先验分布。

L1范数相当于为w加入拉普拉斯分布，L2范数相当于为w加入高斯分布。

如，取先验分布 $w\sim\mathcal{N}(0,\sigma_0^2)$。于是后验为：

  $$ \begin{align} \hat{w}=\mathop{argmax}\limits_wp(w|Y)&=\mathop{argmax}\limits_wp(Y|w)p(w) \\ &=\mathop{argmax}\limits_w\log p(Y|w)p(w) \\ &=\mathop{argmax}\limits_w(\log p(Y|w)+\log p(w)) \\ &=\mathop{argmin}\limits_w[(y-w^Tx)^2+\frac{\sigma^2}{\sigma_0^2}w^Tw] \end{align} $$

我们将会看到，超参数 $\frac{\sigma^2}{\sigma_0^2}$的存在和上面介绍的 Ridge 正则项可以对应，同样的如果将先验分布取为 Laplace 分布，那么就会得到和 L1 正则类似的结果。

![image-20200704165250218](images/image-20200704165250218.png)

如图：

在两侧区间，Gausion<Laplace，即L2<L1，说明L2中值大的w的更少。

在中间区间，Gausion分布中，值小的w和值为0的w概率接近；Laplace分布中，值为0的w概率更高。

