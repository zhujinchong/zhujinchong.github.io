梯度消失原因：

设$f_i$是第i层的输出，记$f_{i+1} = f(f_i * w_{i+1})$，假设共4层，则第2层参数导数为：
$$
\frac{\partial f_{Net}}{\partial w_2}
= \frac{\partial f_4}{\partial f_3} \frac{\partial f_3}{\partial f_2}
= f'_4 w_4 f'_3 w_3 f'_2
$$
Sigmoid导数的取值范围在0~0.25之间，当|w|>1，梯度爆炸；当|w|<1，容易梯度消失。

tanh导数0-1之间，比sigmoid好，但是他的导数仍小于1。

Relu函数的导数在正数部分是恒等于1的，因此在深层网络中使用relu激活函数就不会导致梯度消失和爆炸的问题，且计算速度快。

梯度爆炸解决方法：

* 梯度截断：强制限制在一个范围之内 
* L1/L2正则化：如果发生梯度爆炸，权值的范数就会变的非常大

解决办法：

1. 权重初始化
2. 激活函数
3. batch normalization
4. 构建新颖的网络结构，如Residual Net，而capsule net意图取消BP学习过程，釜底抽薪 

### 权重初始化

不好的初始化参数会导致梯度传播问题, 降低训练速度; 而好的初始化参数, 能够加速收敛, 并且更可能找到较优解。

**0初始化**

问题：如果把w初始化0，那么每一层的神经元学到的东西都是一样的（输出是一样的），而且在bp的时候，每一层内的神经元也是相同的，因为他们的gradient相同。学到最后，迭代就不会起作用。

**初始化为小的随机数，均值为0，方差为0.01的高斯分布**

当神经网络的层数增多时，会发现越往后面的层的激活函数（使用tanH）的输出值几乎都接近于0，造成梯度消失。

**Xavier初始化**（均匀分布）

Xavier initialization是 Glorot 等人为了解决随机初始化的问题提出来的另一种初始化方法，他们的思想倒也简单，就是尽可能的让输入和输出服从相同的分布，这样就能够避免后面层的激活函数的输出值趋向于0。 

根据输入输出之间的方差，反推出W的均匀分布 。算法根据输入和输出神经元的数量自动决定初始化的范围: 

定义参数所在的层的输入维度为n,输出维度为m,那么参数将从 $[ -\sqrt{\frac{6}{m+n}}, \sqrt{\frac{6}{m+n}} ]$ 均匀采样。



缺点：虽然Xavier initialization能够很好的解决tanh梯度消失，但是对于目前神经网络中最常用的ReLU激活函数，还是无能能力。

**He 初始化** 

何恺明大神提出了一种针对ReLU的初始化方法。 对于ReLU激活函数，其使一半数据变成0，初始时这一半的梯度为0，而tanh和sigmoid等的输出初始时梯度接近于1。因此使用ReLU的网络的参数方差可能会波动。He提出放大一倍方差来保持方差稳定。具体过程略。结果就是使用均值为0，方差为$\sqrt\frac{4}{m+n}$ 的高斯分布。

**bias初始化**

通常初始化为0

**总结**

1. 当前的主流初始化方式 Xavier,He两种。
2. 在广泛采用 Batch Normalization 的情况下, 使用普通的小方差的高斯分布即可.
3. 另外, 在迁移学习的情况下, 优先采用预训练的模型进行参数初始化.

### 激活函数

**Relu**

优点：

1. 计算快
2. 解决了梯度消失问题

缺点：

1. 由于负数部分恒为0，会导致一些神经元无法激活
2. 输出不是以0为中心
3. 无法解决梯度爆炸

**leakrelu**

leak relu就是为了解决relu的0区间带来的影响，而且包含了relu的所有优点，其数学表达为：$max(kx, x)$

其中k是leak系数，一般选择0.01或者0.02，或者通过学习而来。

**elu**

if x>0, $elu = x $

if x<=0, $elu = \alpha (e^x - 1)$ 

优点：负数部分会得到激活，避免死亡relu问题

缺点：指数计算复杂

### RNN

RNN还存在一个权值共享的问题，RNN是基于时序的，更容易出现梯度问题。

RNN和一般网络一样，梯度消失问题出现在：$\prod f'_i w_i$  其中一般使用$f=tanh$函数。

### 为什么LSTM能解决梯度消失

* 遗忘门：$f_t = \sigma (W_f [h,x])$ 
* 输入门：$i_t = \sigma (W_i [h,x])$ 
* 输出门：$o_t = \sigma (W_o [h,x])$ 
* 状态单元：$c_t = f_t \otimes c_{t-1} + i_t \otimes tanh(W_c [h,x])$ 
* 隐层输出：$h_t = o_t \otimes tanh(c_t)$ 

LSTM有三个门，是sigmoid函数，输出接近0 or 1。这使得$\frac{\partial c_t}{\partial c_{t-1}} = f_t$ , $\frac{\partial h_t}{\partial h_{t-1}} = o_t$ 。 

当门为1时， 梯度能够很好的在LSTM中传递，很大程度上减轻了梯度消失发生的概率， 当门为0时，说明上一时刻的信息对当前时刻没有影响， 我们也就没有必要传递梯度回去来更新参数了。 

