# 交叉验证

**简单交叉验证：**

划分数据集（70%训练集，30%测试集）。训练集训练模型，测试集验证模型；重新划分数据。。。

**S折交叉验证：**

划分数据S份，S-1份训练，1份测试。

**留一交叉验证：**

N个样本，留一个样本验证模型。N小于50时，采用留一交叉验证。

# 算法评估

TP：预测为正，实际为正

FP：预测为正，实际为负

TN：预测为负，实际为负

FN：预测为负，实际为正



精确率Precision：真正例 / 预测为正：$P = \frac{TP}{TP+FP}$

召回率Recall：真正例 / 实际为正：$R = \frac{TP}{TP+FN}$

F1值，综合评估：$\frac{2}{F_1} = \frac{1}{P} + \frac{1}{R} \quad\quad F_1 = \frac{2PR}{P+R}$

$F_\beta$：$F_\beta = \frac{(1+\beta^2)*P * R}{\beta^2 *P + R}$

### PR曲线

横轴Recall，纵轴Precision

场景：有限样本

方法：固定阈值分类

每个样本都有一个预测分数， 设置一个从高到低的阈值y，大于等于阈值标注为正例，小于阈值标注为负例。 

比如阈值0.9，10个正例1个预测为正，那么precision就是100%，recall就是10%。（10个负例都为负）

比如阈值0.1，10个负例9个预测为正，那么precision就是10/19，recall就是100%。（10个正例都为正）

阈值大，精确率大；阈值小，召回率大。

### Roc曲线

真正率TPR：真正例 / 实际正例，（召回率）：$TPR = \frac{TP}{TP+FN}$

假正率FPR：假正例 / 实际负例：$FPR = \frac{FP}{FP+TN}$ 

以FPR为x轴，TPR为y轴。FPR指实际负样本中被错误预测为正样本的概率。TPR指实际正样本中被预测正确的概率。 

AUC是Roc曲线下的面积（右下角）。

阈值大，都小；阈值小，都大。比较稳定。



> AUC是一种衡量机器学习模型分类性能的重要且非常常用的指标,其只能用于二分类的情况.
>  AUC的本质含义反映的是对于任意一对正负例样本,模型将正样本预测为正例的可能性 大于 将负例预测为正例的可能性的 概率。
>
> TPR表示：将正例预测为正例的比例
>
> FPR表示：将负例预测为正例的比例



### 两者关系

ac是Roc曲线，bd是PR曲线。ab是在样本均衡的情况下，cd是在负样本较多的情况下。

这就说明PR曲线在正负样本比例悬殊较大时更能反映分类的性能。 

![roc-pr](images/18801443447279.jpg)

1. 当正负样本比例差不多的时候，两者区别不大。
2. 当正负样本比例失调时，ROC曲线变化不大，此时用PR曲线更加能反映出分类器性能的好坏。
3. PR曲线比ROC曲线更加关注正样本，而ROC则兼顾了两者。
4. AUC越大，反映出正样本的预测结果更加靠前。（推荐的样本更能符合用户的喜好）

### IoU

目标检测指标$IoU = \frac{交集}{并集}$

### AP

PR曲线的面积(左下角)

### mAP

mean Average Precision:  由于多标签图像的分类任务中，图片的标签不止一个，因此评价不能用普通单标签图像分类的标准，即mean accuracy，是AP平均值















