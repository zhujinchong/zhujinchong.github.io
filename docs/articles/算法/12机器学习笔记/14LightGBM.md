> 参考 https://www.zhihu.com/search?type=content&q=LightGBM 

LightGBM为了解决海量数据时遇到的问题：

1. 如何减少训练数据
2. 如何减少特征
3. 关于稀疏的数据

LGBM和XGBoost区别：

|                |         表头          |                                 |
| :------------: | :-------------------: | :-----------------------------: |
|  叶子生长方式  |         按层          |             按收益              |
| 划分点搜索方法 |      特征预排序       |           直方图算法            |
|    内存开销    | 8个字节（存储特征值） | 1个字节（存储bins中特征的个数） |
| 划分的计算增益 |       数据特征        |            容器特征             |
|  类别特征处理  |        不支持         |              支持               |

也就是增加了三个算法：

1. histogram：减少候选分裂点，XGBoost需要遍历所有特征上的所有候选分裂点
2. Goss：单边梯度采样，减少样本的数量
3. EFB：互斥特征捆绑，减少特征的数量

### Histogram算法

XGBoost中采用预排序的方法，计算过程当中是按照value的排序，逐个数据样本来计算划分收益，这样的算法能够精确的找到最佳划分值，但是代价比较大同时也没有较好的推广性，并且还需要存储原来的数据的序列。

第一个优点：减少存储

在LightGBM中没有使用传统的预排序的思路，而是将这些精确的连续的每一个value划分到一系列离散的域中。这样一来，数据的表达变得更加简化，减少了内存的使用，而且直方图带来了一定的正则化的效果，能够使我们做出来的模型避免过拟合且具有更好的推广性。
第二个优点：计算加速（直方图差速）

选好特征后，计算左子节点数据（一阶、二阶梯度），右子节点可以通过父节点减左子节点得到。

### Goss单边梯度采样

LGB的优化方法是：在保留大梯度样本的同时，随机地保留一些小梯度样本，同时放大小梯度样本的梯度。

比如：a%大梯度样本，随机选择b%个小梯度样本，那么，小梯度样本需要扩大(1-a)/b倍。

### EFB互斥特征捆绑

**如何判断互斥？**（ 利用特征和特征间的关系构造一个加权无向图，图着色问题）

1. 首先特征看成图中的点，不相互独立的特征用一条边连起来，边的权重就是两个相连接的特征的总冲突值
2. 按照节点的度对特征降序排序， 度越大，说明与其他特征的冲突越大
3. 对于每一个特征， 遍历已有的特征簇，如果发现该特征加入到特征簇中的矛盾数不超过某一个阈值，则将该特征加入到该簇中。 如果该特征不能加入任何一个已有的特征簇，则新建一个簇，将该特征加入到新建的簇中。

**如何对同一个簇中的特征捆绑？**

对于特征如何合并，一个重要的原则就是使合并的两个特征可以被顺利区分出来，LGB采取了一个更改阈值的方法。例如对于特征x∈(0, 10)， 特征y∈(0, 20)，就可以把特征y转换为y∈(10,30)，然后再去合并x与y。

