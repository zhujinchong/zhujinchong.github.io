Bengio 在2003年发表在JMLR上的论文“神经网络语言模型”，英文小名NNLM。它生于2003，火于2013。

![img](images/640-1572833442749.webp)

任务：输入前面t-1个单词，预测第t个单词。其中输入是one-hot编码的词Wi，以后乘以矩阵Q获得向量C(Wi)。然后拼接在一起，通过隐层，接着softmax去预测。

上面任务不仅自己能够根据上文预测后接单词是什么，同时获得一个矩阵Q，该矩阵就可以获得词向量。而2013年最火的用语言模型做Word Embedding的工具是Word2Vec，之后是Glove。

1. 2013年，word2vec，固定词向量
2. 2018年，Facebook的ELMO
3. 2018年，OpenAI的GPT
4. 2018年10月，Google的BERT



### 1. Word2Vec

在word2vec，每个词都有一个固定词向量表示。

缺点：一词多义

### 2. ELMO

论文题目“Deep contextualized word representation”。

ELMO的本质思想是：我事先用语言模型学好一个单词的Word Embedding，此时多义词无法区分，不过这没关系。在我实际使用Word Embedding的时候，单词已经具备了特定的上下文了，这个时候我可以根据上下文单词的语义去调整单词的Word Embedding表示，这样经过调整后的Word Embedding更能表达在这个上下文中的具体含义，自然也就解决了多义词的问题了。所以ELMO本身是个根据当前上下文对Word Embedding动态调整的思路。

ELMO采用了典型的两阶段过程，第一个阶段是利用语言模型进行预训练；第二个阶段是在做下游任务时，从预训练网络中提取对应单词的网络各层的Word Embedding作为新特征补充到下游任务中，即特征融合。

具体怎么训练？（我还不清楚）ELMO语言模型训练目标是，根据单词上下文，预测单词，和CBOW模型很像。

![img](images/640-1572835708344.webp)

上图展示的是其预训练过程，网络结构采用了双层双向LSTM，输入是一个句子，句子中每个单词都能得到对应的三个Embedding。



![img](images/640-1572836586081.webp)

使用很简单，输入句子，将三个Embedding加权（权重可以学习）得到特征融合的词向量，词向量用于下游任务。

缺点：

1. LSTM特征抽取能力弱与Transformer
2. 特征融合能力没有微调好

### 3. GPT

1. GPT采取Transformer作为特征提取器
2. 开创"基于Fine-tuning的模式"

GPT也采用两阶段过程，第一个阶段是利用语言模型进行预训练，第二阶段通过Fine-tuning的模式解决下游任务。

GPT预训练采用单向的语言模型，即根据单词上文，预测单词。缺点就是没有使用单词下文。

![img](images/640-1572838893454.webp)

首先，对于不同的下游任务来说，本来你可以任意设计自己的网络结构，现在不行了，你要向GPT的网络结构看齐，把任务的网络结构改造成和GPT的网络结构是一样的。然后，在做下游任务的时候，利用第一步预训练好的参数初始化GPT的网络结构，这样通过预训练学到的语言学知识就被引入到你手头的任务里来了，这是个非常好的事情。再次，你可以用手头的任务去训练这个网络，对网络参数进行Fine-tuning，使得这个网络更适合解决手头的问题。

这里引入了一个新问题：对于NLP各种花样的不同任务，怎么改造才能靠近GPT的网络结构呢？

![img](images/v2-4c1dbed34a8f8469dc0fefe44b860edc_hd.jpg)

GPT论文给了一个改造施工图如上，其实也很简单：对于分类问题，不用怎么动，加上一个起始和终结符号即可；对于句子关系判断问题，比如Entailment，两个句子中间再加个分隔符即可；对文本相似性判断问题，把两个句子顺序颠倒下做出两个输入即可，这是为了告诉模型句子顺序不重要；对于多项选择问题，则多路输入，每一路把文章和答案选项拼接作为输入即可。从上图可看出，这种改造还是很方便的，不同任务只需要在输入部分施工即可。

GPT缺点：

1. 语言模型是单向的
2. 不太会炒作。。

### 3. BERT

相比GPT，改进两点：1.双向语言模型；2.数据规模更大

![img](images/v2-477b738008eb2b5650577bbd220bc58d_hd.jpg)

当然，它也面临着下游任务网络结构改造的问题，在改造任务方面Bert和GPT有些不同，下面简单介绍一下。

![img](images/v2-0245d07d9e227d1cb1091d96bf499032_hd.jpg)

对于种类如此繁多而且各具特点的下游NLP任务，Bert如何改造输入输出部分使得大部分NLP任务都可以使用Bert预训练好的模型参数呢？上图给出示例，对于句子关系类任务，很简单，和GPT类似，加上一个起始和终结符号，句子之间加个分隔符即可。对于输出来说，把第一个起始符号对应的Transformer最后一层位置上面串接一个softmax分类层即可。对于分类问题，与GPT一样，只需要增加起始和终结符号，输出部分和句子关系判断任务类似改造；对于序列标注问题，输入部分和单句分类是一样的，只需要输出部分Transformer最后一层每个单词对应位置都进行分类即可。从这里可以看出，上面列出的NLP四大任务里面，除了生成类任务外，Bert其它都覆盖到了，而且改造起来很简单直观。尽管Bert论文没有提，但是稍微动动脑子就可以想到，其实对于机器翻译或者文本摘要，聊天机器人这种生成式任务，同样可以稍作改造即可引入Bert的预训练成果。只需要附着在S2S结构上，encoder部分是个深度Transformer结构，decoder部分也是个深度Transformer结构。根据任务选择不同的预训练数据初始化encoder和decoder即可。这是相当直观的一种改造方法。当然，也可以更简单一点，比如直接在单个Transformer结构上加装隐层产生输出也是可以的。不论如何，从这里可以看出，NLP四大类任务都可以比较方便地改造成Bert能够接受的方式。这其实是Bert的非常大的优点，这意味着它几乎可以做任何NLP的下游任务，具备普适性，这是很强的。


