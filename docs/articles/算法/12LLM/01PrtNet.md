 PtrNet模型脱胎于Attention机制，使用了Attention机制里的对齐向量a。 

 PtrNet主要应用于NLP的文本摘要任务、对话任务、（特定条件下）的翻译任务，目的在于应对OOV问题。 

### OOV

 在NLP领域中，由于词表大小的限制，部分低频词无法被纳入词表，这些词即为OOV（Out Of Bag）。它们会统一地被UNK替代显示，其语义信息也将被丢弃。 

OOV难以完全规避，有两个主要原因：

（1）命名实体常常包含重要的信息，但是许多命名实体也是低频词，常常无法被纳入词表。

（2）网络新词层出不穷，旧词表无法及时的更新。尤其是在模型越来越大的现状下，加入新词后模型重新训练的成本代价很高。

应对OOV的三个主要方法

（1）扩大词表：扩大词表后，可以将部分低频词纳入了词表，但是这些低频词由于缺乏足够数量的语料，训练出来的词向量往往效果不佳，所以扩大词表在提升模型的效果方面，存在一定的瓶颈。

（2）指针拷贝网络：这正是本文将要介绍的模型。

（3）字向量/n-gram：中文任务使用字向量（例如BERT），英文任务使用n-gram。



### Pointer Networks

 Vinyals O,Fortunato M, Jaitly N. Pointer Networks[J]. Computer Science, 2015, 28. 

 《Pointer Networks》论文是PtrNet模型的开山之作，发表于2015年。 

![img](images/640-1573179657966.webp)

本论文的思路，就是让decoder输出一个由目标序列指向源序列的指针序列，而如何去选择这个指针，就是论文的主要内容。

PtrNet利用了Attention模型的一个中间变量：对齐系数a。因为对齐系数a表示目标序列当前step和源序列所有step之间的相关性大小，所以我们可以通过选取对齐系数向量a（向量a长度为源序列的step长度）中数值最大的那个维度（每个维度指向源序列的一个step），实现一个从目标序列step到源序列step的指针。最后，根据这个指针指向的源序列的词，直接提取这个词进行输出。

通过以上方法，就能够在decoder的每一个step，从源序列的所有step中抽取出一个，进行输出。这样，由于decoder输出的是一个指针序号的序列，而不是具体的词，也就没有了OOV问题；同时，因为不需要构建词表，就从根本上解决了词表内容和词表长度的问题。

很遗憾的，虽然论文中有提及：PtrNet可以在翻译任务中提取命名实体，在摘要任务中提取关键词，但是论文中并没有更进一步，将PtrNet应用于NLP任务，而是利用凸包求解问题、旅行商问题进行了实验和论证。