参考：

* https://mp.weixin.qq.com/s/ifZ_wnJkgRhu0Aa8-6u11Q



2019/10 T5 (from Google)

Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer

https://arxiv.org/abs/1910.10683v1

论文50页，是一个综述。



T5是一个超大预训练模型，把所有的任务都转化成一种形式。

![img](images/640-1574824009325.webp)

上面是T5完成的四个任务：翻译、情感分类、文本语义相似度任务（1-5分）、摘要



### Data: C4

作者从 Common Crawl（一个公开的网页存档数据集，每个月大概抓取 20TB 文本数据） 里清出了 750 GB 的训练数据，然后取名为 ” Colossal Clean Crawled Corpus （超大型干净爬取数据）“，简称 C4，论作者取名之恶趣味。

### Architecture

作者对多种模型架构进行了对比，主要是如下三种：

![img](images/640-1574824889148.webp)

第一种，**Encoder-Decoder 型**，即 Seq2Seq 常用模型，分成 Encoder 和 Decoder 两部分，对于 Encoder 部分，输入可以看到全体，之后结果输给 Decoder，而 Decoder 因为输出方式只能看到之前的。此架构代表是 MASS（今年WMT的胜者），而 BERT 可以看作是其中 Encoder 部分。

第二种， 相当于上面的 **Decoder 部分**，当前时间步只能看到之前时间步信息。典型代表是 GPT2 还有最近 CTRL 这样的。

第三种，**Prefix LM（Language Model） 型**，可看作是上面 Encoder 和 Decoder 的融合体，一部分如 Encoder 一样能看到全体信息，一部分如 Decoder 一样只能看到过去信息。最近开源的 UniLM 便是此结构。

上面这些模型架构都是 Transformer 构成，之所以有这些变换，主要是**对其中注意力机制的 Mask 操作**。

![img](images/640-1574824942143.webp)

通过实验作者们发现，在提出的这个 Text-to-Text 架构中，Encoder-Decoder 模型效果最好。于是乎，就把它定为 T5 模型，因此**所谓的 T5 模型其实就是个 Transformer 的 Encoder-Decoder 模型**。 

### Objectives 重点

之后是对预训练目标的大范围探索，具体做了哪些实验，下面这张图就能一目了然。

![img](images/640-1574825187478.webp)

总共从四方面来进行比较。

第一个方面，**高层次方法（自监督的预训练方法）对比**，总共三种方式。

1. **语言模型式**，就是 GPT-2 那种方式，从左到右预测；
2. **BERT-style 式**，就是像 BERT 一样将一部分给破坏掉，然后还原出来；
3. **Deshuffling （顺序还原）式**，就是将文本打乱，然后还原出来。

其中发现 Bert-style 最好，进入下一轮。

第二方面，对文本一部分进行**破坏时的策略**，也分三种方法。

1. **Mask 法**，如现在大多模型的做法，将被破坏 token 换成特殊符如 [M]；
2. **replace span（小段替换）法**，可以把它当作是把上面 Mask 法中相邻 [M] 都合成了一个特殊符，每一小段替换一个特殊符，提高计算效率；
3. **Drop 法**，没有替换操作，直接随机丢弃一些字符。

此轮获胜的是 **Replace Span 法**，类似做法如 SpanBERT 也证明了有效性。

第三方面，到底该**对文本百分之多少进行破坏**呢，挑了 4 个值，10%，15%，25%，50%，最后发现 BERT 的 **15%** 就很 ok了。这时不得不感叹 BERT 作者 Devlin 这个技术老司机直觉的厉害。

接着进入更细节，第四方面，因为 Replace Span 需要决定**对大概多长的小段进行破坏**，于是对不同长度进行探索，2，3，5，10 这四个值，最后发现 **3** 结果最好。

终于获得了完整的 T5 模型，还有它的训练方法。

- Transformer Encoder-Decoder 模型；
- BERT-style 式的破坏方法；
- Replace Span 的破坏策略；
- 15 %的破坏比；
- 3 的破坏时小段长度。

到此基本上 T5 预训练就大致说完了，之后是些细碎探索。

### Models

训练了不同规模的模型：

- Small，Encoder 和 Decoder 都只有 6 层，隐维度 512，8 头；

- Base，相当于 Encoder 和 Decoder 都用 BERT-base；
- Large，Encoder 和 Decoder 都用 BERT-large 设置，除了层数只用 12 层；
- 3B（Billion）和11B，层数都用 24 层，不同的是其中头数量和前向层的维度。

11B 的模型最后在 GLUE，SuperGLUE，SQuAD，还有 CNN/DM 上取得了 SOTA，而 WMT(翻译) 则没有。

### 总结

1. 本文对多模型多参数进行了对比，找到了一个较好的模型
2. 大模型大数据表现越好