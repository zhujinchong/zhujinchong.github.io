2017/12 Tranformer

《Attention Is All You Need》



2018/6 GPT

《Improving Language Understanding by Generative Pre-Training》

 https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf 



2018/10 BERT

《BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding》

 

2019/2 GPT2.0

《 Language Models are Unsupervised Multitask Learners 》

 https://www.techbooky.com/wp-content/uploads/2019/02/Better-Language-Models-and-Their-Implications.pdf 



2019/6 Tranformer-XL

《transformer-xl attentive language models beyond a fixed-length context》

 https://arxiv.org/abs/1901.02860 



2019/6 XLNet

《XLNet: Generalized Autoregressive Pretraining for Language Understanding》

 https://arxiv.org/abs/1906.08237 



2019/7 RoBERTa

RoBERTa: A Robustly Optimized BERT Pretraining Approach

 https://arxiv.org/abs/1907.11692 



2019/9 ELECTRA

ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators

 https://openreview.net/forum?id=r1xMH1BtvB 



2019/10 DistialBERT (from HuggingFace)

DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter

https://arxiv.org/abs/1910.01108v1



2019/9 ALBERT (from Google)

ALBERT: A Lite BERT for Self-supervised Learning of Language Representations

https://arxiv.org/abs/1909.11942v1



2019/10 T5 (from Google)

Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer

https://arxiv.org/abs/1910.10683v1



2019/9 MMBT (from Facebook)

Supervised Multimodal Bitransformers for Classifying Images and Text

https://arxiv.xilesou.top/abs/1909.02950



XLNET，以及UNILM、MASS、MT-DNN、XLM，都是基于这种思路的扩充，解决相应的任务各有所长。其中微软研究院的UNILM可同时训练得到类似BERT和GPT的模型，而微软MASS采用encoder-decoder训练在机器翻译上效果比较好。还有MT-DNN强调用多任务学习预训练模型，而XLM学习多语言BERT模型，在跨语言迁移学习方面应用效果显著。



MT-DNN-SMART_Microsoft D365

ALICE_v2 王玮

ERNIE_Baidu

https://github.com/PaddlePaddle/ERNIE

