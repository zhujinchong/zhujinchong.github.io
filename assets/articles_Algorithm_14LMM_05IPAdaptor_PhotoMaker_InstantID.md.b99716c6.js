import{_ as e,o,c as t,Q as a}from"./chunks/framework.2516552c.js";const n="/assets/image-20240220223300204.e438acb0.png",r="/assets/image-20240220224418797.71283369.png",i="/assets/image-20240220225241164.975fb529.png",p="/assets/image-20240221222250014.a3db4044.png",s="/assets/image-20240221223350082.e9bdbd99.png",d="/assets/image-20240221223406879.3e60a876.png",c="data:image/png;base64,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",l="/assets/image-20240221222703925.0d040602.png",m="/assets/image-20240221231932278.2d7727f3.png",g="/assets/image-20240221232216801.39abf7a2.png",u="/assets/image-20240221224507338.dca56850.png",f="/assets/image-20240221225355096.a2bbcb6a.png",h="/assets/image-20240221225702579.f24e455b.png",b="/assets/image-20240221230118794.88e2578a.png",I="/assets/image-20240221230138903.526e3fa7.png",A="/assets/image-20240221230326866.298652aa.png",P="/assets/image-20240221231219359.9c881a37.png",U=JSON.parse('{"title":"Dreambooth","description":"","frontmatter":{},"headers":[],"relativePath":"articles/Algorithm/14LMM/05IPAdaptor_PhotoMaker_InstantID.md","filePath":"articles/Algorithm/14LMM/05IPAdaptor_PhotoMaker_InstantID.md","lastUpdated":null}'),k={name:"articles/Algorithm/14LMM/05IPAdaptor_PhotoMaker_InstantID.md"},D=a('<p><a href="https://zhuanlan.zhihu.com/p/678613724" target="_blank" rel="noreferrer">https://zhuanlan.zhihu.com/p/678613724</a></p><h1 id="dreambooth" tabindex="-1">Dreambooth <a class="header-anchor" href="#dreambooth" aria-label="Permalink to &quot;Dreambooth&quot;">​</a></h1><p>2022-04</p><p>背景：</p><p>文生图模型可以根据prompt生成高质量图片，但是模型不能保留ID，在不同情境中生成新颖的图片。（即主角不变，只改变背景）。</p><p>DreamBooth是一种个性化文生图模型：给定某个物体的几张图片作为输入，通过微调预训练的文生图模型（如Imagen），将一个独特的标识符和该物体进行绑定，这样就可以通过含有该标识符的prompt在不同场景下生成包含该物体的新颖图片。</p><p><img src="'+n+'" alt="image-20240220223300204"></p><p>方法：</p><p>作者希望将输入图片中的物体与一个特殊标识符绑定在一起，即用这个特殊标记符来表示输入图片中的物体。因此作者为微调模型设计了一种prompt格式：<code>a [identifier] [class noun]</code>，即将所有输入图片的promt都设置成这种形式，其中<code>identifier</code>是一个与输入图片中物体相关联的特殊标记符，<code>class noun</code>是对物体的类别描述。这里之所以在prompt中加入类别，是因为作者想利用预训练模型中关于该类别物品的先验知识，并将先验知识与特殊标记符相关信息进行融合，这样就可以在不同场景下生成不同姿势的目标物体。</p><p>例如：</p><ul><li>3-10张图片, 最好是不同角度，且背景有变化的图片</li><li>类名class name+独特标识符unique identifier，例如：instance prompt: a photo of <strong>Halley</strong> Dog; class prompt: a photo of <strong>Dog</strong>。这里Halley是独特标识符，Dog是类名。</li><li>微调即可。</li></ul><h1 id="textual-inversion" tabindex="-1">Textual Inversion <a class="header-anchor" href="#textual-inversion" aria-label="Permalink to &quot;Textual Inversion&quot;">​</a></h1><p>时间：2022-08</p><blockquote><p><strong>embedding是textual inversion的结果，因此textual inversion也可以称为embedding。该方法只需要3-5张图像，通过定义新的关键词，就能生成和训练图像相似的风格。</strong></p></blockquote><p>背景：Stable Diffusion它们的Embedding空间的表现力足以捕捉基本的语义信息，但是这两者都无法准确捕捉概念的外观，将它们用于特定主题的图像生成会导致相当大的视觉差异。如果为每个新概念使用特定数据集重新训练模型，那成本将非常高，并且对少数示例图片进行finetune通常会导致灾难性的遗忘。</p><p>本文提出Textual Inversion方法，只需使用用户提供的3~5张概念图片，通过学习文图生成模型Text Embedding空间中的伪词（pseudo-word）来表示这些概念。然后把这些伪词组合成自然语言的句子，指导个性化生成。</p><p><img src="'+r+'" alt="image-20240220224418797"></p><p>原理：</p><ul><li>输入字符串中的每个词（<strong>word</strong>）或子词（<strong>sub-word</strong>）都被转换为一个标记（<strong>Token</strong>），它是预定义词典中的索引（参见<strong>BPE</strong>算法）；</li><li>然后将每个<strong>Token</strong>对应到一个唯一的嵌入向量（<strong>embedding</strong>），这些嵌入向量通常作为文本编码器的一部分进行学习；</li><li>选择这个嵌入向量空间作为反演（<strong>Inversion</strong>）的目标，指定一个占位符字符串 <strong>S*</strong> 来表示希望学习的新概念；</li><li>用新学习的嵌入向量 <strong>v*</strong> 替换与新概念关联的嵌入向量，即将概念注入（<strong>inject</strong>）到词汇表中；</li><li>跟处理其它单词一样，用该概念字符组成新句子，例如： A photo of <strong>S*</strong>, A painting in the style of <strong>S*.</strong></li></ul><p>因此，Textual Inversion只需要训练文本嵌入层即可，而Dreambooth理念需要微调文生图模型。</p><h1 id="hypernetworks" tabindex="-1">Hypernetworks <a class="header-anchor" href="#hypernetworks" aria-label="Permalink to &quot;Hypernetworks&quot;">​</a></h1><p>hypernetworks是一种fine tune的技术，最开始由novel AI开发。hypernetworks是一个附加到stable diffusion model上的小型网络，用于修改扩散模型的风格。</p><p>既然Hypernetworks会附加到diffusion model上，那么会附加到哪一部分呢？答案仍然是UNet的cross-attention模块，Lora模型修改的也是这一部分，不过方法略有不同。</p><p>hypernetworks是一个很常见的神经网络结构，具体而言，是一个带有dropout和激活函数的全连接层。通过插入两个网络转换key和query向量。</p><p><img src="'+i+'" alt="image-20240220225241164"></p><p>和其他方法的对比：</p><p>LoRA: Lora模型和Hypernetworks比较相似，都是通过作用于UNet的cross-attention模块，改变生成图像的风格。区别在于LoRA改变的是cross-attention的权重，而Hypernetworks插入了其他的模块。LoRA的结果通常比Hypernetworks更好，而且模型结构都很小，基本都低于200Mb。值得注意的是，LoRA指的是一种数据存储的方式，没有定义训练过程，因此可以和Dreambooth和其他的训练方式结合起来使用。</p><p>embeddings: 是textual inversion方法的结果，和Hypernetworks方法一样，不会改变模型，会定义一些新的关键字实现某些样式。embedding和Hypernetworks作用的地方不同。embedding在text encoder中创造新的embedding达到左右模型生成图片的效果。根据一些参考资料，embedding的效果比Hypernetworks要稍好一些。</p><h1 id="controlnet" tabindex="-1">ControlNet <a class="header-anchor" href="#controlnet" aria-label="Permalink to &quot;ControlNet&quot;">​</a></h1><p>时间：2023-02</p><p>ControlNet可以从提供的参考图中获取布局或者姿势等信息, 并引导diffusion model生成和参考图类似的图片。调整prompt确实是一件费时费力的事情，而有了ControlNet则可以精确的控制生成图片的细节(当然, 也依赖于你已经有一张参考图了)。</p><p>为了能继承大模型的能力, ControlNet将原来大模型的参数分为两部分, 分别是<strong>可训练</strong>和<strong>冻结</strong>的。</p><p><img src="'+p+'" alt="image-20240221222250014"></p><p>图(a)是之前stable diffusion的输出, 图(b)和图(a)的区别在于添加了ControlNet的结构, 具体而言是将<code>neural network block</code>复制了一份, 作为<code>trainable copy</code>,并且<code>neural network block</code>的网络参数会被冻结住。而且<code>trainable copy</code>前后会有<code>zero convolution</code>, <code>zero convolution</code>其实是1*1的卷积。最后会将<code>neural network block</code>和<code>trainable copy</code>的特征相加。</p><p>Before前模型是这样的：</p><p><img src="'+s+'" alt="image-20240221223350082"></p><p>After模型是这样的：</p><p><img src="'+d+'" alt="image-20240221223406879"></p><p>并且，训练前：</p><p><img src="'+c+'" alt="image-20240221223424633"></p><p>上一部分描述了ControlNet如何控制单个神经网络块，论文中作者是以Stable Diffusion为例子，讲了如何使用ControlNet对大型网络进行控制。灰色部分是原来stable diffusion的结构, 蓝色部分是从U-Net的encode对应部分copy, 经过<code>zero convolution</code>后和U-Net的decode相加。</p><p><img src="'+l+'" alt="image-20240221222703925"></p><p>虽然ControlNet新加了些模型结构, 但由于大部分参数都被冻结, 因此训练时实际上只用原来stable diffusion模型的23%显存和34%的训练时间, 可以在单张40GB的A100卡上做训练（50k数据）。</p><h1 id="anydoor" tabindex="-1">AnyDoor <a class="header-anchor" href="#anydoor" aria-label="Permalink to &quot;AnyDoor&quot;">​</a></h1><p>时间：2023-07</p><p>机构：香港大学、阿里集团、蚂蚁集团联合开源</p><p>AnyDoor实现了零样本的图像嵌入，主要功能是“图像传送”，点两下鼠标，就能把物体无缝「传送」到照片场景中，光线角度和透视也能自动适应。例如，将女生的蓝色短袖换成其他样式的红色衣服。</p><p><img src="'+m+'" alt="image-20240221231932278"></p><p>工作原理：</p><p>首先采用分割模块从对象中删除背景，然后使用ID提取器获取其<strong>ID Tokens</strong>。然后，我们对“干净”的对象应用高通滤波器，将所得的高频图(HF-Map)与期望位置的场景拼接起来，并使用细节提取器以纹理细节补充ID提取器。最后，将<strong>ID Tokens</strong>和<strong>Detail Maps</strong>注入预训练的扩散模型。</p><p>细节：</p><p>1、ID Extractor使用自监督式的物体提取并转换成token，这一步使用的编码器是以目前最好的自监督模型DINO-V2为基础设计的。</p><p>2、ID Extractor专注于细节提取；而&quot;HF-Map&quot;高频图侧重于全局信息。</p><p><img src="'+g+'" alt="image-20240221232216801"></p><h1 id="ip-adapter" tabindex="-1">IP-Adapter <a class="header-anchor" href="#ip-adapter" aria-label="Permalink to &quot;IP-Adapter&quot;">​</a></h1><p>时间：2023-08</p><p>机构：腾讯</p><p>“垫图”这个概念大家肯定都不陌生，此前当无法准确用prompt描述心中那副图时，最简单的办法就是找一张近似的，然后img2img流程启动，一切搞定。img2img有它绕不过去的局限性，比如对prompt的还原度不足、生成画面多样性弱，特别是当需要加入controlnet来进行多层控制时，参考图、模型、controlnet的搭配就需要精心挑选。</p><p>如下图，img2img生成画面多样性弱：对参考图依赖强，对提示词依赖弱。</p><p><img src="'+u+'" alt="image-20240221224507338"></p><p>IP-Adapter （Image Prompt Adapter）基于解耦的交叉注意力机制，允许模型同时处理文本和图像信息，而不会互相干扰。</p><p><img src="'+f+'" alt="image-20240221225355096"></p><p><img src="'+h+'" alt="image-20240221225702579"></p><h1 id="photomaker" tabindex="-1">PhotoMaker <a class="header-anchor" href="#photomaker" aria-label="Permalink to &quot;PhotoMaker&quot;">​</a></h1><p>时间：2023-12</p><p>机构：腾讯</p><p>特点：不用训练，就能复刻人脸（因为这里主要训练了人脸数据）。</p><p>PhotoMaker 主要通过将任意数量的输入图像编码成堆叠的ID嵌入来保存信息。这种嵌入不仅可以全面封装人物特征，还可以容纳不同 ID 的特征以便后续集成。它的工作原理是从文本编码器和图像编码器分别获得来源，通过合并和提取相应类的ID嵌入。从而很好地生成统一ID的内容。简单来说，PhotoMaker能够在保留人物特征的情况下，轻松更换多种风格也不失真，同时，它还满足高质量输出，最终出来的效果相对好很多。</p><p><img src="'+b+'" alt="image-20240221230118794"></p><p><img src="'+I+'" alt="image-20240221230138903"></p><p>在实现机制上，PhotoMaker先从文本和图像编码器中获取相应的嵌入，然后通过合并类别嵌入和图像嵌入来形成融合嵌入。这些嵌入最终被串联成堆叠ID嵌入，供后续的Diffusion Model生成图像使用。</p><p><img src="'+A+'" alt="image-20240221230326866"></p><h1 id="instantid" tabindex="-1">InstantID <a class="header-anchor" href="#instantid" aria-label="Permalink to &quot;InstantID&quot;">​</a></h1><p>时间：2024-01</p><p>机构：据说是小红书</p><p>目前基于<code>diffusion model</code>做定制生成主要有两类方法：inference before fine-tune和inference without fine-tune。inference before fine-tune方法每次有新的概念都需要训练模型，相对繁琐，但效果较好，代表工作有<code>DreamBooth</code>, <code>lora</code>, <code>textual inversion</code>等。inference without fine-tune的方法需要预先在大量数据上训练一个鲁棒的object embedding提取模型，推理时无需再次训练，代表工作有<code>AnyDoor</code>， <code>IPAdapter</code>等。</p><p>人脸的定制生成往往需要更加细粒度（fine-grain）的特征提取，现有的基于inference without fine-tune的方法做的都不是很好。本文提出了一种即插即用plug-and-play 定制人脸生成模型（Plugability），给定一张人脸照片，就能生成指定风格和pos的照片。InstantID不仅前期训练成本低（compatibility），还能实现inference without fine-tune （Tuning-free）和高保真图像的生成。（Superior performance）。取得了fidelity、efficiency、flexible三者很好的平衡。</p><p>为了实现上述目的，设计了3个模块</p><ul><li><code>ID embedding</code>: 用于提取reference image的人脸特征。</li><li><code>Adapted module</code>：用于将人脸特征融入到<code>diffusion model</code>中</li><li><code>IdentityNet</code>：用于将人脸的spatial 信息融入到<code>diffusion model</code>中</li></ul><p><img src="'+P+'" alt="image-20240221231219359"></p><p>三个模块的作用和原理：</p><p>1、<code>ID embedding</code>：作者认为<code>CLIP</code>的训练用了大多数弱对齐的语料，这导致<code>CLIP</code> image encoder提取的特征来自广泛模糊的语义信息，对于ID级别的特征提取的粒度是不够的。因此作者此处用了人脸领域的预训练模型antelopev2提取Id embedding。</p><p>2、<code>Adapted module</code>：这篇paper参考了<code>IP-Adapter</code>的方法，分别将image embedding和text embedding融入到decoupled cross-attention中。</p><p>3、<code>IdentityNet</code>：<code>IdentityNet</code>的核心目的是给<code>diffusion model</code>增加spatial control的能力,来弥补损害的text edit能力。作者采用<code>Controlnet</code>的思路来实现<code>IdentityNet</code>。</p>',84),y=[D];function w(H,N,M,K,C,T){return o(),t("div",null,y)}const Q=e(k,[["render",w]]);export{U as __pageData,Q as default};
