import{_ as t,o as r,c as e,Q as n}from"./chunks/framework.2516552c.js";const a="/assets/v2-eaaf1c00d0c4ea350cd3a79b47de26d3_720w.11b1ebb4.webp",A=JSON.parse('{"title":"参考","description":"","frontmatter":{},"headers":[],"relativePath":"articles/算法/22模型训练和微调/02模型微调.md","filePath":"articles/算法/22模型训练和微调/02模型微调.md","lastUpdated":1698149129000}'),i={name:"articles/算法/22模型训练和微调/02模型微调.md"},o=n('<h1 id="参考" tabindex="-1">参考 <a class="header-anchor" href="#参考" aria-label="Permalink to &quot;参考&quot;">​</a></h1><p>这个作者的一系列文章：</p><p>大模型参数高效微调技术原理综述: <a href="https://zhuanlan.zhihu.com/p/636999010" target="_blank" rel="noreferrer">https://zhuanlan.zhihu.com/p/636999010</a></p><h1 id="微调技术分类" tabindex="-1">微调技术分类 <a class="header-anchor" href="#微调技术分类" aria-label="Permalink to &quot;微调技术分类&quot;">​</a></h1><p>1、全量参数微调：BERT论文中提出了“预训练语言模型 + 下游任务微调”，这里微调指全量参数微调。</p><p>2、高效参数微调Parameter-Efficient Fine-Tuning (PEFT)：GPT-3模型很大，在消费级显卡上进行全量参数微调变得不可行。高效参数微调是指微调少量或额外的模型参数，固定大部分预训练模型（LLM）参数，从而大大降低了计算和存储成本，同时，也能实现与全量参数微调相当的性能。</p><p>高效微调技术可以粗略分为以下三大类：增加额外参数（Additive）、选取一部分参数更新（Selective）、引入重参数化（Reparametrization）。而在增加额外参数这类方法中，又主要分为类适配器（Adapter-like）方法和软提示（Soft prompts）两个小类。</p><p>此外，还有混合多种技术的微调方法，如：MAM Adapter、UniPELT。</p><p>图出自论文：Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning</p><p><img src="'+a+'" alt="img"></p><h2 id="_1、选取部分参数" tabindex="-1">1、选取部分参数 <a class="header-anchor" href="#_1、选取部分参数" aria-label="Permalink to &quot;1、选取部分参数&quot;">​</a></h2><p>BitFit（论文：<strong>BitFit: Simple Parameter-efficient Fine-tuning or Transformer-based Masked Language-models</strong>）是一种稀疏的微调方法，它训练时只更新bias的参数或者部分bias参数。</p><h2 id="_2、增加额外参数" tabindex="-1">2、增加额外参数 <a class="header-anchor" href="#_2、增加额外参数" aria-label="Permalink to &quot;2、增加额外参数&quot;">​</a></h2><p>Prefix Tuning（论文：<strong>Prefix-Tuning: Optimizing Continuous Prompts for Generation</strong>），在输入token之前构造一段任务相关的virtual tokens作为Prefix，然后训练的时候只更新Prefix部分的参数，而PLM中的其他部分参数固定。</p><p>Prompt Tuning（论文：<strong>The Power of Scale for Parameter-Efficient Prompt Tuning</strong>），该方法可以看作是Prefix Tuning的简化版本，它给每个任务定义了自己的Prompt，然后拼接到数据上作为输入，但<strong>只在输入层加入prompt tokens</strong>，并且不需要加入 MLP 进行调整来解决难训练的问题。冻结模型原始权重，只训练prompts参数，训练完以后，用同一个模型可以做多任务推理。</p><p>P-Tuning（论文：<strong>GPT Understands, Too</strong>），该方法将Prompt转换为可以学习的Embedding层，并用MLP+LSTM的方式来对Prompt Embedding进行一层处理。（同Prefix-Tuning类似）</p><p>P-Tuning v2（论文： <strong>P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks</strong>），该方法在每一层都加入了Prompts tokens作为输入，而不是仅仅加在输入层，这带来两个方面的好处：</p><ul><li>更多可学习的参数（从P-tuning和Prompt Tuning的0.01%增加到0.1%-3%），同时也足够参数高效。</li><li>加入到更深层结构中的Prompt能给模型预测带来更直接的影响。</li></ul><p>Adapter Tuning（论文：<strong>Parameter-Efficient Transfer Learning for NLP</strong>），该方法设计了Adapter结构，并将其嵌入Transformer的结构里面，针对每一个Transformer层，增加了两个Adapter结构（分别是多头注意力的投影之后和第二个feed-forward层之后），在训练时，固定住原来预训练模型的参数不变，只对新增的 Adapter 结构和 Layer Norm 层进行微调，从而保证了训练的高效性。</p><p>Adapter Fusion（论文：<strong>AdapterFusion:Non-Destructive Task Composition for Transfer Learning</strong>），一种融合多任务信息的Adapter的变体，在 Adapter 的基础上进行优化，通过将学习过程分为两阶段来提升下游任务表现。</p><ul><li>知识提取阶段：在不同任务下引入各自的Adapter模块，用于学习特定任务的信息。</li><li>知识组合阶段：将预训练模型参数与特定于任务的Adapter参数固定，引入新参数（AdapterFusion）来学习组合多个Adapter中的知识，以提高模型在目标任务中的表现。</li></ul><h2 id="_3、重新参数化" tabindex="-1">3、重新参数化 <a class="header-anchor" href="#_3、重新参数化" aria-label="Permalink to &quot;3、重新参数化&quot;">​</a></h2><p>LoRA（论文：<strong>LoRA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS</strong>），该方法的核心思想就是通过低秩分解来模拟参数的改变量，从而以极小的参数量来实现大模型的间接训练。</p><ul><li>先增加参数（学术上叫低秩分解矩阵，实际是自编码器结构的神经网络）进行微调。</li><li>微调结束后，将低秩分解矩阵的权重 加到 预训练模型上，这样不会改变预训练模型结构。</li></ul><p>AdaLoRA（论文：<strong>ADAPTIVE BUDGET ALLOCATION FOR PARAMETEREFFICIENT FINE-TUNING</strong>），是对LoRA的一种改进，它根据重要性评分动态分配参数预算给权重矩阵。具体做法如下：</p><ul><li><strong>调整增量矩分配</strong>。AdaLoRA将关键的增量矩阵分配高秩以捕捉更精细和任务特定的信息，而将较不重要的矩阵的秩降低，以防止过拟合并节省计算预算。</li><li><strong>以奇异值分解的形式对增量更新进行参数化，并根据重要性指标裁剪掉不重要的奇异值，同时保留奇异向量</strong>。由于对一个大矩阵进行精确SVD分解的计算消耗非常大，这种方法通过减少它们的参数预算来加速计算，同时，保留未来恢复的可能性并稳定训练。</li><li><strong>调整增量矩分配</strong>。AdaLoRA将关键的增量矩阵分配高秩以捕捉更精细和任务特定的信息，而将较不重要的矩阵的秩降低，以防止过拟合并节省计算预算。</li><li><strong>以奇异值分解的形式对增量更新进行参数化，并根据重要性指标裁剪掉不重要的奇异值，同时保留奇异向量</strong>。由于对一个大矩阵进行精确SVD分解的计算消耗非常大，这种方法通过减少它们的参数预算来加速计算，同时，保留未来恢复的可能性并稳定训练。</li></ul><p>QLoRA（论文： <strong>QLORA: Efficient Finetuning of Quantized LLMs</strong>），使用一种新颖的高精度技术将预训练模型量化为 4 bit，然后添加一小组可学习的低秩适配器权重，这些权重通过量化权重的反向传播梯度进行微调。QLORA 有一种低精度存储数据类型（4 bit），还有一种计算数据类型（BFloat16）。实际上，这意味着无论何时使用 QLoRA 权重张量，我们都会将张量反量化为 BFloat16，然后执行 16 位矩阵乘法。QLoRA提出了两种技术实现高保真 4 bit微调——4 bit NormalFloat(NF4) 量化和双量化。此外，还引入了分页优化器，以防止梯度检查点期间的内存峰值，从而导致内存不足的错误，这些错误在过去使得大型模型难以在单台机器上进行微调。具体说明如下：</p><ul><li><strong>4bit NormalFloat</strong>（NF4）：对于正态分布权重而言，一种信息理论上最优的新数据类型，该数据类型对正态分布数据产生比 4 bit整数和 4bit 浮点数更好的实证结果。</li><li><strong>双量化</strong>（<strong>Double Quantization</strong>）：对第一次量化后的那些常量再进行一次量化，减少存储空间。</li><li><strong>分页优化器</strong>：使用NVIDIA统一内存特性，该特性可以在在GPU偶尔OOM的情况下，进行CPU和GPU之间自动分页到分页的传输，以实现无错误的 GPU 处理。该功能的工作方式类似于 CPU 内存和磁盘之间的常规内存分页。使用此功能为优化器状态（Optimizer）分配分页内存，然后在 GPU 内存不足时将其自动卸载到 CPU 内存，并在优化器更新步骤需要时将其加载回 GPU 内存。</li></ul>',28),s=[o];function l(p,g,d,u,P,c){return r(),e("div",null,s)}const f=t(i,[["render",l]]);export{A as __pageData,f as default};
