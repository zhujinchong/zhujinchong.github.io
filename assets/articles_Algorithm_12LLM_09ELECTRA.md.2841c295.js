import{_ as e,o as t,c as a,Q as o}from"./chunks/framework.2516552c.js";const r="/assets/v2-2d8c42a08e37369b3b504ee006d169a3_hd.b3e677a5.jpg",T=JSON.parse('{"title":"","description":"","frontmatter":{},"headers":[],"relativePath":"articles/Algorithm/12LLM/09ELECTRA.md","filePath":"articles/Algorithm/12LLM/09ELECTRA.md","lastUpdated":null}'),n={name:"articles/Algorithm/12LLM/09ELECTRA.md"},s=o('<p>参考： <a href="https://zhuanlan.zhihu.com/p/89763176" target="_blank" rel="noreferrer">https://zhuanlan.zhihu.com/p/89763176</a></p><p>2019/9 斯坦福SALL实验室《 Efficiently Learning an Encoder that Classifies Token Replacements Accurately 》</p><p>创新点：</p><ol><li>GAN架构</li><li>仅用1/4算力达到了RoBERTa的效果</li></ol><h3 id="模型" tabindex="-1">模型 <a class="header-anchor" href="#模型" aria-label="Permalink to &quot;模型&quot;">​</a></h3><p>ELECTRA最主要的贡献是提出了新的预训练任务和框架，把生成式的Masked language model(MLM)预训练任务改成了判别式的Replaced token detection(RTD)任务，判断当前token是否被语言模型替换过。那么问题来了，我随机替换一些输入中的字词，再让BERT去预测是否替换过可以吗？可以的，因为我就这么做过，但效果并不好，因为随机替换<strong>太简单了</strong>。</p><p>于是作者就干脆使用一个MLM的G-BERT来对输入句子进行更改，然后丢给D-BERT去判断哪个字被改过，如下：</p><p><img src="'+r+'" alt="img"></p>',8),l=[s];function i(c,p,_,d,h,m){return t(),a("div",null,l)}const L=e(n,[["render",i]]);export{T as __pageData,L as default};
