import{_ as e,o as t,c as a,Q as r}from"./chunks/framework.2516552c.js";const s="/assets/image-20240317205517236.2bc12ff4.png",S=JSON.parse('{"title":"","description":"","frontmatter":{},"headers":[],"relativePath":"articles/Algorithm/13LLM/45Mamba.md","filePath":"articles/Algorithm/13LLM/45Mamba.md","lastUpdated":null}'),n={name:"articles/Algorithm/13LLM/45Mamba.md"},o=r('<p><a href="https://mp.weixin.qq.com/s/ORZCxa97y-Hn8yme97eBTA" target="_blank" rel="noreferrer">UCLA华人提出全新自我对弈机制！LLM自己训自己，效果碾压GPT-4专家指导 (qq.com)</a></p><p><a href="https://mp.weixin.qq.com/s/9mcccgEICn-jU6Xl4vFltg" target="_blank" rel="noreferrer">微软这篇论文把LLM的上下文扩展到了200万tokens (qq.com)</a></p><p><a href="https://blog.csdn.net/v_JULY_v/article/details/134923301" target="_blank" rel="noreferrer">一文通透想颠覆Transformer的Mamba：从SSM、HiPPO、S4到Mamba_mamba模型-CSDN博客</a></p><p>Mamba: Linear-Time Sequence Modeling with Selective State Spaces</p><p>时间：2023-12</p><p>背景:</p><p>为了解决 Transformer 在长序列上的计算效率低下问题，人们开发了许多亚二次时间架构，如线性注意力、门控卷积和递归模型以及结构化状态空间模型（SSM）。此类模型的一个关键弱点是<strong>无法进行基于内容的推理</strong>。</p><p>其中SSMs最有潜力，它可以被解释为<strong>递归神经网络（RNN）和卷积神经网络（CNN）的结合</strong>，其灵感来自经典的状态空间模型（卡尔曼，1960）。但它们在离散和信息密集数据（如文本）的建模方面不太有效。</p><p><img src="'+s+'" alt="image-20240317205517236"></p>',9),p=[o];function c(m,_,i,l,g,d){return t(),a("div",null,p)}const h=e(n,[["render",c]]);export{S as __pageData,h as default};
