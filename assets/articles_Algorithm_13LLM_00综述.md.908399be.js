import{_ as e,o as r,c as a,Q as t}from"./chunks/framework.2516552c.js";const m=JSON.parse('{"title":"","description":"","frontmatter":{},"headers":[],"relativePath":"articles/Algorithm/13LLM/00综述.md","filePath":"articles/Algorithm/13LLM/00综述.md","lastUpdated":null}'),p={name:"articles/Algorithm/13LLM/00综述.md"},o=t('<p>2017/12 Tranformer</p><p>《Attention Is All You Need》</p><p>2018/6 GPT</p><p>《Improving Language Understanding by Generative Pre-Training》</p><p><a href="https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf" target="_blank" rel="noreferrer">https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf</a></p><p>2018/10 BERT</p><p>《BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding》</p><p>2019/2 GPT2.0</p><p>《 Language Models are Unsupervised Multitask Learners 》</p><p><a href="https://www.techbooky.com/wp-content/uploads/2019/02/Better-Language-Models-and-Their-Implications.pdf" target="_blank" rel="noreferrer">https://www.techbooky.com/wp-content/uploads/2019/02/Better-Language-Models-and-Their-Implications.pdf</a></p><p>2019/6 Tranformer-XL</p><p>《transformer-xl attentive language models beyond a fixed-length context》</p><p><a href="https://arxiv.org/abs/1901.02860" target="_blank" rel="noreferrer">https://arxiv.org/abs/1901.02860</a></p><p>2019/6 XLNet</p><p>《XLNet: Generalized Autoregressive Pretraining for Language Understanding》</p><p><a href="https://arxiv.org/abs/1906.08237" target="_blank" rel="noreferrer">https://arxiv.org/abs/1906.08237</a></p><p>2019/7 RoBERTa</p><p>RoBERTa: A Robustly Optimized BERT Pretraining Approach</p><p><a href="https://arxiv.org/abs/1907.11692" target="_blank" rel="noreferrer">https://arxiv.org/abs/1907.11692</a></p><p>2019/9 ELECTRA</p><p>ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators</p><p><a href="https://openreview.net/forum?id=r1xMH1BtvB" target="_blank" rel="noreferrer">https://openreview.net/forum?id=r1xMH1BtvB</a></p><p>2019/10 DistialBERT (from HuggingFace)</p><p>DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter</p><p><a href="https://arxiv.org/abs/1910.01108v1" target="_blank" rel="noreferrer">https://arxiv.org/abs/1910.01108v1</a></p><p>2019/9 ALBERT (from Google)</p><p>ALBERT: A Lite BERT for Self-supervised Learning of Language Representations</p><p><a href="https://arxiv.org/abs/1909.11942v1" target="_blank" rel="noreferrer">https://arxiv.org/abs/1909.11942v1</a></p><p>2019/10 T5 (from Google)</p><p>Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</p><p><a href="https://arxiv.org/abs/1910.10683v1" target="_blank" rel="noreferrer">https://arxiv.org/abs/1910.10683v1</a></p><p>2019/9 MMBT (from Facebook)</p><p>Supervised Multimodal Bitransformers for Classifying Images and Text</p><p><a href="https://arxiv.xilesou.top/abs/1909.02950" target="_blank" rel="noreferrer">https://arxiv.xilesou.top/abs/1909.02950</a></p><p>XLNET，以及UNILM、MASS、MT-DNN、XLM，都是基于这种思路的扩充，解决相应的任务各有所长。其中微软研究院的UNILM可同时训练得到类似BERT和GPT的模型，而微软MASS采用encoder-decoder训练在机器翻译上效果比较好。还有MT-DNN强调用多任务学习预训练模型，而XLM学习多语言BERT模型，在跨语言迁移学习方面应用效果显著。</p><p>MT-DNN-SMART_Microsoft D365</p><p>ALICE_v2 王玮</p><p>ERNIE_Baidu</p><p><a href="https://github.com/PaddlePaddle/ERNIE" target="_blank" rel="noreferrer">https://github.com/PaddlePaddle/ERNIE</a></p>',39),n=[o];function s(i,l,g,d,f,h){return r(),a("div",null,n)}const T=e(p,[["render",s]]);export{m as __pageData,T as default};
