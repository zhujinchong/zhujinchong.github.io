import{_ as a,o,c as e,Q as s}from"./chunks/framework.2516552c.js";const p="/assets/image-20240225100416552.51eee24c.png",r="/assets/image-20240225100658909.4ea8a4bd.png",t="/assets/image-20240225102246303.b36fd8de.png",i="/assets/image-20240225102722066.67c517c7.png",n="/assets/image-20240225105529227.ff8e7e55.png",l="/assets/image-20240225110728040.960dfca2.png",c="/assets/image-20240225111754340.a5f6115c.png",g="/assets/image-20240225204827682.d74847a7.png",m="/assets/image-20240225204843086.b1796678.png",h="/assets/image-20240317125543314.a18e684e.png",N="/assets/image-20240317120344248.0cfaa4b9.png",d="data:image/png;base64,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",x="/assets/image-20240317113605821.77bc93c6.png",f="/assets/image-20240317113633389.bf43acdc.png",u="/assets/image-20240317114151843.c8b9328c.png",P="/assets/v2-4b15828dfee19be726835b671748cc4d_720w.ae01b4bf.webp",b="/assets/image-20240317131330342.3deaa7a9.png",R="/assets/image-20240225114758871.71ec9d43.png",C="/assets/image-20240317125306572.a5c24bbf.png",O="/assets/image-20240317134635248.551a959b.png",S="/assets/image-20240225204854660.8c6af3ec.png",A="/assets/image-20240225204904971.664c6a36.png",v="/assets/image-20240226090814547.3d73b691.png",F=JSON.parse('{"title":"传统目标检测算法","description":"","frontmatter":{},"headers":[],"relativePath":"articles/Algorithm/14LMM/03目标检测RCNN_YOLO_SSD.md","filePath":"articles/Algorithm/14LMM/03目标检测RCNN_YOLO_SSD.md","lastUpdated":null}'),L={name:"articles/Algorithm/14LMM/03目标检测RCNN_YOLO_SSD.md"},q=s('<blockquote><p><a href="https://blog.csdn.net/v_july_v/article/details/80170182" target="_blank" rel="noreferrer">https://blog.csdn.net/v_july_v/article/details/80170182</a></p></blockquote><h1 id="传统目标检测算法" tabindex="-1">传统目标检测算法 <a class="header-anchor" href="#传统目标检测算法" aria-label="Permalink to &quot;传统目标检测算法&quot;">​</a></h1><p>这个任务本质上就是这两个问题：一：图像识别，二：定位。</p><p>一：图像识别：输入是图片，输出是类别。</p><p>二：定位：输入是图片，输出是方框的位置（x,y,w,h）</p><p>为什么要图像识别？：因为regression的训练参数收敛的时间要长得多，所以通常采取了用classification的网络来计算出网络共同部分的连接权值。（这一步可做可不做）</p><p><img src="'+p+'" alt="image-20240225100416552"></p><p>思路一：回归问题</p><p>我们只需要预测出（x,y,w,h）四个参数的值，从而得出方框的位置。（图像分类是为了加速训练，可做可不做）</p><p>也叫一阶段任务：不提取候选框，直接做回归。如YOLO,SSD等</p><p><img src="'+r+'" alt="image-20240225100658909"></p><p>思路二：图像窗口</p><p>使用候选区域（region proposal method）创建目标检测感兴趣的区域ROI，将图片截取出来，输入到CNN，然后CNN会输出这个框的得分（classification）以及这个框图片对应的x,y,h,w（regression）。</p><p>也叫两个阶段任务：先提取候选框，然后再分类or回归。如R-CNN系列算法都是这种思路。</p><p><img src="'+t+'" alt="image-20240225102246303"></p><p>所以，传统目标检测的主要问题是：</p><ul><li>基于滑动窗口的区域选择策略没有针对性，<strong>时间复杂度高</strong>，窗口冗余</li><li>手工设计的特征对于多样性的变化没有很好的鲁棒性</li></ul><h1 id="r-cnn" tabindex="-1">R-CNN <a class="header-anchor" href="#r-cnn" aria-label="Permalink to &quot;R-CNN&quot;">​</a></h1><p>2014年，RBG（Ross B. Girshick）</p><p>思路：预先找出图中目标可能出现的位置，即候选区域（Region Proposal）。利用图像中的纹理、边缘、颜色等信息，可以保证在选取较少窗口(几千甚至几百）的情况下保持较高的召回率（Recall）。</p><p>方法：对于每张图片，RCNN首先采用选择性搜索算法Selective Search生成大约2000个候选区域。随后将每个候选区域的尺寸转换为固定大小227x227输入到CNN，将CNN的fc7层的输出作为候选区域的特征。随后使用SVM分类器判断候选区域的类别，再使用线性回归模型为每个物体生成更精确的边界框（精细修正候选框位置）。</p><p>缺点：</p><ol><li>CNN分类需要对图片warp/crop成统一尺寸227x227，使图像变形失真，减低了识别精度。SPP Net解决了。</li><li>选择性搜索算法Selective Search生成大约2000个候选区域，这个也非常耗时。Faster R-CNN解决了。</li><li>计算所有region proposal进行特征提取时会有重复计算，计算量很大，导致R-CNN检测速度很慢，一张图都需要47s。Fast R-CNN解决了。</li></ol><p><img src="'+i+'" alt="image-20240225102722066"></p><h1 id="spp-net" tabindex="-1">SPP Net <a class="header-anchor" href="#spp-net" aria-label="Permalink to &quot;SPP Net&quot;">​</a></h1><p>SPP：Spatial Pyramid Pooling（空间金字塔池化）</p><p>SPP-Net是出自2015年发表在IEEE上的论文-《Spatial Pyramid Pooling in Deep ConvolutionalNetworks for Visual Recognition》。</p><p>CNN存在的问题：CNN一般都含有卷积部分和全连接部分，其中，<strong>卷积层</strong>不需要固定尺寸的图像，而<strong>全连接层</strong>是需要固定大小的输入。所以输入图片要求warp/crop成统一尺寸。</p><p>思考：卷积层可以适应任何尺寸，为何不能在卷积层的最后加入某种结构，使得后面全连接层得到的输入变成固定的呢？</p><p>方法：输入h和w的尺寸不是固定的，输出是固定的nxn，那么max pooling的核大小和步长可以通过公式计算得到。<a href="https://cloud.tencent.com/developer/article/1076488" target="_blank" rel="noreferrer">空间金字塔池化(Spatial Pyramid Pooling, SPP)原理和代码实现(Pytorch)-腾讯云开发者社区-腾讯云 (tencent.com)</a></p><p><img src="'+n+'" alt="image-20240225105529227"></p><p>SPP Net在普通的CNN结构中加入了ROI池化层（ROI Pooling），使得网络的输入图像可以是任意尺寸的，输出则不变，同样是一个固定维数的向量。在实验中，CNN输出尺寸不是固定的，但是通道是固定的256。SSP Net采用了三个固定尺寸的特征图，即4x4,2x2,1x1，通道256是固定的。最后Flatten成一维21x255全连层。</p><p><img src="'+l+'" alt="image-20240225110728040"></p><p>如此这般，R-CNN要对每个区域计算卷积，而SPPNet只需要计算一次卷积，从而节省了大量的计算时间，比R-CNN有一百倍左右的提速。</p><h1 id="fast-r-cnn" tabindex="-1">Fast R-CNN <a class="header-anchor" href="#fast-r-cnn" aria-label="Permalink to &quot;Fast R-CNN&quot;">​</a></h1><p>SPP Net真是个好方法，R-CNN的进阶版Fast R-CNN就是在R-CNN的基础上采纳了SPP Net方法，对R-CNN作了改进，使得性能进一步提高。</p><p>R-CNN的缺点：计算所有region进行特征提取时会有重复计算，Fast-RCNN正是为了解决这个问题诞生的。</p><p>方法：</p><p>1、最后一个卷积层后加了一个ROI pooling layer，实际上是SPP-NET的一个精简版，只需要下采样到一个7x7的特征图。（SPP-Net下采样了三个尺寸的特征图）</p><p>2、直接使用softmax替代SVM分类。</p><p>3、对图片进行一次特征计算，得到图片特征（包含了所关注区域的特征）。与候选区域的相对位置结合，就可以得到候选区域的特征。随后使用ROI池化层将候选区域调整至相同尺寸，将多任务损失函数边框回归加入到了网络。（multi-task模型）</p><p><img src="'+c+'" alt="image-20240225111754340"></p><h1 id="faster-r-cnn" tabindex="-1">Faster R-CNN <a class="header-anchor" href="#faster-r-cnn" aria-label="Permalink to &quot;Faster R-CNN&quot;">​</a></h1><blockquote><p><a href="https://zhuanlan.zhihu.com/p/31426458" target="_blank" rel="noreferrer">https://zhuanlan.zhihu.com/p/31426458</a></p></blockquote><p>Fast R-CNN存在的问题：选择性搜索，找出所有的候选框，这个也非常耗时。那我们能不能找出一个更加高效的方法来求出这些候选框呢？</p><p>解决：加入一个提取边缘的神经网络Region Proposal Network(RPN)，也就说找到候选框bounding box的工作也交给神经网络来做了。同时引入anchor box应对目标形状的变化问题（anchor就是位置和大小固定的box，可以理解成事先设置好的固定大小的patch）。</p><p><img src="'+g+'" alt="image-20240225204827682"></p><p>Faster RCNN其实可以分为4个主要内容：</p><ol><li>Conv layers。作为一种CNN网络目标检测方法，Faster RCNN首先使用一组基础的conv+relu+pooling层提取image的feature maps。该feature maps被共享用于后续RPN层和全连接层。(输入和输出尺寸是固定的)</li><li>Region Proposal Networks。RPN网络用于生成region proposals。该层通过softmax判断anchors属于positive或者negative，再利用bounding box regression修正anchors获得精确的proposals。</li><li>RoI Pooling。该层收集输入的feature maps和proposals，综合这些信息后提取proposal feature maps，送入后续全连接层判定目标类别。</li><li>Classification。利用proposal feature maps计算proposal的类别，同时再次bounding box regression获得检测框最终的精确位置。</li></ol><p><img src="'+m+'" alt="image-20240225204843086"></p><p>一种网络，四个损失函数; 　　• RPN calssification(anchor good.bad)： 　　• RPN regression(anchor-&gt;propoasal)： 　　• Fast R-CNN classification(over classes)： 　　• Fast R-CNN regression(proposal -&gt;box)：</p><p>看下整体结构：(softmax前后有两个reshape可以忽略，这是因为caffe框架自身问题)</p><p><img src="'+h+'" alt="image-20240317125543314"></p><h2 id="conv-layers" tabindex="-1">Conv Layers <a class="header-anchor" href="#conv-layers" aria-label="Permalink to &quot;Conv Layers&quot;">​</a></h2><p>在Faster RCNN Conv layers中对所有的卷积都做了扩边处理（ pad=1，即填充一圈0），导致原图变为 (M+2)x(N+2)大小，再做3x3卷积后输出MxN 。正是这种设置，导致Conv layers中的conv层不改变输入和输出矩阵大小。</p><p><img src="'+N+'" alt="image-20240317120344248"></p><p>这一步得到feature map，feautre map只是下采样，但不会改变图像宽高比。</p><p>那么，一个MxN大小的矩阵经过Conv layers固定变为(M/16)x(N/16)！</p><h2 id="region-proposal-networks" tabindex="-1">Region Proposal Networks <a class="header-anchor" href="#region-proposal-networks" aria-label="Permalink to &quot;Region Proposal Networks&quot;">​</a></h2><p>传统的滑动窗口和SS(Selective Search)方法都很耗时。</p><p>Faster R-CNN做法：类似于滑动窗口，但是RPN通过使用固定大小的anchor（参考框）来生成不同尺寸的bounding box（边界框）。</p><p>Anchor要做两个事情，所以，这里需要两个目标函数。</p><p>1、某个框内是否含有物体（good or bad）。</p><p>2、某个框是否框的准，如果框的不准我们要如何调整框（偏移量）。以前大家认为回归任务需要预测<img src="'+d+'" alt="image-20240317113443512">，论文提出参考框这样表示<img src="'+x+'" alt="image-20240317113605821">。所以，预测偏移量<img src="'+f+'" alt="image-20240317113633389">，用数学语言来表示这个偏移就是ground-truth真实物体。</p><p>Anchor：</p><p>Anchor是一组固定的框，它们以不同的大小以及宽高比放在整个图像当中。</p><p><img src="'+u+'" alt="image-20240317114151843"></p><p>论文中设置了3种大小，3种比例1:1,1:2,2:1，一共9个预定义的anchors。需要注意anchors size是针对具体图像尺寸设置的！例如针对检测图像（800x600）设置，anchors中长宽1:2中最大为352x704，长宽2:1中最大736x384，基本cover了800x600的各个尺度和形状。</p><p>计算feature map上一共多少anchor?</p><p><img src="'+P+'" alt="img"></p><p>特征图上每个像素都对应了k个anchors，会产生大量anchors。800x600经过VGG下采样16倍后有1900个像素点。feature map每个点设置9个Anchor，则一共会产生50 x 38 x 9 = 17100个anchor。后面NMS去除冗余窗口。选取128个postive anchors+128个negative anchors进行训练。</p><p>RPN后期处理：</p><p>1、按照输入的positive softmax scores由大到小排序anchors，提取前n个</p><p>2、限定超出图像边界的positive anchors为图像边界，防止后续roi pooling时proposal超出图像边界</p><p>3、忽略掉长或者宽太小的建议框</p><p>4、对剩余的positive anchors进行NMS（nonmaximum suppression）</p><p>5、最后重新再按得分排序，取得分前N个框</p><p>再看下RPN整体结构：（假设原图是MxN，Conv Layers下采样后是WxH）</p><p>1、上面1x1x18卷积核，会把图像变成WxHx18，18是因为有9个Anchor，分类又各需要两个输出。</p><p>2、下面1x1x36卷积核，会把图像变成WxHx36，36是因为有9个Anchor，回归又各需要四个输出。</p><p><img src="'+b+'" alt="image-20240317131330342"></p><p>anchor的输出</p><p><img src="'+R+'" alt="image-20240225114758871"></p><p>对于每个Proposal，我们首先使用一个3×3的卷积层，然后使用两个并行的1×1的卷积内核，k是Anchor数量，2k用于分类，4k用于回归。</p><p>这里的3x3卷积是为了融合更多空间信息，这里的256只是feautre map的每个点的维度，没有什么含义。当然，最后肯定是2k和4k的输出。</p><h2 id="roi-pooling" tabindex="-1">RoI Pooling <a class="header-anchor" href="#roi-pooling" aria-label="Permalink to &quot;RoI Pooling&quot;">​</a></h2><p>Rol pooling层有2个输入：</p><ol><li>原始的feature maps</li><li>RPN输出的proposal boxes（大小各不相同）</li></ol><p>RoI Pooling layer forward过程：</p><p>我们将得到的建议框对应的feature map裁剪出来，这时候不同的建议框对应裁剪出来的feature map的尺寸是不一样的，但是我们把这些尺寸不一的feature map统一划分为7x7块，对每块使用max_pool池化，这样最终获得的feature map的尺寸就统一了。</p><p><img src="'+C+'" alt="image-20240317125306572"></p><h2 id="classification" tabindex="-1">Classification <a class="header-anchor" href="#classification" aria-label="Permalink to &quot;Classification&quot;">​</a></h2><p>从RoI Pooling获取到7x7=49大小的proposal feature maps后，送入后续网络，可以看到做了如下2件事：</p><ol><li>通过全连接和softmax对proposals进行分类，这实际上已经是识别的范畴了</li><li>再次对proposals进行bounding box regression，获取更高精度的rect box</li></ol><p><img src="'+O+'" alt="image-20240317134635248"></p><h1 id="yolo" tabindex="-1">YOLO <a class="header-anchor" href="#yolo" aria-label="Permalink to &quot;YOLO&quot;">​</a></h1><p>CVPR2016</p><p>Faster R-CNN的方法目前是主流的目标检测方法，但是速度上并不能满足实时的要求。YOLO一类的方法慢慢显现出其重要性，这类方法使用了回归的思想，利用整张图作为网络的输入，直接在图像的多个位置上回归出这个位置的目标边框，以及目标所属的类别。</p><p>(1) 给个一个输入图像，首先将图像划分成7x7的网格 (2) 对于每个网格，我们都预测2个边框（包括每个边框是目标的置信度以及每个边框区域在多个类别上的概率） (3) 根据上一步可以预测出7x7x2个目标窗口，然后根据阈值去除可能性比较低的目标窗口，最后NMS去除冗余窗口即可</p><p><img src="'+S+'" alt="image-20240225204854660"></p><p>优点：YOLO将目标检测任务转换成一个回归问题，大大加快了检测的速度，使得YOLO可以每秒处理45张图像。而且由于每个网络预测目标窗口时使用的是全图信息，使得false positive比例大幅降低（充分的上下文信息）。</p><p>缺点：没有了Region Proposal机制，只使用7x7的网格回归会使得目标不能非常精准的定位，这也导致了YOLO的检测精度并不是很高。</p><h1 id="ssd" tabindex="-1">SSD <a class="header-anchor" href="#ssd" aria-label="Permalink to &quot;SSD&quot;">​</a></h1><p>YOLO缺点：没有了Region Proposal机制，只使用7x7的网格回归会使得目标不能非常精准的定位，这也导致了YOLO的检测精度并不是很高。</p><p>思路：结合YOLO的回归思想以及Faster R-CNN的anchor机制。</p><p>YOLO预测某个位置使用的是全图的特征，SSD预测某个位置使用的是这个位置周围的特征（感觉更合理一些）。</p><p><img src="'+A+'" alt="image-20240225204904971"></p><h1 id="detr" tabindex="-1">DETR <a class="header-anchor" href="#detr" aria-label="Permalink to &quot;DETR&quot;">​</a></h1><p>一般目标检测的任务是预测一系列的Bounding Box的坐标以及Label，而大多数检测器的具体做法是</p><ul><li>要么基于proposal，比如RCNN系列的工作，类似Faster R-CNN、Mask R-CNN</li><li>要么基于anchor，比如YOLO</li></ul><p>把问题构建成为一个分类和回归问题来间接地完成这个任务，但最后都会生成很多个预测框(确定框的坐标及框内是什么物体)，从而不可避免的出现很多冗余的框，而要去除这些冗余的框，则都需要做一个NMS(non-maximum suppersion，非极大值抑制)的后处理(使得最后调参不易、部署不易)，所以如果要是有一个端对端的模型，不需要做NMS之类的后处理 也不需要太多先验知识则该有多好。</p><p>而通过论文《End-to-End Object Detection with Transformers》提出的DETR则满足了大家这个期待，其取代了现在的模型需要手工设计的工作，效果不错且可扩展性强(在DETR上加个专用的分割头便可以做全景分割)，其解决的方法是把检测问题看做是一个集合预测的问题(即set prediction problem，说白了，各种预测框本质就是一个集合)，其基本流程如下图所示 <img src="'+v+'" alt="image-20240226090814547"></p><p>1、CNN抽特征且拉直 2、全局建模，给到transformer-encoder去进一步学习全局信息 通过借助Transformer中的的self-attention机制，可以显式地对一个序列中的所有elements两两之间的interactions进行建模或交互，如此就知道了图片中哪块是哪个物体，从而对于同一个物体只需出一个预测框即可 3、接着通过不带掩码机制的transformer-decoder生成很多预测框 注意是并行预测(即并行出框，而不是像原始transformer预测下一个token时一个一个往外蹦)，相当于一次性生成 N 个box prediction，其中 N是一个事先设定的远远大于image中object个数的一个整数(比如100) 4、预测框和真实框做二分图匹配 最后通过bipartite matching loss的方法，基于预测的100个box和ground truth box的二分图做匹配，计算loss的大小，从而使得预测的box的位置和类别更接近于ground truth。当然，这第4步更多是做模型训练的时候用，如果模型训练好了去做推理时，该第4步就不需要了，可以直接在预测的100个框中设定个阈值，比如置信度大于0.7的预测框保留下来，其他视为背景物体而 舍弃。</p>',113),Y=[q];function E(k,V,X,I,D,_){return o(),e("div",null,Y)}const Z=a(L,[["render",E]]);export{F as __pageData,Z as default};
