import{_ as e,o as s,c as a,Q as t}from"./chunks/framework.2516552c.js";const n="/assets/v2-cbc067b9d12ad2c25aff103be299bf94_720w.f3b95070.webp",o="/assets/v2-6267e80bfe5730f52aa20f8f4f248672_720w.feb7ea7f.webp",p="/assets/v2-e8ec5f55d7ea46c506f709219e9c6eb6_720w.b1a5466b.webp",r="/assets/v2-8cf1a63bd06f0fa7b4b05d331f663550_720w.0bbad838.webp",i="/assets/v2-4632cb6be013a8b0aa27e812d620cd5f_720w.2c8ecf1d.webp",l="/assets/v2-709e01c5cbf610cd85d52bd63ce0df4f_720w.c399a48f.webp",c="/assets/v2-086fc4db6ac0ec2a76f0e5d0c5c37b4f_720w.98c84f73.webp",d="/assets/v2-ca66e9375558b5e04cd9c76fa0f6d122_720w.59ab365f.webp",f="/assets/v2-065321f6d060da5503ff8311de1e6b5a_720w.8bfb0214.webp",g="/assets/v2-a643ee39e80807d6b7236d15f1c289a8_720w.8029e12e.webp",m="/assets/v2-a390d53cc59c0e76b0bbc86864f226ac_720w.fdd71171.webp",u="/assets/v2-8fafb5695089ea1d9fa8a5217877bd65_720w.44037e26.webp",b="/assets/v2-17b7c75d9f4a693f3711d602d8e971ca_720w.e22dd190.webp",h="/assets/image-20231124142400350.1e0545f0.png",_="/assets/v2-858572f82c893e361004763afdfa0586_720w.4990e7ae.webp",U=JSON.parse('{"title":"SD原理","description":"","frontmatter":{},"headers":[],"relativePath":"articles/Algorithm/14LLMV/02StableDiffusion.md","filePath":"articles/Algorithm/14LLMV/02StableDiffusion.md","lastUpdated":null}'),S={name:"articles/Algorithm/14LLMV/02StableDiffusion.md"},D=t('<p>时间：2022年8月</p><p>机构：Stability AI</p><h1 id="sd原理" tabindex="-1">SD原理 <a class="header-anchor" href="#sd原理" aria-label="Permalink to &quot;SD原理&quot;">​</a></h1><h2 id="通俗易懂" tabindex="-1">通俗易懂 <a class="header-anchor" href="#通俗易懂" aria-label="Permalink to &quot;通俗易懂&quot;">​</a></h2><p>SD可以文生图、图生图、图片修复inpainting。</p><p>1、如果是文生图：输入文本信息，再用random函数生成一个Latent Feature作为输入。</p><p>2、如果是图生图：输入文本信息，将原图片通过图像编码器（VAE Encoder）生成Latent Feature（隐空间特征）作为输入。</p><p><img src="'+n+'" alt="img"></p><p>SD有三大模块：CLIP、UNet、VAE</p><p>1、CLIP Text Encoder负责文本嵌入。嵌入向量辅助UNet生图。</p><p>2、VAE负责将图片编码或解码。编码：将图像压缩到Latent Feature，解码：将图像从Latent Feature重建出来。</p><p>3、U-Net网络负责预测噪声，不断优化生成过程，在预测噪声的同时不断注入文本语义信息。而schedule算法对每次U-Net预测的噪声进行优化处理（动态调整预测的噪声，控制U-Net预测噪声的强度），从而统筹生成过程的进度。在SD中，U-Net的迭代优化步数大概是50或者100次，在这个过程中Latent Feature的质量不断的变好（纯噪声减少，图像语义信息增加，文本语义信息增加）。</p><p><img src="'+o+'" alt="img"></p><h2 id="核心原理" tabindex="-1">核心原理 <a class="header-anchor" href="#核心原理" aria-label="Permalink to &quot;核心原理&quot;">​</a></h2><p>与GAN等生成式模型一致的是，SD模型同样学习拟合训练集分布，并能够生成与训练集分布相似的输出结果，但与GAN相比，SD模型训练过程更稳定，而且具备更强的泛化性能。这些都归功于扩散模型中核心的<strong>前向扩散过程（forward diffusion process）<strong>和</strong>反向生成过程（reverse generation process）。</strong></p><p>此外，扩散模型早就有了，而Latent大大降低了显存。让SD变得易用。</p><p><strong>前向扩散</strong></p><p>在前向扩散过程中，SD模型持续对一张图像<strong>添加高斯噪声</strong>直至变成随机噪声矩阵。扩散是固定的，有Schedule算法进行控制。</p><p><strong>反向生成</strong></p><p>而在反向生成过程中，SD模型进行<strong>去噪声过程</strong>，将一个随机噪声矩阵逐渐去噪声直至生成一张图像。</p><p><img src="'+p+'" alt="img"></p><p>总结：</p><p><strong>SD模型是生成式模型</strong>，输入可以是图片，文本以及两者的结合，输出是生成的图片。</p><p><strong>SD模型属于扩散模型</strong>，扩散模型的整体逻辑的特点是过程分步化与可迭代，这给整个生成过程引入更多约束与优化提供了可能。</p><p><strong>SD模型是基于Latent的扩散模型</strong>，将输入数据压缩到Latent隐空间中，比起常规扩散模型，大幅提高计算效率的同时，降低了显存占用，成为了SD模型破圈的关键一招</p><h2 id="unet训练过程" tabindex="-1">UNet训练过程 <a class="header-anchor" href="#unet训练过程" aria-label="Permalink to &quot;UNet训练过程&quot;">​</a></h2><p><strong>【1】训练集加入噪声</strong></p><p>SD模型训练时，我们需要输入加噪的数据集，每一次迭代我们用random函数生成从强到弱各个强度的噪声，通常来说会生成0-1000一共1001种不同的噪声强度。</p><p><img src="'+r+'" alt="img"></p><p>在训练过程中，我们首先对干净样本进行加噪处理，采用多次逐步增加噪声的方式，直至干净样本转变成为纯噪声。</p><p><img src="'+i+'" alt="img"></p><p><strong>【2】训练中去噪</strong></p><p>使用U-Net预测噪声，并结合Schedule算法逐步去噪。</p><p><img src="'+l+'" alt="img"></p><p>如下图所示分为四个步骤：从训练集中选取一张加噪过的图片和噪声强度（timestep），然后将其输入到U-Net中，让U-Net预测噪声（下图中的Unet Prediction），接着再计算预测噪声与真实噪声的误差（loss），最后通过反向传播更新U-Net的参数。</p><p>输入加噪图片 - 输出预测噪声 = 实际图片</p><p><img src="'+c+'" alt="img"></p><p><strong>【3】语义信息对图片生成的控制</strong></p><p>在SD模型的训练中，每个训练样本都会对应一个标签，我们将对应标签通过CLIP Text Encoder输出Text Embeddings，并将Text Embeddings以<strong>Cross Attention</strong>的形式与U-Net结构耦合，使得每次输入的图片信息与文字信息进行融合训练，如下图所示：</p><p><img src="'+d+'" alt="img"></p><p><strong>【4】SD模型训练时的输入</strong></p><p>有了上面的介绍，<strong>我们在这里可以小结一下SD模型训练时的输入，一共有三个部分组成：图片，文本，噪声强度</strong>。其中图片和文本是固定的，而噪声强度在每一次训练参数更新时都会随机选择一个进行叠加。</p><p><img src="'+f+'" alt="img"></p><h1 id="sd网络结构详解" tabindex="-1">SD网络结构详解 <a class="header-anchor" href="#sd网络结构详解" aria-label="Permalink to &quot;SD网络结构详解&quot;">​</a></h1><h2 id="sd整体架构" tabindex="-1">SD整体架构 <a class="header-anchor" href="#sd整体架构" aria-label="Permalink to &quot;SD整体架构&quot;">​</a></h2><p><strong>Stable Diffusion模型整体上是一个End-to-End模型</strong>，主要由VAE（变分自编码器，Variational Auto-Encoder），U-Net以及CLIP Text Encoder三个核心组件构成。</p><p>在FP16精度下Stable Diffusion模型大小2G（FP32：4G），其中U-Net大小1.6G，VAE模型大小160M以及CLIP Text Encoder模型大小235M。其中U-Net结构包含约860M参数，以FP32精度下大小为3.4G左右。</p><p><img src="'+g+'" alt="img"></p><h2 id="vae" tabindex="-1">VAE <a class="header-anchor" href="#vae" aria-label="Permalink to &quot;VAE&quot;">​</a></h2><p>在Stable Diffusion中，VAE（变分自编码器，Variational Auto-Encoder）的Encoder（编码器）结构能将输入图像转换为低维Latent特征，并作为U-Net的输入。VAE的Decoder（解码器）结构能将低维Latent特征重建还原成像素级图像。</p><p><img src="'+m+'" alt="img"></p><h2 id="u-net" tabindex="-1">U-Net <a class="header-anchor" href="#u-net" aria-label="Permalink to &quot;U-Net&quot;">​</a></h2><p>在Stable Diffusion中，<strong>U-Net模型是一个关键核心</strong>，作为扩散模型其主要是<strong>预测噪声残差</strong>，并结合Sampling method（调度算法：PNDM，DDIM，K-LMS等）对输入的特征矩阵进行重构，<strong>逐步将其从随机高斯噪声转化成图片的Latent Feature</strong>。</p><p>Stable Diffusion中的U-Net，在传统深度学习时代的Encoder-Decoder结构的基础上，<strong>增加了ResNetBlock（包含Time Embedding）模块，Spatial Transformer（SelfAttention + CrossAttention + FeedForward）模块以及CrossAttnDownBlock，CrossAttnUpBlock和CrossAttnMidBlock模块</strong>。</p><p><img src="'+u+'" alt="img"></p><h2 id="clip" tabindex="-1">CLIP <a class="header-anchor" href="#clip" aria-label="Permalink to &quot;CLIP&quot;">​</a></h2><p><strong>CLIP模型包含Text Encoder和Image Encoder两个模型。<strong>在训练时，从训练集中随机取出一张图片和标签文本。CLIP模型的任务主要是通过Text Encoder和Image Encoder分别将标签文本和图片提取</strong>embedding向量</strong>，然后用<strong>余弦相似度</strong>来比较两个embedding向量的<strong>相似性</strong>，以判断随机抽取的标签文本和图片是否匹配，并进行梯度反向传播，不断进行优化训练。</p><p><img src="'+b+`" alt="img"></p><p>在Stable Diffusion中主要使用了Text Encoder模型。<strong>CLIP Text Encoder模型将输入的文本Prompt进行编码，转换成Text Embeddings（文本的语义信息）</strong>，通过前面一章节提到的U-Net网络中的CrossAttention模块嵌入Stable Diffusion中作为Condition，对生成图像的内容进行一定程度上的控制与引导，目前SD模型使用的的是CLIP ViT-L/14 中的Text Encoder模型。</p><p>CLIP ViT-L/14 中的Text Encoder是只包含Transformer结构的模型，一共由12个CLIPEncoderLayer模块组成，具体CLIP Text Encoder模型结构如下图所示。其中特征维度为768，token数量是77，<strong>所以输出的Text Embeddings的维度为77x768</strong>。</p><p><img src="https://pic3.zhimg.com/80/v2-46fcafb5a14d108cd29d2751e453a142_720w.webp" alt="img"></p><h1 id="应用" tabindex="-1">应用 <a class="header-anchor" href="#应用" aria-label="Permalink to &quot;应用&quot;">​</a></h1><h2 id="diffuers库" tabindex="-1">diffuers库 <a class="header-anchor" href="#diffuers库" aria-label="Permalink to &quot;diffuers库&quot;">​</a></h2><p>huaggingface开源的库。</p><p>安装</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki github-dark has-diff vp-code-dark"><code><span class="line"><span style="color:#e1e4e8;">pip install diffusers</span></span>
<span class="line"><span style="color:#e1e4e8;">pip install transformers==4.27.0 accelerate==0.12.0 safetensors==0.2.7 invisible_watermark</span></span></code></pre><pre class="shiki github-light has-diff vp-code-light"><code><span class="line"><span style="color:#24292e;">pip install diffusers</span></span>
<span class="line"><span style="color:#24292e;">pip install transformers==4.27.0 accelerate==0.12.0 safetensors==0.2.7 invisible_watermark</span></span></code></pre></div><p>从hugggingface下载模型</p><p>其中text_encoder，scheduler，unet，vae分别代表了上面讲到过的SD模型的核心结构。</p><p><img src="`+h+`" alt="image-20231124142400350"></p><p>代码</p><p>有时候我们运行完pipeline之后，会出现纯黑色图片，这表示我们本次生成的图片触发了NSFW机制，出现了一些违规的图片，我们可以修改seed重新进行生成。</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki github-dark has-diff vp-code-dark"><code><span class="line"><span style="color:#e1e4e8;">from diffusers import StableDiffusionPipeline</span></span>
<span class="line"><span style="color:#e1e4e8;"></span></span>
<span class="line"><span style="color:#e1e4e8;">#初始化SD模型，加载预训练权重</span></span>
<span class="line"><span style="color:#e1e4e8;">pipe = StableDiffusionPipeline.from_pretrained(&quot;runwayml/stable-diffusion-v1-5&quot;)</span></span>
<span class="line"><span style="color:#e1e4e8;">#使用GPU加速</span></span>
<span class="line"><span style="color:#e1e4e8;">pipe.to(&quot;cuda&quot;)</span></span>
<span class="line"><span style="color:#e1e4e8;">#如GPU的内存少于10GB，可以加载float16精度的SD模型</span></span>
<span class="line"><span style="color:#e1e4e8;">pipe = StableDiffusionPipeline.from_pretrained(&quot;runwayml/stable-diffusion-v1-5&quot;, revision=&quot;fp16&quot;, torch_dtype=torch.float16)</span></span>
<span class="line"><span style="color:#e1e4e8;">#接下来，我们就可以运行pipeline了</span></span>
<span class="line"><span style="color:#e1e4e8;">prompt = &quot;a photograph of an astronaut riding a horse&quot;</span></span>
<span class="line"><span style="color:#e1e4e8;">image = pipe(prompt).images[0]</span></span></code></pre><pre class="shiki github-light has-diff vp-code-light"><code><span class="line"><span style="color:#24292e;">from diffusers import StableDiffusionPipeline</span></span>
<span class="line"><span style="color:#24292e;"></span></span>
<span class="line"><span style="color:#24292e;">#初始化SD模型，加载预训练权重</span></span>
<span class="line"><span style="color:#24292e;">pipe = StableDiffusionPipeline.from_pretrained(&quot;runwayml/stable-diffusion-v1-5&quot;)</span></span>
<span class="line"><span style="color:#24292e;">#使用GPU加速</span></span>
<span class="line"><span style="color:#24292e;">pipe.to(&quot;cuda&quot;)</span></span>
<span class="line"><span style="color:#24292e;">#如GPU的内存少于10GB，可以加载float16精度的SD模型</span></span>
<span class="line"><span style="color:#24292e;">pipe = StableDiffusionPipeline.from_pretrained(&quot;runwayml/stable-diffusion-v1-5&quot;, revision=&quot;fp16&quot;, torch_dtype=torch.float16)</span></span>
<span class="line"><span style="color:#24292e;">#接下来，我们就可以运行pipeline了</span></span>
<span class="line"><span style="color:#24292e;">prompt = &quot;a photograph of an astronaut riding a horse&quot;</span></span>
<span class="line"><span style="color:#24292e;">image = pipe(prompt).images[0]</span></span></code></pre></div><p>文生图流程图</p><p><img src="`+_+'" alt="img"></p>',74),P=[D];function q(v,w,y,E,L,x){return s(),a("div",null,P)}const k=e(S,[["render",q]]);export{U as __pageData,k as default};
