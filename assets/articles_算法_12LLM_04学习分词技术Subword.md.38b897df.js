import{_ as e,o,c as a,Q as i}from"./chunks/framework.2516552c.js";const _=JSON.parse('{"title":"NLP三大Subword模型详解：BPE、WordPiece、ULM","description":"","frontmatter":{},"headers":[],"relativePath":"articles/算法/12LLM/04学习分词技术Subword.md","filePath":"articles/算法/12LLM/04学习分词技术Subword.md","lastUpdated":1698198415000}'),r={name:"articles/算法/12LLM/04学习分词技术Subword.md"},t=i('<h1 id="nlp三大subword模型详解-bpe、wordpiece、ulm" tabindex="-1">NLP三大Subword模型详解：BPE、WordPiece、ULM <a class="header-anchor" href="#nlp三大subword模型详解-bpe、wordpiece、ulm" aria-label="Permalink to &quot;NLP三大Subword模型详解：BPE、WordPiece、ULM&quot;">​</a></h1><p>在NLP任务中，神经网络模型的训练和预测都需要借助词表来对句子进行表示。传统构造词表的方法，是先对各个句子进行分词，然后再统计并选出频数最高的前N个词组成词表。通常训练集中包含了大量的词汇，以英语为例，总的单词数量在17万到100万左右。出于计算效率的考虑，通常N的选取无法包含训练集中的所有词。因而，这种方法构造的词表存在着如下的问题：</p><ul><li>实际应用中，模型预测的词汇是开放的，对于未在词表中出现的词(Out Of Vocabulary, OOV)，模型将无法处理及生成；</li><li>词表中的低频词/稀疏词在模型训练过程中无法得到充分训练，进而模型不能充分理解这些词的语义；</li><li>一个单词因为不同的形态会产生不同的词，如由&quot;look&quot;衍生出的&quot;looks&quot;, &quot;looking&quot;, &quot;looked&quot;，显然这些词具有相近的意思，但是在词表中这些词会被当作不同的词处理，一方面增加了训练冗余，另一方面也造成了大词汇量问题。</li></ul><p>一种解决思路是使用字符粒度来表示词表，虽然能够解决OOV问题，但单词被拆分成字符后，一方面丢失了词的语义信息，另一方面，模型输入会变得很长，这使得模型的训练更加复杂难以收敛。</p><p>针对上述问题，Subword(子词)模型方法横空出世。它的划分粒度介于词与字符之间，比如可以将”looking”划分为”look”和”ing”两个子词，而划分出来的&quot;look&quot;，”ing”又能够用来构造其它词，如&quot;look&quot;和&quot;ed&quot;子词可组成单词&quot;looked&quot;，因而Subword方法能够大大降低词典的大小，同时对相近词能更好地处理。</p><p>目前有三种主流的Subword算法，它们分别是：Byte Pair Encoding (BPE), WordPiece和Unigram Language Model。</p><h2 id="byte-pair-encoding-bpe" tabindex="-1">Byte Pair Encoding (BPE) <a class="header-anchor" href="#byte-pair-encoding-bpe" aria-label="Permalink to &quot;Byte Pair Encoding (BPE)&quot;">​</a></h2><p>BPE最早是一种数据压缩算法，由Sennrich等人于2015年引入到NLP领域并很快得到推广。该算法简单有效，因而目前它是最流行的方法。GPT-2和RoBERTa使用的Subword算法都是BPE。</p><p>BPE获得Subword的步骤如下：</p><ol><li>准备足够大的训练语料，并确定期望的Subword词表大小；</li><li>将单词拆分为成最小单元。比如英文中26个字母加上各种符号，这些作为初始词表；</li><li>在语料上统计单词内相邻单元对的频数，选取频数最高的单元对合并成新的Subword单元；</li><li>重复第3步直到达到第1步设定的Subword词表大小或下一个最高频数为1。</li></ol><p>实际上，随着合并的次数增加，词表大小通常先增加后减小。在得到Subword词表后，针对每一个单词，我们可以采用如下的方式来进行编码：</p><ol><li>将词典中的所有子词按照长度由大到小进行排序；</li><li>对于单词w，依次遍历排好序的词典。查看当前子词是否是该单词的子字符串，如果是，则输出当前子词，并对剩余单词字符串继续匹配；</li><li>如果遍历完字典后，仍然有子字符串没有匹配，则将剩余字符串替换为特殊符号输出，如<code>&lt;unk&gt;</code>；</li><li>单词的表示即为上述所有输出子词。</li></ol><p>解码过程比较简单，如果相邻子词间没有中止符，则将两子词直接拼接，否则两子词之间添加分隔符。</p><h2 id="wordpiece" tabindex="-1">WordPiece <a class="header-anchor" href="#wordpiece" aria-label="Permalink to &quot;WordPiece&quot;">​</a></h2><p>Google的Bert模型在分词的时候使用的是WordPiece算法。与BPE算法类似，WordPiece算法也是每次从词表中选出两个子词合并成新的子词。与BPE的最大区别在于，如何选择两个子词进行合并：BPE选择频数最高的相邻子词合并，而WordPiece选择能够提升语言模型概率最大的相邻子词加入词表。</p><h2 id="unigram-language-model-ulm" tabindex="-1">Unigram Language Model (ULM) <a class="header-anchor" href="#unigram-language-model-ulm" aria-label="Permalink to &quot;Unigram Language Model (ULM)&quot;">​</a></h2><p>与WordPiece一样，Unigram Language Model(ULM)同样使用语言模型来挑选子词。不同之处在于，BPE和WordPiece算法的词表大小都是从小到大变化，属于增量法。而Unigram Language Model则是减量法,即先初始化一个大词表，根据评估准则不断丢弃词表，直到满足限定条件。ULM算法考虑了句子的不同分词可能，因而能够输出带概率的多个子词分段。</p><h2 id="sentencepiece" tabindex="-1">SentencePiece <a class="header-anchor" href="#sentencepiece" aria-label="Permalink to &quot;SentencePiece&quot;">​</a></h2><p>如何使用上述子词算法？一种简便的方法是使用SentencePiece，它是谷歌推出的子词开源工具包，其中集成了BPE、ULM子词算法。除此之外，SentencePiece还能支持字符和词级别的分词。更进一步，为了能够处理多语言问题，sentencePiece将句子视为Unicode编码序列，从而子词算法不用依赖于语言的表示。</p>',19),l=[t];function d(n,c,u,p,P,s){return o(),a("div",null,l)}const g=e(r,[["render",d]]);export{_ as __pageData,g as default};
