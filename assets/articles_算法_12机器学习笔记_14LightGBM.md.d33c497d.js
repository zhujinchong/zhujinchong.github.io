import{_ as t,o as e,c as a,Q as l}from"./chunks/framework.2516552c.js";const x=JSON.parse('{"title":"","description":"","frontmatter":{},"headers":[],"relativePath":"articles/算法/12机器学习笔记/14LightGBM.md","filePath":"articles/算法/12机器学习笔记/14LightGBM.md","lastUpdated":1698165159000}'),r={name:"articles/算法/12机器学习笔记/14LightGBM.md"},i=l('<blockquote><p>参考 <a href="https://www.zhihu.com/search?type=content&amp;q=LightGBM" target="_blank" rel="noreferrer">https://www.zhihu.com/search?type=content&amp;q=LightGBM</a></p></blockquote><p>LightGBM为了解决海量数据时遇到的问题：</p><ol><li>如何减少训练数据</li><li>如何减少特征</li><li>关于稀疏的数据</li></ol><p>LGBM和XGBoost区别：</p><table><thead><tr><th style="text-align:center;"></th><th style="text-align:center;">表头</th><th style="text-align:center;"></th></tr></thead><tbody><tr><td style="text-align:center;">叶子生长方式</td><td style="text-align:center;">按层</td><td style="text-align:center;">按收益</td></tr><tr><td style="text-align:center;">划分点搜索方法</td><td style="text-align:center;">特征预排序</td><td style="text-align:center;">直方图算法</td></tr><tr><td style="text-align:center;">内存开销</td><td style="text-align:center;">8个字节（存储特征值）</td><td style="text-align:center;">1个字节（存储bins中特征的个数）</td></tr><tr><td style="text-align:center;">划分的计算增益</td><td style="text-align:center;">数据特征</td><td style="text-align:center;">容器特征</td></tr><tr><td style="text-align:center;">类别特征处理</td><td style="text-align:center;">不支持</td><td style="text-align:center;">支持</td></tr></tbody></table><p>也就是增加了三个算法：</p><ol><li>histogram：减少候选分裂点，XGBoost需要遍历所有特征上的所有候选分裂点</li><li>Goss：单边梯度采样，减少样本的数量</li><li>EFB：互斥特征捆绑，减少特征的数量</li></ol><h3 id="histogram算法" tabindex="-1">Histogram算法 <a class="header-anchor" href="#histogram算法" aria-label="Permalink to &quot;Histogram算法&quot;">​</a></h3><p>XGBoost中采用预排序的方法，计算过程当中是按照value的排序，逐个数据样本来计算划分收益，这样的算法能够精确的找到最佳划分值，但是代价比较大同时也没有较好的推广性，并且还需要存储原来的数据的序列。</p><p>第一个优点：减少存储</p><p>在LightGBM中没有使用传统的预排序的思路，而是将这些精确的连续的每一个value划分到一系列离散的域中。这样一来，数据的表达变得更加简化，减少了内存的使用，而且直方图带来了一定的正则化的效果，能够使我们做出来的模型避免过拟合且具有更好的推广性。 第二个优点：计算加速（直方图差速）</p><p>选好特征后，计算左子节点数据（一阶、二阶梯度），右子节点可以通过父节点减左子节点得到。</p><h3 id="goss单边梯度采样" tabindex="-1">Goss单边梯度采样 <a class="header-anchor" href="#goss单边梯度采样" aria-label="Permalink to &quot;Goss单边梯度采样&quot;">​</a></h3><p>LGB的优化方法是：在保留大梯度样本的同时，随机地保留一些小梯度样本，同时放大小梯度样本的梯度。</p><p>比如：a%大梯度样本，随机选择b%个小梯度样本，那么，小梯度样本需要扩大(1-a)/b倍。</p><h3 id="efb互斥特征捆绑" tabindex="-1">EFB互斥特征捆绑 <a class="header-anchor" href="#efb互斥特征捆绑" aria-label="Permalink to &quot;EFB互斥特征捆绑&quot;">​</a></h3><p><strong>如何判断互斥？</strong>（ 利用特征和特征间的关系构造一个加权无向图，图着色问题）</p><ol><li>首先特征看成图中的点，不相互独立的特征用一条边连起来，边的权重就是两个相连接的特征的总冲突值</li><li>按照节点的度对特征降序排序， 度越大，说明与其他特征的冲突越大</li><li>对于每一个特征， 遍历已有的特征簇，如果发现该特征加入到特征簇中的矛盾数不超过某一个阈值，则将该特征加入到该簇中。 如果该特征不能加入任何一个已有的特征簇，则新建一个簇，将该特征加入到新建的簇中。</li></ol><p><strong>如何对同一个簇中的特征捆绑？</strong></p><p>对于特征如何合并，一个重要的原则就是使合并的两个特征可以被顺利区分出来，LGB采取了一个更改阈值的方法。例如对于特征x∈(0, 10)， 特征y∈(0, 20)，就可以把特征y转换为y∈(10,30)，然后再去合并x与y。</p>',20),s=[i];function n(o,d,c,h,p,g){return e(),a("div",null,s)}const y=t(r,[["render",n]]);export{x as __pageData,y as default};
