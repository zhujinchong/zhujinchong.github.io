import{_ as e,o as a,c as t,Q as i}from"./chunks/framework.2516552c.js";const n="/assets/BERT.c1c8b94d.png",r="/assets/image-20191108144506569.8d791b73.png",o="/assets/image-20191108092814378.4b881206.png",s="/assets/image-20191108145014609.17c16e8d.png",p="/assets/image-20191108093052670.99af3a73.png",d="/assets/v2-f25d1b04b74ea4210c3368a2b609f63c_1440w.f4134e76.webp",l="/assets/image-20191108093326500.c6239cbb.png",c="/assets/640-1573176999544.73165cf6.webp",E=JSON.parse('{"title":"","description":"","frontmatter":{},"headers":[],"relativePath":"articles/算法/12LLM/04bert2详解.md","filePath":"articles/算法/12LLM/04bert2详解.md","lastUpdated":1698198415000}'),m={name:"articles/算法/12LLM/04bert2详解.md"},g=i('<p>论文 《Pre-training of Deep Bidirectional Transformers for Language Understanding》</p><h3 id="前言" tabindex="-1">前言 <a class="header-anchor" href="#前言" aria-label="Permalink to &quot;前言&quot;">​</a></h3><p>BERT的本质上是通过在海量的语料的基础上运行自监督学习方法为单词学习一个好的特征表示，所谓自监督学习是指在没有人工标注的数据上运行的监督学习。在以后特定的NLP任务中，我们可以直接使用BERT的特征表示作为该任务的词嵌入特征。</p><p>论文主要特点：</p><ul><li>使用了Transformer。</li><li>为了解决单向的缺陷，论文中提出了两种预训练方法，分别是 “Masked Language model” 和 &quot;next sentence prediction&quot;</li><li>使用更强大的机器训练更大规模的数据， 用户可以直接使用BERT作为Word2Vec的转换矩阵并高效的将其应用到自己的任务中。</li></ul><h3 id="模型架构" tabindex="-1">模型架构 <a class="header-anchor" href="#模型架构" aria-label="Permalink to &quot;模型架构&quot;">​</a></h3><p>这个图害我不浅，我不理解为什么每个单词要对应一个Transformer。其实BERT一行是一个Encoder；GPT只是一个Decoder.</p><p><img src="'+n+'" alt="image"></p><p>基本架构是Transformer编码器的架构。</p><p><img src="'+r+'" alt="image-20191108144506569"></p><h3 id="输入表征-input-representation" tabindex="-1">输入表征 input representation <a class="header-anchor" href="#输入表征-input-representation" aria-label="Permalink to &quot;输入表征 input representation&quot;">​</a></h3><p><img src="'+o+'" alt="image-20191108092814378"></p><p>更多说明：</p><p><img src="'+s+'" alt="image-20191108145014609"></p><h3 id="token-embeddings" tabindex="-1">token embeddings <a class="header-anchor" href="#token-embeddings" aria-label="Permalink to &quot;token embeddings&quot;">​</a></h3><p>tokens化是使用一种叫做WordPiece token化的方法来完成的。这是一种数据驱动的token化方法，旨在实现词汇量和非词汇量之间的平衡。这就是“strawberries”被分成“straw”和“berries”的方式。对这种方法的详细描述超出了本文的范围。感兴趣的读者可以参考Wu et al. (2016)和Schuster &amp; Nakajima (2012)中的第4.1节。单词token化的使用使得BERT只能在其词汇表中存储30522个“词”，而且在对英语文本进行token化时，很少会遇到词汇表以外的单词。</p><p>另外，在tokens的开始([CLS])和结束([SEP])处添加额外的tokens。这些tokens的目的是作为分类任务的输入表示，并分别分隔一对输入文本。</p><p>token嵌入层将每个wordpiece token转换为768维向量表示形式。这将使得我们的6个输入token被转换成一个形状为(6,768)的矩阵，或者一个形状为(1,6,768)的张量，如果我们包括批处理维度的话。</p><h3 id="segment-embedding" tabindex="-1">segment embedding <a class="header-anchor" href="#segment-embedding" aria-label="Permalink to &quot;segment embedding&quot;">​</a></h3><p>Segment嵌入层只有两个向量表示。第一个向量(索引0)分配给属于输入1的所有tokens，而最后一个向量(索引1)分配给属于输入2的所有tokens。如果一个输入只有一个输入语句，那么它的Segment嵌入就是对应于Segment嵌入表的索引为0的向量。</p><h3 id="position-embedding" tabindex="-1">position embedding <a class="header-anchor" href="#position-embedding" aria-label="Permalink to &quot;position embedding&quot;">​</a></h3><p>BERT被设计用来处理长度为512的输入序列。作者通过让BERT学习每个位置的向量表示来包含输入序列的顺序特征。这意味着Position嵌入层是一个大小为(512,768)的查找表。</p><p>postiton是学习得到的。推理时，只需要查表就可以。</p><h3 id="预训练任务" tabindex="-1">预训练任务 <a class="header-anchor" href="#预训练任务" aria-label="Permalink to &quot;预训练任务&quot;">​</a></h3><p>论文不使用传统的从左到右或从右到左的语言模型来预训练BERT。相反，使用两个新的无监督预测任务对BERT进行预训练。</p><p><img src="'+p+'" alt="image-20191108093052670"></p><p><img src="'+d+'" alt="img"></p><p><img src="'+l+'" alt="image-20191108093326500"></p><h3 id="fine-tune" tabindex="-1">fine-tune <a class="header-anchor" href="#fine-tune" aria-label="Permalink to &quot;fine-tune&quot;">​</a></h3><p>对于不同的任务，模型修改非常简单，最多加一层神经网络即可。</p><p><img src="'+c+'" alt="img"></p><h3 id="模型的影响" tabindex="-1">模型的影响 <a class="header-anchor" href="#模型的影响" aria-label="Permalink to &quot;模型的影响&quot;">​</a></h3><ol><li>深度学习就是表征学习。 在11项BERT刷出新境界的任务中，大多只在预训练表征（pre-trained representation）微调（fine-tuning）的基础上加一个线性层作为输出（linear output layer）。</li><li>规模很重要。这种遮挡（mask）在语言模型上的应用对很多人来说已经不新鲜了，但确是BERT的作者在如此超大规模的数据+模型+算力的基础上验证了其强大的表征学习能力。这样的模型，甚至可以延伸到很多其他的模型，可能之前都被不同的实验室提出和试验过，只是由于规模的局限没能充分挖掘这些模型的潜力，而遗憾地让它们被淹没在了滚滚的paper洪流之中。</li><li>预训练价值很大。预训练已经被广泛应用在各个领域了（e.g. ImageNet for CV, Word2Vec in NLP），多是通过大模型大数据，这样的大模型给小规模任务能带来的提升有几何，作者也给出了自己的答案。BERT模型的预训练是用Transformer做的，但我想换做LSTM或者GRU的话应该不会有太大性能上的差别，当然训练计算时的并行能力就另当别论了。</li><li>只有encoder没有decoder的双向语言表示预训练模型，可以接各种下游任务，它的输出只是文本表示，所以不能使用固定的decoder。</li><li>动态词向量。在Word2Vec，GloVe的年代，词向量都是静态的，一旦训练之后词向量就固定不变了，但是这就限制了模型对多义词的识别能力，比如apple可以指水果也可以指苹果公司，因此词向量需要动态变化。</li></ol>',33),h=[g];function _(b,u,f,k,T,q){return a(),t("div",null,h)}const B=e(m,[["render",_]]);export{E as __pageData,B as default};
