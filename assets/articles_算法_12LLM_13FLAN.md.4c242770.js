import{_ as t,o as a,c as e,Q as o}from"./chunks/framework.2516552c.js";const n="/assets/image-20230805222741096.e561e8d7.png",i="/assets/image-20230805223058480.c62d5c78.png",s="/assets/image-20230805223702400.8b86ab37.png",r="/assets/image-20230805224038066.a773df8b.png",l="/assets/image-20230805230410489.90c191bc.png",p="/assets/image-20230805231451728.81e4b7b9.png",P=JSON.parse('{"title":"Abstract","description":"","frontmatter":{},"headers":[],"relativePath":"articles/算法/12LLM/13FLAN.md","filePath":"articles/算法/12LLM/13FLAN.md","lastUpdated":1698165534000}'),h={name:"articles/算法/12LLM/13FLAN.md"},c=o('<p>论文：Finetuned Language Models Are Zero-Shot Learners</p><p>机构：谷歌</p><p>时间：2021.09.03</p><p>arxiv: [<a href="https://arxiv.org/abs/2109.01652" target="_blank" rel="noreferrer">2109.01652] Finetuned Language Models Are Zero-Shot Learners (arxiv.org)</a></p><p>Github: <a href="https://github.com/google-research/flan" target="_blank" rel="noreferrer">https://github.com/google-research/flan</a></p><h1 id="abstract" tabindex="-1">Abstract <a class="header-anchor" href="#abstract" aria-label="Permalink to &quot;Abstract&quot;">​</a></h1><p>本文探讨了一种提高语言模型zero-shot 学习能力的简单方法。我们展示了，instruction tuning（指令优化）–将语言模型在通过指示描述的数据集集合上进行微调–可观地提升了在不可见任务上的 zero-shot 性能。</p><p>我们采用了一个 137B 参数的预训练语言模型，并在60多个通过自然语言指令模板语言化的NLP数据集上进行指令优化。我们在不可见任务类型上测试了这个 instruction-tuned 模型，我们称之为 FLAN。FLAN在很大程度上提高了其未修改对应版本的性能，并且在我们评估的25个数据集中的20个上都超过了 zero-shot 175B GPT-3。FLAN甚至在 ANLI,RTE,BoolQ,AI2-ARC,OpenbookQA,StoryCloze上超过 few-shot GPT-3很多。消融研究表明，微调数据集的数量、模型规模和自然语言指令是 instruction tuning 成功的关键。</p><h1 id="_1-introduction" tabindex="-1">1.Introduction <a class="header-anchor" href="#_1-introduction" aria-label="Permalink to &quot;1.Introduction&quot;">​</a></h1><p>大规模的语言模型（lm），如GPT-3（Brown et al.，2020），已被证明具有非常好的few-shot学习性能。然而，他们在zero-shot学习方面却不那么成功。例如，GPT-3在阅读理解、问题回答和自然语言推理等任务上的zero-shot表现比few-shot表现要差得多。一个潜在的原因是，如果没有few-shot的示例，模型就很难在与训练前数据格式不相似的提示上表现良好。</p><p>在本文中，我们探索了一种简单的方法来提高大型语言模型的zero-shot性能，这将把它们的覆盖范围扩展到更广泛的受众。我们利用了NLP任务可以通过自然语言指令来描述的直觉，比如“这部电影评论的情绪是积极的还是消极的？”或者“把《你好吗》翻译成中文。”我们采用一个包含137个B参数的预训练语言模型，并在通过自然语言指令表示的60多个NLP数据集上对模型进行指令调微调。我们将这个结果模型称为FLAN，即 Finetuned Language Net。</p><p>为了评估FLAN在看不见的任务上的zero-shot性能，我们将NLP数据集根据其任务类型划分为集群，并在所有其他集群上进行指令调优时保留每个集群以对FLAN进行评估。例如，如图1所示，为了评估FLAN执行自然语言推理的能力，我们指导在一系列其他NLP任务上调整模型，如常识推理、翻译和情绪分析。由于这种设置确保了FLAN在指令调优中没有看到任何自然语言推理任务，然后我们评估了它执行zero-shot自然语言推理的能力。</p><p>图1：instruction-tuning在几个任务，推理在另外一个任务。</p><p><img src="'+n+'" alt="image-20230805222741096"></p><p>我们的评估表明，FLAN大大提高了基础137b参数模型的zero-shot性能。在我们评估的25个数据集中的20个，FLAN的zero-shot也超过175个b参数GPT-3的zero-shot，甚至在ANLI、RTE、OoolQ、AI2-ARC、OpenbooQA和StoryCloze上大大超过GPT-3的zero-shot。在消融研究中，我们发现在指令调整中增加任务簇的数量可以提高对看不见的任务的性能，而且只有在有足够的模型规模的情况下才能出现指令调优的好处。</p><p>指令调优是一种简单的方法，如图2所示，通过微调监督来改进语言模型对推理-文本交互的响应，结合了预训练微调和提示范式的吸引人的方面。我们的实证结果表明，语言模型能够执行纯粹通过指令来描述的任务。用于加载用于FLAN的指令调优数据集的源代码可以在<a href="https://github.com/google-research/flan%E4%B8%8A%E5%85%AC%E5%BC%80%E8%8E%B7%E5%BE%97%E3%80%82" target="_blank" rel="noreferrer">https://github.com/google-research/flan上公开获得。</a></p><p>图2：Fine-tuning、Prompting、Instrction-tuning区别</p><p><img src="'+i+'" alt="image-20230805223058480"></p><h1 id="_2-flan" tabindex="-1">2.FLAN <a class="header-anchor" href="#_2-flan" aria-label="Permalink to &quot;2.FLAN&quot;">​</a></h1><p>指令调优的动机是为了提高语言模型对NLP指令的响应能力。其想法是，通过使用监督来教LM执行通过指令所描述的任务，LM将学会遵循指令，即使对看不见的任务也要这样做。为了评估不可见任务的性能，我们根据任务类型将数据集分组为集群，保留每个任务集群进行评估时，对所有剩余集群进行指令调优。</p><h2 id="_2-1-tasks-templates" tabindex="-1">2.1.Tasks &amp; Templates <a class="header-anchor" href="#_2-1-tasks-templates" aria-label="Permalink to &quot;2.1.Tasks &amp; Templates&quot;">​</a></h2><p>由于从头创建包含许多任务的指令调优数据集将是资源密集型的，我们将研究社区的现有数据集转换为教学格式。我们将62个在Tensorflflow Datasets中公开可用的文本数据集，包括语言理解和语言生成任务，聚合成一个单一的混合物。图3显示了这些数据集——每个数据集被分类为12个任务集群中的一个，其中给定集群中的数据集具有相同的任务类型。每个数据集的描述、大小和示例见附录G。</p><p><img src="'+s+'" alt="image-20230805223702400"></p><p>对于每个数据集，我们手动组成了10个独特的模板，它们使用自然语言指令来描述该数据集的任务。虽然这10个模板中的大多数描述了原始任务，以增加多样性，对于每个数据集，我们还包括多达三个“扭转任务”的模板（例如，对于情感分类，我们包括要求生成电影评论的模板）。然后，我们在所有数据集的混合上调整一个预先训练的语言模型，每个数据集中的例子通过该数据集随机选择的指令模板进行格式化。图4显示了一个自然语言推理数据集的多个指令模板。</p><p><img src="'+r+'" alt="image-20230805224038066"></p><h2 id="_2-2-evaluation-splits" tabindex="-1">2.2.Evaluation Splits <a class="header-anchor" href="#_2-2-evaluation-splits" aria-label="Permalink to &quot;2.2.Evaluation Splits&quot;">​</a></h2><p>我们感兴趣的是FLAN如何在指令调优中没有看到的任务上执行任务，因此定义什么是不可见的任务是至关重要的。虽然之前的一些工作通过不允许在训练中出现相同的数据集来定义看不见的任务，但我们使用了一个更保守的定义，它利用了图3中的任务集群。在这项工作中，我们只考虑如果在指令调整过程中没有看到来自D所属的任何任务集群的数据集，那么在评估时看不到的数据集D。例如，如果D是一个隐含任务，那么在指令调优中就没有出现隐含数据集，并且我们对所有其他集群进行了指令调优。</p><h2 id="_2-3-classification-with-options" tabindex="-1">2.3.Classification with Options <a class="header-anchor" href="#_2-3-classification-with-options" aria-label="Permalink to &quot;2.3.Classification with Options&quot;">​</a></h2><p>给定任务的输出空间是多个类（分类）或自由文本（生成）。由于FLAN是仅解码器语言模型的指令调优版本，它自然地在自由文本中响应，因此生成任务不需要进一步的修改。</p><p>对于分类任务，之前的工作（Brown et al.，2020）使用了一种等级分类方法，例如，只考虑两个输出（“是”和“否”），并以较高概率的一个作为模型的预测。虽然这个过程在逻辑上是合理的，但它是不完美的，因为答案的概率质量可能在说每个答案的方法之间有一个不期望的分布（例如，大量的说“是”的替代方法可能会降低分配给“是”的概率质量）。因此，我们包含了一个选项后缀，在其中，我们将令牌选项附加到分类任务的结尾，以及该任务的输出类列表。这使得模型在响应分类任务时能够意识到需要哪些选择。NLI显示了选项的使用示例，图1中的常识性示例。</p><h2 id="_2-4-training-details" tabindex="-1">2.4.Training Details <a class="header-anchor" href="#_2-4-training-details" aria-label="Permalink to &quot;2.4.Training Details&quot;">​</a></h2><p><strong>Model architecture and pretraining.</strong></p><p>在我们的实验中，我们使用LaMDA-PT，这是一个包含137B参数的从左到右的解码器转换语言模型（Thoppilan et al.，2022）。该模型在一组web文档（包括那些带有计算机代码的文档）、对话框数据和维基百科上进行了预训练，使用句子库被标记为具有32k个词汇表的2.49T BPE令牌（Kudo &amp;理查森，2018）。大约10%的训练前数据是非英语数据。请注意，LaMDA-PT只有语言模型的预训练(c.f。LaMDA，适合对话)。</p><p><strong>Instruction tuning procedure.</strong></p><p>FLAN 是 LaMDA-PT 的指令调优版本。我们的指令调优混合所有数据集，并从每个数据集中随机采样。为了平衡数据集的不同大小，我们将每个数据集的训练示例数量限制为30000个。同时，存在一些数据集，它们的训练示例数量比较少(例如，CommitmentBank只有250个)，为了防止这些数据集被边缘化，本文采用示例-比例混合方案(Raffel et al., 2020)，混合率最高为 3K。我们使用学习率为 3 e − 5 的Adafacotor优化器，一个batch 8192 tokens 来对所有模型进行 30k 梯度步的微调。微调时输入和目标序列的长度分别是1024和256。我们使用paking(Raffel et al., 2020)将多个训练示例组合成单个序列，使用特殊的EOS令牌将输入与目标分离。instuction tuning在 128核的TPUv3上花了大约60小时。</p><h1 id="_3-results" tabindex="-1">3.Results <a class="header-anchor" href="#_3-results" aria-label="Permalink to &quot;3.Results&quot;">​</a></h1><p>我们评估了自然语言推理、阅读理解、闭书QA、翻译、常识推理、共参考解析和结构到文本。如2.2所述，我们通过将数据集分组到任务集群中，并保留每个集群进行评估，同时对每个剩余集群（即每个评估任务集群使用不同的检查点）进行指令进行评估。对于每个数据集，我们评估了所有模板上的性能平均值，它给出了给定一个典型的自然语言指令的预期性能。由于开发集有时可用于手动提示工程（Brown et al.，2020），对于每个数据集，我们也使用具有最佳开发集性能的模板获得测试集性能。</p><p>为了进行比较，我们使用与GPT-3相同的提示来报告LaMDA-PT的zero-shot和few-shot结果（因为LaMDA-PT不适用于没有指令调优的自然指令）。这个基线提供了最直接的指令调优帮助。指令调优显著提高了大多数数据集上的LaMDA-PT。</p><p>zero-shot效果：FLAN-LaMDA-PT 好于 GPT-3、LaMDA-PT、GLaM</p><p>具体结果，略。</p><h1 id="_4-ablation-studies-further-analysis" tabindex="-1">4.Ablation studies &amp; Further Analysis <a class="header-anchor" href="#_4-ablation-studies-further-analysis" aria-label="Permalink to &quot;4.Ablation studies &amp; Further Analysis&quot;">​</a></h1><h2 id="_4-1-指令调优集群数" tabindex="-1">4.1.指令调优集群数 <a class="header-anchor" href="#_4-1-指令调优集群数" aria-label="Permalink to &quot;4.1.指令调优集群数&quot;">​</a></h2><p>如果有更多集群加入到 instruction tuning 中，性能可能会进一步提高。</p><h2 id="_4-2-scaling-laws" tabindex="-1">4.2.Scaling laws <a class="header-anchor" href="#_4-2-scaling-laws" aria-label="Permalink to &quot;4.2.Scaling laws&quot;">​</a></h2><p>Brown等人(2020)表明，对于较大的模型，语言模型的 zero-shot 和 few-shot 能力会大幅提高，接下来我们将探讨instruction tuning的好处是如何受到模型规模影响的。</p><p><img src="'+l+'" alt="image-20230805230410489"></p><h2 id="_4-3-role-of-instructions" tabindex="-1">4.3.Role of instructions <a class="header-anchor" href="#_4-3-role-of-instructions" aria-label="Permalink to &quot;4.3.Role of instructions&quot;">​</a></h2><p>最后一个消融学习，我们探索了微调时 instructions 的作用。因为一个可能的原因是，表现的提升完全来源于多任务 fine-tuning，不使用 instruction 模型也能得到这么好的结果。因此，我们在没有指令的情况下考虑两种 finetune 设置。</p><ul><li>在 no template的设置下，只有输入和输出被给到模型（eg.对于翻译，输入将是“The dog runs”。而输出结果将是“Le chien court”）</li><li>在 dataset name 设置下，每个输入都带有任务和数据集名称的前缀（eg.翻译为 French，输入是：“[Translation: WMT’14 to French] The dog runs.”)</li></ul><p>我们将这两种消融与使用自然指令（eg.“Please translate this sentence to French: ‘The dog runs.’”)的FLAN的微调过程进行比较。我们对图5中的四个保留的集群执行评估。 图8显示了结果——两种消融配置的表现都明显低于FLAN，这表明指令训练对未知任务的零射击表现至关重要。</p><p><img src="'+p+'" alt="image-20230805231451728"></p><h2 id="_4-4-instructions-with-few-shot-examples" tabindex="-1">4.4.Instructions with few-shot examples <a class="header-anchor" href="#_4-4-instructions-with-few-shot-examples" aria-label="Permalink to &quot;4.4.Instructions with few-shot examples&quot;">​</a></h2><p>到目前为止，我们一直关注于在zero-shot设置中的指令调优。在这里，我们研究了在推理时使用few-shot范例时，结果表明，效果更好，略。</p><h2 id="_4-5-instruction-tuning-facilitates-prompt-tuning" tabindex="-1">4.5.Instruction tuning facilitates prompt tuning <a class="header-anchor" href="#_4-5-instruction-tuning-facilitates-prompt-tuning" aria-label="Permalink to &quot;4.5.Instruction tuning facilitates prompt tuning&quot;">​</a></h2><p>正如我们所看到的，指令调优提高了模型响应指令的能力，因此，如果FLAN确实更适合执行NLP任务，那么它在使用 soft prompt(由通过 prompt tuning 优化的预置连续变量表示)执行推理时也应该获得更好的性能。作为进一步分析，我们依照2.2中的类别划分对SuperGLUE中的每个任务进行连续 prompt 训练，这样的话，当对任务 T 进行 prompt-tuning时，没有与 T 相同簇的任务会在 instruction tuning时被看到。我们的 prompt tuning设置遵循了Lester等人(2021)的过程，除了我们使用的提示长度为10，权重衰减为1e-4，并且没有在注意力评分上使 dropout。我们在初步实验中发现，这些变化提高了LaMDA-PT的性能。</p><p>什么是prompt tuning，可以进一步去看论文。这里，略。</p><h1 id="_5-related-work" tabindex="-1">5.Related work <a class="header-anchor" href="#_5-related-work" aria-label="Permalink to &quot;5.Related work&quot;">​</a></h1><p>略。</p><h1 id="_6-discussion" tabindex="-1">6.Discussion <a class="header-anchor" href="#_6-discussion" aria-label="Permalink to &quot;6.Discussion&quot;">​</a></h1><p>我们的论文探讨了零镜头提示中的一个简单问题：对作为指令措辞的任务集合进行微调的模型是否能提高其在不可见任务上的性能？我们通过指令调优来操作这个问题，这是一种简单的方法，它结合了训练前-微调和提示范式的吸引力方面。我们的指令调优模型FLAN比未调优模型提高了性能，并且在我们评估的大多数任务上都超过了零射击GPT-3。消融研究表明，看不见的任务的性能随着指令调整任务集群数量的增加而提高，有趣的是，只有在足够的模型规模下，指令调整的性能才会提高。此外，指令调优还可以与其他提示方法相结合，如zero-shot提示和提示调优。</p><p>大规模语言模型的不同能力让人们注意到专家模型（每个任务一个模型）和通用模型（许多任务的一个模型之间的权衡；阿里瓦哈根等人，2019；Pratap等人，2020年），我们的研究具有潜在的意义。尽管人们可能期望标记数据在改进专家模型方面发挥最自然的作用，但指令调优演示了如何使用标记数据来帮助大型语言模型执行许多看不见的任务。换句话说，指令调整对跨任务泛化的积极影响表明，特定任务的训练是对一般语言建模的补充，并激发了对通用模型的进一步研究。</p><p>至于我们研究的局限性，在将任务分配到集群时存在一定程度的主观性（尽管我们试图在文献中使用公认的分类），而且我们只探索使用典型的相对较短的单一句子的指令(c.f。给人群工人的详细说明)。我们评估的一个局限性是，个别例子可能出现在模型的训练前数据中，其中包括web文档，尽管在事后分析（附录C）中，我们没有发现任何证据表明数据重叠显著影响结果。最后，FLAN 137B的规模使其服务成本高昂。未来的指令调整工作可能包括收集/生成更多的任务集群用于微调、跨语言实验、使用FLAN生成数据来训练下游分类器，以及使用微调来改进在偏差和公平方面的模型行为（Soliaman&amp;Dennison，2021）。</p><h1 id="_7-conclusion" tabindex="-1">7.Conclusion <a class="header-anchor" href="#_7-conclusion" aria-label="Permalink to &quot;7.Conclusion&quot;">​</a></h1><p>本文探索了一种简单的方法来只基于 instruction，提高大规模语言模型执行zero-shot 任务的能力。我们的 instruction-tuned 模型，FLAN，与GPT-3相比更具优势，并且显示了大规模语言模型遵循 instruction 的潜在能力。</p>',64),u=[c];function d(g,m,f,_,b,L){return a(),e("div",null,u)}const T=t(h,[["render",d]]);export{P as __pageData,T as default};
