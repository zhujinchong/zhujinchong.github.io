import{_ as e,o as a,c as t,Q as r}from"./chunks/framework.2516552c.js";const u=JSON.parse('{"title":"思路","description":"","frontmatter":{},"headers":[],"relativePath":"articles/Algorithm/23RAG/100基于大语言模型知识问答应用落地实践.md","filePath":"articles/Algorithm/23RAG/100基于大语言模型知识问答应用落地实践.md","lastUpdated":null}'),n={name:"articles/Algorithm/23RAG/100基于大语言模型知识问答应用落地实践.md"},i=r('<blockquote><p>参考1：亚马逊AWS官方微博-基于大语言模型知识问答应用落地实践</p><p><a href="https://aws.amazon.com/cn/blogs/china/practice-of-knowledge-question-answering-application-based-on-llm-knowledge-base-construction-part-1/" target="_blank" rel="noreferrer">https://aws.amazon.com/cn/blogs/china/practice-of-knowledge-question-answering-application-based-on-llm-knowledge-base-construction-part-1/</a></p></blockquote><h1 id="思路" tabindex="-1">思路 <a class="header-anchor" href="#思路" aria-label="Permalink to &quot;思路&quot;">​</a></h1><p>大语言模型（LLM）+知识召回（Knowledge Retrieval）解决通用大语言模型在专业领域回答缺乏依据、存在幻觉等问题，在私域知识问答方面可以很好的弥补通用大语言模型的一些短板。其基本思路是把私域知识文档进行切片然后向量化后续通过向量检索进行召回，再作为上下文输入到大语言模型进行归纳总结。</p><p>知识库可以采取基于倒排和基于向量的两种索引方式进行构建，基于向量数据库索引需要考虑几个关键点:</p><ul><li>文档量级、切分粒度、向量维度</li></ul><p>为了召回效果的追求，对文档的切分常常会采用按句或者按段进行多粒度的冗余存贮。</p><h1 id="知识库向量化" tabindex="-1">知识库向量化 <a class="header-anchor" href="#知识库向量化" aria-label="Permalink to &quot;知识库向量化&quot;">​</a></h1><h2 id="_1、文档拆分" tabindex="-1">1、文档拆分 <a class="header-anchor" href="#_1、文档拆分" aria-label="Permalink to &quot;1、文档拆分&quot;">​</a></h2><p>a. 拆分片段的方法</p><p>目前使用较多的基础方式是采用 Langchain 中的 RecursiveCharacterTextSplitter，属于是 Langchain 的默认拆分器。它采用这个多级分隔字符列表 – [“\\n\\n”， “\\n”， ” “， “”] 来进行拆分，默认先按照段落做拆分，如果拆分结果的 chunk_size 超出，再继续利用下一级分隔字符继续拆分，直到满足 chunk_size 的要求。</p><p>但这种做法相对来说还是比较粗糙，还是可能会造成一些关键内容会被拆开。对于一些其他的文档格式可以有一些更细致的做法。</p><ol><li>FAQ 文件，必须按照一问一答粒度拆分，后续向量化的输入可以仅仅使用问题，也可以使用问题+答案。</li><li>Markdown 文件，”#”是用于标识标题的特殊字符，可以采用 MarkdownHeaderTextSplitter 作为分割器，它能更好的保证内容和标题对应的被提取出来。</li><li>PDF 文件，会包含更丰富的格式信息。Langchain 里面提供了非常多的 Loader，但 Langchain 中的 PDFMinerPDFasHTMLLoader 的切分效果上会更好，它把 PDF 转换成 HTML，通过 HTML 的<code>&lt;div&gt;</code>块进行切分，这种方式能保留每个块的字号信息，从而可以推导出每块内容的隶属关系，把一个段落的标题和上一级父标题关联上，使得信息更加完整。类似下面这种效果。</li></ol><p>b. 模型对片段长度的支持</p><p>由于拆分的片段后续需要通过向量化模型进行推理，所以必须考虑向量化模型的 Max_seq_length 的限制，超出这个限制可能会导致出现截断，导致语义不完整。从支持的 Max_seq_length 来划分，目前主要有两类 Embedding 模型，如下表所示（这四个是有过实践经验的模型）。</p><table><thead><tr><th>模型名称</th><th>Max_seq_length</th></tr></thead><tbody><tr><td><a href="https://huggingface.co/sentence-transformers/paraphrase-multilingual-mpnet-base-v2" target="_blank" rel="noreferrer">paraphrase-multilingual-mpnet-base-v2</a> (<a href="http://sbert.net/" target="_blank" rel="noreferrer">sbert.net</a>)</td><td>128</td></tr><tr><td><a href="https://huggingface.co/shibing624/text2vec-base-chinese" target="_blank" rel="noreferrer">text2vec-base-chinese </a> (<a href="https://pypi.org/project/text2vec/" target="_blank" rel="noreferrer">text2vec</a>)</td><td>128</td></tr><tr><td><a href="https://huggingface.co/GanymedeNil/text2vec-large-chinese" target="_blank" rel="noreferrer">text2vec-large-chinese</a> (<a href="https://pypi.org/project/text2vec/" target="_blank" rel="noreferrer">text2vec</a>)</td><td>512</td></tr><tr><td>text-embedding-ada-002 (openai)</td><td>8192</td></tr></tbody></table><p>这里的 Max_seq_length 是指 Token 数，和字符数并不等价。依据之前的测试经验，前三个模型一个 token 约为 1.5 个汉字字符左右。而对于大语言模型，如 chatglm，一个 token 一般为 2 个字符左右。如果在切分时不方便计算 token 数，也可以简单按照这个比例来简单换算，保证不出现截断的情况。</p><p>前三个模型属于基于 Bert 的 Embedding 模型，OpenAI 的 text-embedding-ada-002 模型是基于 GPT3 的模型。前者适合句或者短段落的向量化，后者 OpenAI 的 SAAS 化接口，适合长文本的向量化，但不能私有化部署。</p><p>可以根据召回效果进行验证选择。从目前的实践经验上看 text-embedding-ada-002 对于中文的相似性打分排序性可以，但区分度不够（集中 0.7 左右），不太利于直接通过阈值判断是否有相似知识召回。</p><p>另外，对于长度限制的问题也有另外一种改善方法，可以对拆分的片段进行编号，相邻的片段编号也临近，当召回其中一个片段时，可以通过向量数据库的 range search 把附近的片段也召回回来，也能保证召回内容的语意完整性。</p><h2 id="_2、向量化模型选择" tabindex="-1">2、向量化模型选择 <a class="header-anchor" href="#_2、向量化模型选择" aria-label="Permalink to &quot;2、向量化模型选择&quot;">​</a></h2><p>上节提到四个模型只是提到了模型对于文本长度的支持差异，效果方面目前并没有非常权威的结论。可以通过 <a href="https://huggingface.co/spaces/mteb/leaderboard" target="_blank" rel="noreferrer">leaderboard </a>来了解各个模型的性能，榜上的大多数的模型的评测还是基于公开数据集的 benchmark，对于真实生产中的场景 benchmark 结论是否成立还需要 case by case 地来看。但原则上有以下几方面的经验可以分享：</p><ol><li>经过垂直领域 Finetune 的模型比原始向量模型有明显优势</li><li>目前的向量化模型分为两类，对称和非对称。未进行微调的情况下，对于 FAQ 建议走对称召回，也就是 Query 到 Question 的召回。对于文档片段知识，建议使用非对称召回模型，也就是 Query 到 Answer（文档片段）的召回。</li><li>没有效果上的明显的差异的情况下，尽量选择向量维度短的模型，高维向量（如 openai 的 text-embedding-ada-002）会给向量数据库造成检索性能和成本两方面的压力。</li></ol><p>更多的内容会在本系列的召回优化部分进行深入讨论。</p><h1 id="向量数据库" tabindex="-1">向量数据库 <a class="header-anchor" href="#向量数据库" aria-label="Permalink to &quot;向量数据库&quot;">​</a></h1><p>向量库选择哪种搜索算法：k-NN、</p><p>算法引擎支持：Faiss</p><h1 id="知识召回" tabindex="-1">知识召回 <a class="header-anchor" href="#知识召回" aria-label="Permalink to &quot;知识召回&quot;">​</a></h1><p>分两种：倒排召回（语义召回）、向量召回</p><ul><li>在实际的生产实践中，倒排的召回方式也非常实用，它具备精确匹配、索引效率和可解释的优势。</li><li>近些年深度学习的发展，基于语义的向量召回表现了强大的检索效果。</li></ul><p>倒排召回：</p><p>倒排索引（Inverted index）作为一种广泛使用的索引方式，也常被称为反向索引、置入档案或反向档案，它是文档检索系统中最常用的数据结构。离线索引构建时，通过分词器对文档进行切词，得到一系列的关键词（Term）集合，然后以 Term 为 key 构建它与相关文档的映射关系。在线搜索流程中，首先通过切词器对用户输入进行切词，得到 Term 列表，然后根据 BM25 打分公式进行打分排序返回给客户。（ES）</p><h2 id="_1、对称召回和非对称召回" tabindex="-1">1、对称召回和非对称召回 <a class="header-anchor" href="#_1、对称召回和非对称召回" aria-label="Permalink to &quot;1、对称召回和非对称召回&quot;">​</a></h2><p>为了方便描述，下文把对称召回称为 QQ（Query-Question）召回，把非对称召回称为 QD（Query-Document）召回。</p><p>在真实的生产场景中，知识的存在形式有两种 —— FAQ 和文本文档， FAQ 由于是一问一答的形式，天然是比较适合进行 QQ 召回，而文本文档比较适应 QD 召回。但在实际的 case 中，单独 QQ 召回或者 QD 召回都存在一定的问题。相对于 QQ 召回，QD 召回需要 embedding 模型对语义有更强的理解。</p><h2 id="_2、知识增强优化召回" tabindex="-1">2、知识增强优化召回 <a class="header-anchor" href="#_2、知识增强优化召回" aria-label="Permalink to &quot;2、知识增强优化召回&quot;">​</a></h2><p>知识增强一般是指，根据上下文重新 Refine 原始的文本使得原始的文本信息更完整。</p><p>如：把原始文档通过ChatGPT编程FAQ文档。</p><h2 id="_3、finetune向量模型" tabindex="-1">3、Finetune向量模型 <a class="header-anchor" href="#_3、finetune向量模型" aria-label="Permalink to &quot;3、Finetune向量模型&quot;">​</a></h2><h2 id="_4、多路召回" tabindex="-1">4、多路召回 <a class="header-anchor" href="#_4、多路召回" aria-label="Permalink to &quot;4、多路召回&quot;">​</a></h2><p>在传统的搜索场景中，如何结合这两路召回的结果是一个问题，因为两者不在一个评分体系下。在基于大语言模型的知识问答机器人场景， 可以简单采用倒排召回和向量召回各 TopK 去重取并集的方式。主要是因为大语言模型对召回内容的相关性排序不敏感，且能够容忍一些不相关的知识召回内容。</p>',40),l=[i];function o(h,d,c,s,p,_){return a(),t("div",null,l)}const g=e(n,[["render",o]]);export{u as __pageData,g as default};
