import{_ as a,o as e,c as l,Q as t}from"./chunks/framework.2516552c.js";const r="/assets/v2-7946bc1070d007f8409f4bce00fe1731_1440w.516472fa.webp",D=JSON.parse('{"title":"并行训练","description":"","frontmatter":{},"headers":[],"relativePath":"articles/算法/22模型训练和微调/01并行训练.md","filePath":"articles/算法/22模型训练和微调/01并行训练.md","lastUpdated":null}'),i={name:"articles/算法/22模型训练和微调/01并行训练.md"},o=t('<p>这篇文章提出：模型并行、算子融合、模型简化，三个方便提高训练效率</p><p>[大模型工业化的方法论，都藏在GPU里-工业模型 (51cto.com)]</p><p><a href="https://www.51cto.com/article/744026.html" target="_blank" rel="noreferrer">https://www.51cto.com/article/744026.html</a></p><p><a href="https://zhuanlan.zhihu.com/p/581677880" target="_blank" rel="noreferrer">https://zhuanlan.zhihu.com/p/581677880</a></p><h1 id="并行训练" tabindex="-1">并行训练 <a class="header-anchor" href="#并行训练" aria-label="Permalink to &quot;并行训练&quot;">​</a></h1><p>概念：</p><ul><li><p>数据并行（data parallel）</p></li><li><p>模型并行（model parallel）</p></li><li><ul><li>tensor并行</li><li>pipeline并行</li><li>Sequence并行</li></ul></li><li><p>Zero Redundancy Data Parallelism （ZeRO）</p></li></ul><p><img src="'+r+'" alt="img"></p><p>Pipeline 并行也就是层间并行（图中上半部分），将不同的层划分到不同的 GPU 进行计算。这种模式的通信只发生在层的边界，通信次数和通信数据量较少，但会引入额外的 GPU 空间等待时间。</p><p>Tensor 并行也就是层内并行（图中下半部分），将一个层的计算划分到不同的 GPU 上计算。这种模式的实现更加容易，对于大矩阵的效果更好，更好实现 GPU 之间的负载均衡，但通信次数和数据量都比较大。</p><p>模型训练过程中涉及到的参数主要包含两大类，<em>model data</em> 和 <em>non-model data</em>，具体表示如下：</p><ul><li><p>model data</p></li><li><ul><li>模型权重</li><li>模型权重的梯度</li><li>优化器的状态</li></ul></li><li><p>non-model data</p></li><li><ul><li>模型逐层的特征向量（也叫作activations）</li></ul></li></ul><h2 id="数据并行data-parallel" tabindex="-1">数据并行Data Parallel <a class="header-anchor" href="#数据并行data-parallel" aria-label="Permalink to &quot;数据并行Data Parallel&quot;">​</a></h2><p>把一个模型复制很多份，每份模型学习不同数据来加速训练。每隔一段时间（比如一个batch或者若干个batch）后需要彼此之间同步模型权重的梯度。</p><p>方法：PyTroch的DP DDP方法</p><h2 id="模型并行model-parallel" tabindex="-1">模型并行Model Parallel <a class="header-anchor" href="#模型并行model-parallel" aria-label="Permalink to &quot;模型并行Model Parallel&quot;">​</a></h2><p>主要目的是解决模型太大在一张卡上放不下的问题。</p><h3 id="pipeline并行" tabindex="-1">pipeline并行 <a class="header-anchor" href="#pipeline并行" aria-label="Permalink to &quot;pipeline并行&quot;">​</a></h3><p>已经有很多Pipeline相关的研究工作了，例如PipeDream，GPipe，和Chimera。它们的主要目的都是降低bubble time。这里不做过多介绍。</p><h3 id="tensor并行" tabindex="-1">tensor并行 <a class="header-anchor" href="#tensor并行" aria-label="Permalink to &quot;tensor并行&quot;">​</a></h3><p><a href="https://zhuanlan.zhihu.com/p/581677880" target="_blank" rel="noreferrer">https://zhuanlan.zhihu.com/p/581677880</a></p><h3 id="sequence并行" tabindex="-1">Sequence并行 <a class="header-anchor" href="#sequence并行" aria-label="Permalink to &quot;Sequence并行&quot;">​</a></h3><p>Tensor parallelism主要是为了解决由 model data （模型权重，梯度和优化器状态）导致的内存瓶颈，但是 non-model data也可能成为性能瓶颈。比如像AlphaFold和NAS任务中会存在很多中间特征值（也叫activations）。</p><p>以DARTS算法为例，它的模型参数量其实并不多，但是它有很多分支，所以activations会消耗大量GPU内存，这也是为什么很多NAS算法只能在CIFAR-10上搜索到合适的模型结构后，再做人工扩展，最后应用到ImageNet上做性能验证。</p><p>同样地，在使用Transformer训练语言模型时，由于Transformer层中的Self-attention机制的复杂度是O(n2)，其中 n 是序列长度。换言之，长序列数据将增加中间activation内存使用量，从而限制设备的训练能力。</p><p>Sequential Parallelism （SP）就为了解决non-model data导致的性能瓶颈而提出的。下图给出了SP在Transform并行训练上的应用，具体的原理可以查看原论文.</p><h2 id="zero-redundancy-data-parallelism-zero" tabindex="-1">Zero Redundancy Data Parallelism （ZeRO） <a class="header-anchor" href="#zero-redundancy-data-parallelism-zero" aria-label="Permalink to &quot;Zero Redundancy Data Parallelism （ZeRO）&quot;">​</a></h2><p>训练过程中GPU内存开销主要包含以下几个方面：</p><ul><li><p>模型状态内存（Model State Memory）：</p></li><li><ul><li>梯度</li><li>模型参数</li><li>优化器状态：当使用像Adam这样的优化器时，优化器的状态会成为GPU内存开销的大头。前面介绍的DP，TP， PP算法并没有考虑这个问题。</li></ul></li><li><p>激活内存（Activation Memory）：在优化了模型状态内存之后，人们发现激活函数也会导致瓶颈。激活函数计算位于前向传播之中，用于支持后向传播。</p></li><li><p>碎片内存（Fragmented Memory）：深度学习模型的低效有时是由于内存碎片所导致的。在模型之中，每个张量的生命周期不同，由于不同张量寿命的变化而会导致一些内存碎片。由于这些碎片的存在，会导致即使有足够的可用内存，也会因为缺少连续内存而使得内存分配失败。ZeRO 根据张量的不同寿命主动管理内存，防止内存碎片。</p></li></ul><p>ZeRO针对模型状态的三部分都做了对应的内存改进方法：</p><ul><li>ZeRO1：只划分优化器状态(optimizer states, os)，</li><li>ZeRO2：划分优化器状态和梯度(gradient, g)，</li><li>ZeRO3：划分优化器状态和梯度和模型参数(parameters, p)，</li></ul><h2 id="算法并行" tabindex="-1">算法并行 <a class="header-anchor" href="#算法并行" aria-label="Permalink to &quot;算法并行&quot;">​</a></h2><h1 id="代码学习" tabindex="-1">代码学习 <a class="header-anchor" href="#代码学习" aria-label="Permalink to &quot;代码学习&quot;">​</a></h1><h2 id="dp和ddp" tabindex="-1">DP和DDP <a class="header-anchor" href="#dp和ddp" aria-label="Permalink to &quot;DP和DDP&quot;">​</a></h2><p>用pytorch进行数据并行主要有两种方法，一种是用<code>DataParallel</code>，另一种是<code>DistributedDataParallel</code>。数据并行的核心是把模型复制多份，每份模型接受不同的batch，然后在做模型参数更新的时候保持复制出来的不同模型的参数一致。在pytorch里面，<code>DataParallel</code>和<code>DistributedDataParallel</code>都是用来保持不同复制出来的模型参数一致的，只不过<code>DataParallel</code>是通过多线程实现的，<code>DistributedDataParallel</code>是在多进程的环境中使用的。</p><p>DP和DDP原理：</p><p><a href="https://blog.csdn.net/kuweicai/article/details/120516410" target="_blank" rel="noreferrer">https://blog.csdn.net/kuweicai/article/details/120516410</a></p><p>DP和DDP和Apex加速效果对比图：</p><p><a href="https://zhuanlan.zhihu.com/p/98535650" target="_blank" rel="noreferrer">https://zhuanlan.zhihu.com/p/98535650</a></p><p>DP和DDP:</p><ul><li>DP多线程；DDP多进程</li><li>DP不支持多机；DDP支持多机；</li><li>DP代码简单；DDP更快；</li></ul><p>DP：主GPU分配数据，各自前向传播，主GPU汇总output，在主GPU上计算梯度并分发新的权重</p><p>DDP：没有主GPU，各自前向传播及计算梯度；在反向传播期间，梯度下降在所有GPU上均被执行（这里需要GPU通信），从而确保每个 GPU 在反向传播结束时最终得到平均梯度的相同副本。</p><p>DDP使用需要学习：<a href="https://zhuanlan.zhihu.com/p/467103734" target="_blank" rel="noreferrer">https://zhuanlan.zhihu.com/p/467103734</a></p><p>DP/DDP代码教程：<a href="https://zhuanlan.zhihu.com/p/640758965" target="_blank" rel="noreferrer">https://zhuanlan.zhihu.com/p/640758965</a></p><h2 id="deepspeed" tabindex="-1">DeepSpeed <a class="header-anchor" href="#deepspeed" aria-label="Permalink to &quot;DeepSpeed&quot;">​</a></h2><p>学习：<a href="https://blog.csdn.net/weixin_43301333/article/details/127237122" target="_blank" rel="noreferrer">https://blog.csdn.net/weixin_43301333/article/details/127237122</a></p><p>相应代码：<a href="https://github.com/Tsai-chasel/training-methods-tutorial/blob/main/05_cifar10_deepspeed.py" target="_blank" rel="noreferrer">https://github.com/Tsai-chasel/training-methods-tutorial/blob/main/05_cifar10_deepspeed.py</a></p><h2 id="megatron" tabindex="-1">Megatron <a class="header-anchor" href="#megatron" aria-label="Permalink to &quot;Megatron&quot;">​</a></h2><p>Megatron 采用了模型并行、Sequence 并行等优化手段以高效地训练 Transformer 大模型，可训练万亿级别参数量的模型。</p>',50),p=[o];function n(h,d,s,c,u,m){return e(),l("div",null,p)}const b=a(i,[["render",n]]);export{D as __pageData,b as default};
