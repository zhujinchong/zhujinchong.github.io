import{_ as a,o as t,c as e,Q as h}from"./chunks/framework.2516552c.js";const n="/assets/image-20231201093252234.c5729484.png",r="/assets/image-20231201093508688.db42b2d8.png",o="/assets/image-20231201093729224.9d17cbba.png",p="/assets/image-20231201093807112.82358a68.png",g=JSON.parse('{"title":"中兴PPT","description":"","frontmatter":{},"headers":[],"relativePath":"articles/Algorithm/21模型部署/04中兴LLM加速.md","filePath":"articles/Algorithm/21模型部署/04中兴LLM加速.md","lastUpdated":null}'),i={name:"articles/Algorithm/21模型部署/04中兴LLM加速.md"},l=h('<h1 id="中兴ppt" tabindex="-1">中兴PPT <a class="header-anchor" href="#中兴ppt" aria-label="Permalink to &quot;中兴PPT&quot;">​</a></h1><p>1000并发：初始版本单卡吞吐量30token/s，系统需求为4k token/s, 。</p><p>优化技术：FlashAttention, KV Cache, Continuous Batching等</p><p><img src="'+n+'" alt="image-20231201093252234"></p><p>瓶颈：</p><p><img src="'+r+'" alt="image-20231201093508688"></p><p>特点：</p><p><img src="'+o+'" alt="image-20231201093729224"></p><p>最终用了4张40G A100，供1000人用。</p><p>所以，他们用了Int4量化模型。</p><p><img src="'+p+'" alt="image-20231201093807112"></p><h1 id="技术总结" tabindex="-1">技术总结 <a class="header-anchor" href="#技术总结" aria-label="Permalink to &quot;技术总结&quot;">​</a></h1><h2 id="一、服务层" tabindex="-1">一、服务层 <a class="header-anchor" href="#一、服务层" aria-label="Permalink to &quot;一、服务层&quot;">​</a></h2><h3 id="continuous-batching" tabindex="-1">Continuous Batching <a class="header-anchor" href="#continuous-batching" aria-label="Permalink to &quot;Continuous Batching&quot;">​</a></h3><p>优点：加速计算</p><p>参考：<a href="https://zhuanlan.zhihu.com/p/657586838" target="_blank" rel="noreferrer">https://zhuanlan.zhihu.com/p/657586838</a></p><h2 id="二、引擎层" tabindex="-1">二、引擎层 <a class="header-anchor" href="#二、引擎层" aria-label="Permalink to &quot;二、引擎层&quot;">​</a></h2><h3 id="kv-cache" tabindex="-1">KV Cache <a class="header-anchor" href="#kv-cache" aria-label="Permalink to &quot;KV Cache&quot;">​</a></h3><p>优点：加速计算</p><p>参考：<a href="https://zhuanlan.zhihu.com/p/662498827" target="_blank" rel="noreferrer">https://zhuanlan.zhihu.com/p/662498827</a></p><h3 id="paged-attention" tabindex="-1">Paged Attention <a class="header-anchor" href="#paged-attention" aria-label="Permalink to &quot;Paged Attention&quot;">​</a></h3><p>优化KV Cache。KV Cache需要占用连续显存，造成碎片化，浪费空间。受操作系统中虚拟内存和分页经典思想启发，PagedAttention 允许在非连续的内存空间中存储连续的 key 和 value。</p><p>这种内存效率的提升被证明非常有用，允许系统将更多序列进行批处理，提高 GPU 使用率，显著提升吞吐量。</p><p>参考：</p><p><a href="https://zhuanlan.zhihu.com/p/661152161" target="_blank" rel="noreferrer">https://zhuanlan.zhihu.com/p/661152161</a></p><h3 id="tp-dp-pp" tabindex="-1">TP/DP/PP <a class="header-anchor" href="#tp-dp-pp" aria-label="Permalink to &quot;TP/DP/PP&quot;">​</a></h3><p>并行化方案：主要解决模型、数据太大的问题。</p><p>参考：</p><p><a href="https://zhuanlan.zhihu.com/p/581677880" target="_blank" rel="noreferrer">https://zhuanlan.zhihu.com/p/581677880</a></p><h2 id="三、算子层" tabindex="-1">三、算子层 <a class="header-anchor" href="#三、算子层" aria-label="Permalink to &quot;三、算子层&quot;">​</a></h2><h3 id="算子融合" tabindex="-1">算子融合 <a class="header-anchor" href="#算子融合" aria-label="Permalink to &quot;算子融合&quot;">​</a></h3><p>理解：软硬件结合解决。框架层面、结合硬件。</p><p>参考：</p><p>[什么是算子融合-怎么搞 - 知乎 (zhihu.com)](<a href="https://zhuanlan.zhihu.com/p/664070841#:~:text=%E7%AE%97%E5%AD%90%E8%9E%8D%E5%90%88%EF%BC%88Operator" target="_blank" rel="noreferrer">https://zhuanlan.zhihu.com/p/664070841#:~:text=算子融合（Operator</a> Fusion）是一种常用的优化技术，常用于深度学习和其他计算密集型任务中。 基本操作就是将多个连续的操作或算子合并成一个单一的算子，以减少计算和内存开销。,使用算子融合非常有助于提高执行效率，减少数据传输和临时存储的需要，以及提高缓存利用率。 其实它是一种直接的优化&quot;少动数据多计算&quot;，通过 这种方式达到了更多的利用计算资源。)</p><p><a href="https://zhuanlan.zhihu.com/p/581755093" target="_blank" rel="noreferrer">Op Fusion（一）： 什么是算子融合 - 知乎 (zhihu.com)</a></p><h3 id="flash-attention" tabindex="-1">Flash Attention <a class="header-anchor" href="#flash-attention" aria-label="Permalink to &quot;Flash Attention&quot;">​</a></h3><p>两个观点：</p><p>1、本方法主要是减少内存访问的开销（瓶颈）；其他人主要是专注于减少FLOP，所以他们计算速度没加快。。</p><p>2、为了减少内存读写，将softmax矩阵分块计算，最终Flash Attention计算是线性的。</p><p>参考：</p><p><a href="https://zhuanlan.zhihu.com/p/661152161" target="_blank" rel="noreferrer">https://zhuanlan.zhihu.com/p/661152161</a></p><p><a href="https://zhuanlan.zhihu.com/p/651280772" target="_blank" rel="noreferrer">https://zhuanlan.zhihu.com/p/651280772</a></p><h3 id="量化quantization" tabindex="-1">量化Quantization <a class="header-anchor" href="#量化quantization" aria-label="Permalink to &quot;量化Quantization&quot;">​</a></h3><p>LLM.int8()、ZeroQuant、SmoothQuant、GPTQ、AWQ、SqueezeLLM、QLora</p><p>参考：</p><p><a href="https://zhuanlan.zhihu.com/p/645308698" target="_blank" rel="noreferrer">https://zhuanlan.zhihu.com/p/645308698</a></p><p><a href="https://zhuanlan.zhihu.com/p/667455383" target="_blank" rel="noreferrer">https://zhuanlan.zhihu.com/p/667455383</a></p><h1 id="其他" tabindex="-1">其他 <a class="header-anchor" href="#其他" aria-label="Permalink to &quot;其他&quot;">​</a></h1><p>推荐：</p><p>大语言模型推理加速技术：<a href="https://zhuanlan.zhihu.com/p/667455383" target="_blank" rel="noreferrer">https://zhuanlan.zhihu.com/p/667455383</a></p>',50),s=[l];function u(c,m,d,_,z,f){return t(),e("div",null,s)}const P=a(i,[["render",u]]);export{g as __pageData,P as default};
