import{_ as p,o as a,c as l,k as s,a as n,Q as e}from"./chunks/framework.2516552c.js";const t="/assets/image-20231214142426407.4496551e.png",o="/assets/v2-4e5bacfab55c6fc21990be89e3c84e14_720w.90a1f7a3.webp",r="/assets/v2-d45c62f548c4da5d47a904a09ba20159_720w.22ce88e2.webp",i="/assets/image-20231214142846436.e8f3f85e.png",c="/assets/1042406-20170407134342035-1966704661.0af98029.png",d="/assets/v2-5ec8d534e62e24befd16865da662f197_r.84704bcf.jpg",w="/assets/image-20231214084913459.213883c4.png",T="/assets/image-20231214084959106.bda39f2a.png",g="data:image/png;base64,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",m="data:image/png;base64,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",y="/assets/image-20231214185123914.4404f567.png",Q="/assets/image-20231214185251951.2f299a21.png",h="/assets/image-20231214191606352.b44c5940.png",Y=JSON.parse('{"title":"N-gram模型","description":"","frontmatter":{},"headers":[],"relativePath":"articles/Blog/05分词技术.md","filePath":"articles/Blog/05分词技术.md","lastUpdated":null}'),u={name:"articles/Blog/05分词技术.md"},f=e(`<ol><li>BPE是2015年被Sennrich等人在《Neural Machine Translation of Rare Words with Subword Units》中提出的。</li></ol><p>下面我们每一步在整张词表中找出频率最高相邻序列，并把它合并，依次循环。</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki github-dark vp-code-dark"><code><span class="line"><span style="color:#e1e4e8;">原始词表 {&#39;l o w e r &lt;/w&gt;&#39;: 2, &#39;n e w e s t &lt;/w&gt;&#39;: 6, &#39;w i d e s t &lt;/w&gt;&#39;: 3, &#39;l o w &lt;/w&gt;&#39;: 5}</span></span>
<span class="line"><span style="color:#e1e4e8;">出现最频繁的序列 (&#39;s&#39;, &#39;t&#39;) 9</span></span>
<span class="line"><span style="color:#e1e4e8;">合并最频繁的序列后的词表 {&#39;n e w e st &lt;/w&gt;&#39;: 6, &#39;l o w e r &lt;/w&gt;&#39;: 2, &#39;w i d e st &lt;/w&gt;&#39;: 3, &#39;l o w &lt;/w&gt;&#39;: 5}</span></span>
<span class="line"><span style="color:#e1e4e8;">出现最频繁的序列 (&#39;e&#39;, &#39;st&#39;) 9</span></span>
<span class="line"><span style="color:#e1e4e8;">合并最频繁的序列后的词表 {&#39;l o w e r &lt;/w&gt;&#39;: 2, &#39;l o w &lt;/w&gt;&#39;: 5, &#39;w i d est &lt;/w&gt;&#39;: 3, &#39;n e w est &lt;/w&gt;&#39;: 6}</span></span>
<span class="line"><span style="color:#e1e4e8;">出现最频繁的序列 (&#39;est&#39;, &#39;&lt;/w&gt;&#39;) 9</span></span>
<span class="line"><span style="color:#e1e4e8;">合并最频繁的序列后的词表 {&#39;w i d est&lt;/w&gt;&#39;: 3, &#39;l o w e r &lt;/w&gt;&#39;: 2, &#39;n e w est&lt;/w&gt;&#39;: 6, &#39;l o w &lt;/w&gt;&#39;: 5}</span></span>
<span class="line"><span style="color:#e1e4e8;">出现最频繁的序列 (&#39;l&#39;, &#39;o&#39;) 7</span></span>
<span class="line"><span style="color:#e1e4e8;">合并最频繁的序列后的词表 {&#39;w i d est&lt;/w&gt;&#39;: 3, &#39;lo w e r &lt;/w&gt;&#39;: 2, &#39;n e w est&lt;/w&gt;&#39;: 6, &#39;lo w &lt;/w&gt;&#39;: 5}</span></span>
<span class="line"><span style="color:#e1e4e8;">出现最频繁的序列 (&#39;lo&#39;, &#39;w&#39;) 7</span></span>
<span class="line"><span style="color:#e1e4e8;">合并最频繁的序列后的词表 {&#39;w i d est&lt;/w&gt;&#39;: 3, &#39;low e r &lt;/w&gt;&#39;: 2, &#39;n e w est&lt;/w&gt;&#39;: 6, &#39;low &lt;/w&gt;&#39;: 5}</span></span>
<span class="line"><span style="color:#e1e4e8;">出现最频繁的序列 (&#39;n&#39;, &#39;e&#39;) 6</span></span>
<span class="line"><span style="color:#e1e4e8;">合并最频繁的序列后的词表 {&#39;w i d est&lt;/w&gt;&#39;: 3, &#39;low e r &lt;/w&gt;&#39;: 2, &#39;ne w est&lt;/w&gt;&#39;: 6, &#39;low &lt;/w&gt;&#39;: 5}</span></span>
<span class="line"><span style="color:#e1e4e8;">出现最频繁的序列 (&#39;w&#39;, &#39;est&lt;/w&gt;&#39;) 6</span></span>
<span class="line"><span style="color:#e1e4e8;">合并最频繁的序列后的词表 {&#39;w i d est&lt;/w&gt;&#39;: 3, &#39;low e r &lt;/w&gt;&#39;: 2, &#39;ne west&lt;/w&gt;&#39;: 6, &#39;low &lt;/w&gt;&#39;: 5}</span></span>
<span class="line"><span style="color:#e1e4e8;">出现最频繁的序列 (&#39;ne&#39;, &#39;west&lt;/w&gt;&#39;) 6</span></span>
<span class="line"><span style="color:#e1e4e8;">合并最频繁的序列后的词表 {&#39;w i d est&lt;/w&gt;&#39;: 3, &#39;low e r &lt;/w&gt;&#39;: 2, &#39;newest&lt;/w&gt;&#39;: 6, &#39;low &lt;/w&gt;&#39;: 5}</span></span>
<span class="line"><span style="color:#e1e4e8;">出现最频繁的序列 (&#39;low&#39;, &#39;&lt;/w&gt;&#39;) 5</span></span>
<span class="line"><span style="color:#e1e4e8;">合并最频繁的序列后的词表 {&#39;w i d est&lt;/w&gt;&#39;: 3, &#39;low e r &lt;/w&gt;&#39;: 2, &#39;newest&lt;/w&gt;&#39;: 6, &#39;low&lt;/w&gt;&#39;: 5}</span></span>
<span class="line"><span style="color:#e1e4e8;">出现最频繁的序列 (&#39;i&#39;, &#39;d&#39;) 3</span></span>
<span class="line"><span style="color:#e1e4e8;">合并最频繁的序列后的词表 {&#39;w id est&lt;/w&gt;&#39;: 3, &#39;newest&lt;/w&gt;&#39;: 6, &#39;low&lt;/w&gt;&#39;: 5, &#39;low e r &lt;/w&gt;&#39;: 2}</span></span></code></pre><pre class="shiki github-light vp-code-light"><code><span class="line"><span style="color:#24292e;">原始词表 {&#39;l o w e r &lt;/w&gt;&#39;: 2, &#39;n e w e s t &lt;/w&gt;&#39;: 6, &#39;w i d e s t &lt;/w&gt;&#39;: 3, &#39;l o w &lt;/w&gt;&#39;: 5}</span></span>
<span class="line"><span style="color:#24292e;">出现最频繁的序列 (&#39;s&#39;, &#39;t&#39;) 9</span></span>
<span class="line"><span style="color:#24292e;">合并最频繁的序列后的词表 {&#39;n e w e st &lt;/w&gt;&#39;: 6, &#39;l o w e r &lt;/w&gt;&#39;: 2, &#39;w i d e st &lt;/w&gt;&#39;: 3, &#39;l o w &lt;/w&gt;&#39;: 5}</span></span>
<span class="line"><span style="color:#24292e;">出现最频繁的序列 (&#39;e&#39;, &#39;st&#39;) 9</span></span>
<span class="line"><span style="color:#24292e;">合并最频繁的序列后的词表 {&#39;l o w e r &lt;/w&gt;&#39;: 2, &#39;l o w &lt;/w&gt;&#39;: 5, &#39;w i d est &lt;/w&gt;&#39;: 3, &#39;n e w est &lt;/w&gt;&#39;: 6}</span></span>
<span class="line"><span style="color:#24292e;">出现最频繁的序列 (&#39;est&#39;, &#39;&lt;/w&gt;&#39;) 9</span></span>
<span class="line"><span style="color:#24292e;">合并最频繁的序列后的词表 {&#39;w i d est&lt;/w&gt;&#39;: 3, &#39;l o w e r &lt;/w&gt;&#39;: 2, &#39;n e w est&lt;/w&gt;&#39;: 6, &#39;l o w &lt;/w&gt;&#39;: 5}</span></span>
<span class="line"><span style="color:#24292e;">出现最频繁的序列 (&#39;l&#39;, &#39;o&#39;) 7</span></span>
<span class="line"><span style="color:#24292e;">合并最频繁的序列后的词表 {&#39;w i d est&lt;/w&gt;&#39;: 3, &#39;lo w e r &lt;/w&gt;&#39;: 2, &#39;n e w est&lt;/w&gt;&#39;: 6, &#39;lo w &lt;/w&gt;&#39;: 5}</span></span>
<span class="line"><span style="color:#24292e;">出现最频繁的序列 (&#39;lo&#39;, &#39;w&#39;) 7</span></span>
<span class="line"><span style="color:#24292e;">合并最频繁的序列后的词表 {&#39;w i d est&lt;/w&gt;&#39;: 3, &#39;low e r &lt;/w&gt;&#39;: 2, &#39;n e w est&lt;/w&gt;&#39;: 6, &#39;low &lt;/w&gt;&#39;: 5}</span></span>
<span class="line"><span style="color:#24292e;">出现最频繁的序列 (&#39;n&#39;, &#39;e&#39;) 6</span></span>
<span class="line"><span style="color:#24292e;">合并最频繁的序列后的词表 {&#39;w i d est&lt;/w&gt;&#39;: 3, &#39;low e r &lt;/w&gt;&#39;: 2, &#39;ne w est&lt;/w&gt;&#39;: 6, &#39;low &lt;/w&gt;&#39;: 5}</span></span>
<span class="line"><span style="color:#24292e;">出现最频繁的序列 (&#39;w&#39;, &#39;est&lt;/w&gt;&#39;) 6</span></span>
<span class="line"><span style="color:#24292e;">合并最频繁的序列后的词表 {&#39;w i d est&lt;/w&gt;&#39;: 3, &#39;low e r &lt;/w&gt;&#39;: 2, &#39;ne west&lt;/w&gt;&#39;: 6, &#39;low &lt;/w&gt;&#39;: 5}</span></span>
<span class="line"><span style="color:#24292e;">出现最频繁的序列 (&#39;ne&#39;, &#39;west&lt;/w&gt;&#39;) 6</span></span>
<span class="line"><span style="color:#24292e;">合并最频繁的序列后的词表 {&#39;w i d est&lt;/w&gt;&#39;: 3, &#39;low e r &lt;/w&gt;&#39;: 2, &#39;newest&lt;/w&gt;&#39;: 6, &#39;low &lt;/w&gt;&#39;: 5}</span></span>
<span class="line"><span style="color:#24292e;">出现最频繁的序列 (&#39;low&#39;, &#39;&lt;/w&gt;&#39;) 5</span></span>
<span class="line"><span style="color:#24292e;">合并最频繁的序列后的词表 {&#39;w i d est&lt;/w&gt;&#39;: 3, &#39;low e r &lt;/w&gt;&#39;: 2, &#39;newest&lt;/w&gt;&#39;: 6, &#39;low&lt;/w&gt;&#39;: 5}</span></span>
<span class="line"><span style="color:#24292e;">出现最频繁的序列 (&#39;i&#39;, &#39;d&#39;) 3</span></span>
<span class="line"><span style="color:#24292e;">合并最频繁的序列后的词表 {&#39;w id est&lt;/w&gt;&#39;: 3, &#39;newest&lt;/w&gt;&#39;: 6, &#39;low&lt;/w&gt;&#39;: 5, &#39;low e r &lt;/w&gt;&#39;: 2}</span></span></code></pre></div><h1 id="n-gram模型" tabindex="-1">N-gram模型 <a class="header-anchor" href="#n-gram模型" aria-label="Permalink to &quot;N-gram模型&quot;">​</a></h1><p><strong>什么是N-gram模型？</strong></p><p>如果我们有一个由 m 个字/词组成的序列（或者说一个句子），我们希望算得句子的概率，根据链式规则，可得</p><p><img src="`+t+'" alt="image-20231214142426407"></p><p>这个概率显然并不好算，不妨利用马尔科夫链的假设，即当前这个词仅仅跟前面几个有限的词相关。</p><p>当 n=1, 一个一元模型（unigram model)即为 ：</p><p><img src="'+o+'" alt="img"></p><p>当 n=2, 一个二元模型（bigram model)即为 ：</p><p><img src="'+r+'" alt="img"></p><p>通过我们的标准语料库，我们可以近似的计算出所有的分词之间的二元条件概率，比如任意两个词，它们的条件概率分布可以近似的表示为</p><p><img src="'+i+'" alt="image-20231214142846436"></p><p>利用语料库建立的统计概率，对于一个新的句子，我们就可以通过计算各种分词方法对应的联合分布概率，找到最大概率对应的分词方法，即为最优分词。</p><p>N元模型的分词方法虽然很好，但是要在实际中应用也有很多问题：</p><ul><li>某些生僻词，概率为0。（这种情况我们一般会使用拉普拉斯平滑，即给它一个较小的概率值）</li><li>句子长，分词有很多情况，计算量也非常大，这时我们可以用下一节维特比算法来优化算法时间复杂度。</li></ul><p><strong>上面只是根据分词计算出句子概率，那么如何分词？维特比算法。</strong></p><p>为了简化原理描述，我们本节的讨论都是以二元模型为基础。</p><p>维特比算法采用的是动态规划来解决这个最优分词问题的，动态规划要求局部路径也是最优路径的一部分，很显然我们的问题是成立的。首先我们看一个简单的分词例子：&quot;人生如梦境&quot;。它的可能分词可以用下面的概率图表示：</p><p>对于每个词，我们都可以计算出它的频率和二元条件概率。</p><p><img src="'+c+'" alt="img"></p><p>从后往前推，最终的分词结果为&quot;人生/如/梦境&quot;。</p><h1 id="wordpiece" tabindex="-1">WordPiece <a class="header-anchor" href="#wordpiece" aria-label="Permalink to &quot;WordPiece&quot;">​</a></h1><p>最早出现在2016年谷歌的论文Google’s Neural Machine Translation system。 2018 年 BERT 中也用了。</p><p>它可以被认为是 BPE 和 Unigram 算法的折中。WordPiece 也是一种贪婪算法，它利用似然性而不是计数频率来合并每次迭代中的最佳对，但配对字符的选择是基于计数频率的。因此，在选择要配对的字符方面，它与 BPE 类似；在选择要合并的最佳对方面，它与 Unigram 类似。</p><p>BPE算法通过循环的方式不断将一些高频的pair进行合并，通过贪婪的方式，每一步都将将高频的组合进行合并。这种方法存在的一个主要问题是：一个词可能存在多种拆分方式，对于算法来说，难以评估使用那个拆分方式比较合理，可以组合的列表中的优先级无法确定，通常会直接取第一个，举个例子：</p><p><img src="'+d+'" alt="img"></p><p>例如：linear = <strong>li + near</strong> 或者 <strong>li + n + ea + r</strong>，这两种拆分方法哪个好坏，无法评价。所以，比BPE使用频次进行merge更好的方法是， 在merge的时候考虑merge前后的影响到底有多大。</p><p>WordPiece选择能够提升语言模型似然概率最大的相邻子词加入词表。例如，在考虑将&quot;e&quot;和&quot;s&quot;合并的时候除了会考虑&quot;es&quot;的概率值，还会考虑&quot;e&quot;和&quot;s&quot;的概率值。</p><p><strong>似然概率怎么计算？</strong></p>',31),_={class:"MathJax",jax:"SVG",style:{direction:"ltr",position:"relative"}},b={style:{overflow:"visible","min-height":"1px","min-width":"1px","vertical-align":"-0.566ex"},xmlns:"http://www.w3.org/2000/svg",width:"16.331ex",height:"2.262ex",role:"img",focusable:"false",viewBox:"0 -750 7218.2 1000","aria-hidden":"true"},x=e('<g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D446" d="M308 24Q367 24 416 76T466 197Q466 260 414 284Q308 311 278 321T236 341Q176 383 176 462Q176 523 208 573T273 648Q302 673 343 688T407 704H418H425Q521 704 564 640Q565 640 577 653T603 682T623 704Q624 704 627 704T632 705Q645 705 645 698T617 577T585 459T569 456Q549 456 549 465Q549 471 550 475Q550 478 551 494T553 520Q553 554 544 579T526 616T501 641Q465 662 419 662Q362 662 313 616T263 510Q263 480 278 458T319 427Q323 425 389 408T456 390Q490 379 522 342T554 242Q554 216 546 186Q541 164 528 137T492 78T426 18T332 -20Q320 -22 298 -22Q199 -22 144 33L134 44L106 13Q83 -14 78 -18T65 -22Q52 -22 52 -14Q52 -11 110 221Q112 227 130 227H143Q149 221 149 216Q149 214 148 207T144 186T142 153Q144 114 160 87T203 47T255 29T308 24Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(922.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(1978.6,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(2367.6,0)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z" style="stroke-width:3;"></path></g><g data-mml-node="mn" transform="translate(2728.6,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(3228.6,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(3673.2,0)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z" style="stroke-width:3;"></path></g><g data-mml-node="mn" transform="translate(4034.2,0)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(4534.2,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(4978.9,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(5423.6,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(5868.2,0)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(6229.2,0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(6829.2,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" style="stroke-width:3;"></path></g></g></g>',1),v=[x],q=s("mjx-assistive-mml",{unselectable:"on",display:"inline",style:{top:"0px",left:"0px",clip:"rect(1px, 1px, 1px, 1px)","-webkit-touch-callout":"none","-webkit-user-select":"none","-khtml-user-select":"none","-moz-user-select":"none","-ms-user-select":"none","user-select":"none",position:"absolute",padding:"1px 0px 0px 0px",border:"0px",display:"block",width:"auto",overflow:"hidden"}},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("mi",null,"S"),s("mo",null,"="),s("mo",{stretchy:"false"},"("),s("mi",null,"t"),s("mn",null,"1"),s("mo",null,","),s("mi",null,"t"),s("mn",null,"2"),s("mo",null,","),s("mo",null,","),s("mo",null,","),s("mi",null,"t"),s("mi",null,"n"),s("mo",{stretchy:"false"},")")])],-1),k=e('<p><img src="'+w+'" alt="image-20231214084913459"></p><p>假设把相邻位置的x和y两个子词进行合并，合并后产生的子词记为z，此时句子S似然值的变化可表示为：</p><p><img src="'+T+'" alt="image-20231214084959106"></p><p><img src="'+g+`" alt="image-20231214195702259"></p><p>遍历所有的字符对，合并Loss最大的字符对。</p><p><strong>不太好理解？</strong></p><p>拆分前信息熵为-log(p(tz))，拆分后为-log(p(tx)) - log(p(ty)) ，拆分后-拆分前的信息增益为-log(p(tx)) - log(p(ty)) + log(p(tz))，可以变形为上式。信息增益越大越好。</p><p><strong>WordPiece算法的主要步骤如下：</strong></p><ol><li>准备足够大的训练语料</li><li>确定期望的subword词表大小</li><li>将单词拆分成字符序列</li><li>基于第3步数据训练语言模型</li><li>从所有可能的subword单元中选择加入语言模型后能最大程度地增加训练数据概率的单元作为新的单元</li><li>重复第5步直到达到第2步设定的subword词表大小或概率增量低于某一阈值</li></ol><p>构建分词表后，编码解码步骤和BPE一样。</p><p>WordPiece有两种代码实现方式：bottom-up 和 top-down。最初的WordPiece和BPE一样基于bottom-up的，而BERT是基于 top-down。对于日语、中文和韩语，这种 top-down 的方法不起作用，因为没有明确的token units 可以开始。<a href="https://www.tensorflow.org/text/guide/subwords_tokenizer#optional_the_algorithm" target="_blank" rel="noreferrer">https://www.tensorflow.org/text/guide/subwords_tokenizer#optional_the_algorithm</a></p><p><strong>WordPiece的bottom-up算法实现</strong></p><p>WordPiece算法分词时，会在非单词开头的字符上加上<code>##</code>前缀。如love，分词是</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki github-dark vp-code-dark"><code><span class="line"><span style="color:#e1e4e8;">l</span></span>
<span class="line"><span style="color:#e1e4e8;">##o</span></span>
<span class="line"><span style="color:#e1e4e8;">##v</span></span>
<span class="line"><span style="color:#e1e4e8;">##e</span></span></code></pre><pre class="shiki github-light vp-code-light"><code><span class="line"><span style="color:#24292e;">l</span></span>
<span class="line"><span style="color:#24292e;">##o</span></span>
<span class="line"><span style="color:#24292e;">##v</span></span>
<span class="line"><span style="color:#24292e;">##e</span></span></code></pre></div><p>代码示例</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki github-dark vp-code-dark"><code><span class="line"><span style="color:#e1e4e8;">from collections import defaultdict</span></span>
<span class="line"><span style="color:#e1e4e8;">from typing import Dict, List, Tuple</span></span>
<span class="line"><span style="color:#e1e4e8;"></span></span>
<span class="line"><span style="color:#e1e4e8;">def get_word_freqs(pre_tokenized_res: List[str]) -&gt; Dict[str, int]:</span></span>
<span class="line"><span style="color:#e1e4e8;">    word_freqs = defaultdict(int)</span></span>
<span class="line"><span style="color:#e1e4e8;">    for word in pre_tokenized_res:</span></span>
<span class="line"><span style="color:#e1e4e8;">        word_freqs[word] += 1</span></span>
<span class="line"><span style="color:#e1e4e8;">    return word_freqs</span></span>
<span class="line"><span style="color:#e1e4e8;"></span></span>
<span class="line"><span style="color:#e1e4e8;"></span></span>
<span class="line"><span style="color:#e1e4e8;">def get_word_splits(word_freqs: Dict[str, int]) -&gt; Dict[str, List[str]]:</span></span>
<span class="line"><span style="color:#e1e4e8;">    word_splits = {}</span></span>
<span class="line"><span style="color:#e1e4e8;">    for word in word_freqs.keys():</span></span>
<span class="line"><span style="color:#e1e4e8;">        word_splits[word] = [c if i == 0 else f&quot;##{c}&quot; for i, c in enumerate(word)]</span></span>
<span class="line"><span style="color:#e1e4e8;">    return word_splits</span></span>
<span class="line"><span style="color:#e1e4e8;"></span></span>
<span class="line"><span style="color:#e1e4e8;"></span></span>
<span class="line"><span style="color:#e1e4e8;">def get_base_vocab(word_splits: Dict[str, List[str]]) -&gt; List[str]:</span></span>
<span class="line"><span style="color:#e1e4e8;">    vocab = []</span></span>
<span class="line"><span style="color:#e1e4e8;">    for split in word_splits.values():</span></span>
<span class="line"><span style="color:#e1e4e8;">        vocab.extend(split)</span></span>
<span class="line"><span style="color:#e1e4e8;">    vocab = sorted(list(set(vocab)))</span></span>
<span class="line"><span style="color:#e1e4e8;">    return vocab</span></span>
<span class="line"><span style="color:#e1e4e8;"></span></span>
<span class="line"><span style="color:#e1e4e8;"></span></span>
<span class="line"><span style="color:#e1e4e8;">def get_pair_scores(</span></span>
<span class="line"><span style="color:#e1e4e8;">    word_freqs: Dict[str, int],</span></span>
<span class="line"><span style="color:#e1e4e8;">    word_splits: Dict[str, List[str]],</span></span>
<span class="line"><span style="color:#e1e4e8;">) -&gt; Dict[Tuple[str, str], int]:</span></span>
<span class="line"><span style="color:#e1e4e8;">    pair_freqs = defaultdict(int)</span></span>
<span class="line"><span style="color:#e1e4e8;">    subword_freqs = defaultdict(int)</span></span>
<span class="line"><span style="color:#e1e4e8;"></span></span>
<span class="line"><span style="color:#e1e4e8;">    for freq, split in zip(word_freqs.values(), word_splits.values()):</span></span>
<span class="line"><span style="color:#e1e4e8;">        if len(split) == 1:</span></span>
<span class="line"><span style="color:#e1e4e8;">            subword_freqs[split[0]] += freq</span></span>
<span class="line"><span style="color:#e1e4e8;">            continue</span></span>
<span class="line"><span style="color:#e1e4e8;">        for i in range(len(split) - 1):</span></span>
<span class="line"><span style="color:#e1e4e8;">            pair_freqs[(split[i], split[i + 1])] += freq</span></span>
<span class="line"><span style="color:#e1e4e8;">            subword_freqs[split[i]] += freq</span></span>
<span class="line"><span style="color:#e1e4e8;">        subword_freqs[split[-1]] += freq</span></span>
<span class="line"><span style="color:#e1e4e8;"></span></span>
<span class="line"><span style="color:#e1e4e8;">    pair_scores = {</span></span>
<span class="line"><span style="color:#e1e4e8;">        pair: freq / (subword_freqs[pair[0]] * subword_freqs[pair[1]]) \\</span></span>
<span class="line"><span style="color:#e1e4e8;">        for pair, freq in pair_freqs.items()</span></span>
<span class="line"><span style="color:#e1e4e8;">    }</span></span>
<span class="line"><span style="color:#e1e4e8;">    return pair_scores</span></span>
<span class="line"><span style="color:#e1e4e8;"></span></span>
<span class="line"><span style="color:#e1e4e8;"></span></span>
<span class="line"><span style="color:#e1e4e8;">def merge_pair(pair, word_splits):</span></span>
<span class="line"><span style="color:#e1e4e8;">    a, b = pair</span></span>
<span class="line"><span style="color:#e1e4e8;">    for word in word_splits.keys():</span></span>
<span class="line"><span style="color:#e1e4e8;">        split = word_splits[word]</span></span>
<span class="line"><span style="color:#e1e4e8;">        if len(split) == 1:</span></span>
<span class="line"><span style="color:#e1e4e8;">            continue</span></span>
<span class="line"><span style="color:#e1e4e8;">        i = 0</span></span>
<span class="line"><span style="color:#e1e4e8;">        while i &lt; len(split) - 1:</span></span>
<span class="line"><span style="color:#e1e4e8;">            if split[i] == a and split[i + 1] == b:</span></span>
<span class="line"><span style="color:#e1e4e8;">                new_word = a + b[2:] if b.startswith(&#39;##&#39;) else a + b</span></span>
<span class="line"><span style="color:#e1e4e8;">                split = split[:i] + [new_word] + split[i + 2:]</span></span>
<span class="line"><span style="color:#e1e4e8;">            i += 1</span></span>
<span class="line"><span style="color:#e1e4e8;">        word_splits[word] = split</span></span>
<span class="line"><span style="color:#e1e4e8;">    return word_splits</span></span>
<span class="line"><span style="color:#e1e4e8;"></span></span>
<span class="line"><span style="color:#e1e4e8;"></span></span>
<span class="line"><span style="color:#e1e4e8;">def wordpiece_train(vocab_size, word_freqs, word_splits):</span></span>
<span class="line"><span style="color:#e1e4e8;">    vocab = get_base_vocab(word_splits)</span></span>
<span class="line"><span style="color:#e1e4e8;">    while len(vocab) &lt; vocab_size:</span></span>
<span class="line"><span style="color:#e1e4e8;">        pair_scores = get_pair_scores(word_freqs, word_splits)</span></span>
<span class="line"><span style="color:#e1e4e8;">        if not pair_scores:</span></span>
<span class="line"><span style="color:#e1e4e8;">            break</span></span>
<span class="line"><span style="color:#e1e4e8;">        max_score_pair = max(pair_scores, key=pair_scores.get)</span></span>
<span class="line"><span style="color:#e1e4e8;">        word_splits = merge_pair(max_score_pair, word_splits)</span></span>
<span class="line"><span style="color:#e1e4e8;">        a, b = max_score_pair</span></span>
<span class="line"><span style="color:#e1e4e8;">        new_word = a + b[2:] if b.startswith(&#39;##&#39;) else a + b</span></span>
<span class="line"><span style="color:#e1e4e8;">        vocab.append(new_word)</span></span>
<span class="line"><span style="color:#e1e4e8;">    return vocab</span></span>
<span class="line"><span style="color:#e1e4e8;"></span></span>
<span class="line"><span style="color:#e1e4e8;"></span></span>
<span class="line"><span style="color:#e1e4e8;"></span></span>
<span class="line"><span style="color:#e1e4e8;">&quot;&quot;&quot;</span></span>
<span class="line"><span style="color:#e1e4e8;">I love you very much</span></span>
<span class="line"><span style="color:#e1e4e8;">I am very hungry</span></span>
<span class="line"><span style="color:#e1e4e8;">&quot;&quot;&quot;</span></span>
<span class="line"><span style="color:#e1e4e8;">corpus = [&#39;I&#39;, &#39;love&#39;, &#39;you&#39;, &#39;very&#39;, &#39;much&#39;, &#39;I&#39;, &#39;am&#39;, &#39;very&#39;, &#39;hungry&#39;]</span></span>
<span class="line"><span style="color:#e1e4e8;">word_freqs = get_word_freqs(corpus)</span></span>
<span class="line"><span style="color:#e1e4e8;">print(word_freqs)</span></span>
<span class="line"><span style="color:#e1e4e8;">word_splits = get_word_splits(word_freqs)</span></span>
<span class="line"><span style="color:#e1e4e8;">print(word_splits)</span></span>
<span class="line"><span style="color:#e1e4e8;">vocab = wordpiece_train(30, word_freqs, word_splits)</span></span>
<span class="line"><span style="color:#e1e4e8;"># print(vocab)</span></span></code></pre><pre class="shiki github-light vp-code-light"><code><span class="line"><span style="color:#24292e;">from collections import defaultdict</span></span>
<span class="line"><span style="color:#24292e;">from typing import Dict, List, Tuple</span></span>
<span class="line"><span style="color:#24292e;"></span></span>
<span class="line"><span style="color:#24292e;">def get_word_freqs(pre_tokenized_res: List[str]) -&gt; Dict[str, int]:</span></span>
<span class="line"><span style="color:#24292e;">    word_freqs = defaultdict(int)</span></span>
<span class="line"><span style="color:#24292e;">    for word in pre_tokenized_res:</span></span>
<span class="line"><span style="color:#24292e;">        word_freqs[word] += 1</span></span>
<span class="line"><span style="color:#24292e;">    return word_freqs</span></span>
<span class="line"><span style="color:#24292e;"></span></span>
<span class="line"><span style="color:#24292e;"></span></span>
<span class="line"><span style="color:#24292e;">def get_word_splits(word_freqs: Dict[str, int]) -&gt; Dict[str, List[str]]:</span></span>
<span class="line"><span style="color:#24292e;">    word_splits = {}</span></span>
<span class="line"><span style="color:#24292e;">    for word in word_freqs.keys():</span></span>
<span class="line"><span style="color:#24292e;">        word_splits[word] = [c if i == 0 else f&quot;##{c}&quot; for i, c in enumerate(word)]</span></span>
<span class="line"><span style="color:#24292e;">    return word_splits</span></span>
<span class="line"><span style="color:#24292e;"></span></span>
<span class="line"><span style="color:#24292e;"></span></span>
<span class="line"><span style="color:#24292e;">def get_base_vocab(word_splits: Dict[str, List[str]]) -&gt; List[str]:</span></span>
<span class="line"><span style="color:#24292e;">    vocab = []</span></span>
<span class="line"><span style="color:#24292e;">    for split in word_splits.values():</span></span>
<span class="line"><span style="color:#24292e;">        vocab.extend(split)</span></span>
<span class="line"><span style="color:#24292e;">    vocab = sorted(list(set(vocab)))</span></span>
<span class="line"><span style="color:#24292e;">    return vocab</span></span>
<span class="line"><span style="color:#24292e;"></span></span>
<span class="line"><span style="color:#24292e;"></span></span>
<span class="line"><span style="color:#24292e;">def get_pair_scores(</span></span>
<span class="line"><span style="color:#24292e;">    word_freqs: Dict[str, int],</span></span>
<span class="line"><span style="color:#24292e;">    word_splits: Dict[str, List[str]],</span></span>
<span class="line"><span style="color:#24292e;">) -&gt; Dict[Tuple[str, str], int]:</span></span>
<span class="line"><span style="color:#24292e;">    pair_freqs = defaultdict(int)</span></span>
<span class="line"><span style="color:#24292e;">    subword_freqs = defaultdict(int)</span></span>
<span class="line"><span style="color:#24292e;"></span></span>
<span class="line"><span style="color:#24292e;">    for freq, split in zip(word_freqs.values(), word_splits.values()):</span></span>
<span class="line"><span style="color:#24292e;">        if len(split) == 1:</span></span>
<span class="line"><span style="color:#24292e;">            subword_freqs[split[0]] += freq</span></span>
<span class="line"><span style="color:#24292e;">            continue</span></span>
<span class="line"><span style="color:#24292e;">        for i in range(len(split) - 1):</span></span>
<span class="line"><span style="color:#24292e;">            pair_freqs[(split[i], split[i + 1])] += freq</span></span>
<span class="line"><span style="color:#24292e;">            subword_freqs[split[i]] += freq</span></span>
<span class="line"><span style="color:#24292e;">        subword_freqs[split[-1]] += freq</span></span>
<span class="line"><span style="color:#24292e;"></span></span>
<span class="line"><span style="color:#24292e;">    pair_scores = {</span></span>
<span class="line"><span style="color:#24292e;">        pair: freq / (subword_freqs[pair[0]] * subword_freqs[pair[1]]) \\</span></span>
<span class="line"><span style="color:#24292e;">        for pair, freq in pair_freqs.items()</span></span>
<span class="line"><span style="color:#24292e;">    }</span></span>
<span class="line"><span style="color:#24292e;">    return pair_scores</span></span>
<span class="line"><span style="color:#24292e;"></span></span>
<span class="line"><span style="color:#24292e;"></span></span>
<span class="line"><span style="color:#24292e;">def merge_pair(pair, word_splits):</span></span>
<span class="line"><span style="color:#24292e;">    a, b = pair</span></span>
<span class="line"><span style="color:#24292e;">    for word in word_splits.keys():</span></span>
<span class="line"><span style="color:#24292e;">        split = word_splits[word]</span></span>
<span class="line"><span style="color:#24292e;">        if len(split) == 1:</span></span>
<span class="line"><span style="color:#24292e;">            continue</span></span>
<span class="line"><span style="color:#24292e;">        i = 0</span></span>
<span class="line"><span style="color:#24292e;">        while i &lt; len(split) - 1:</span></span>
<span class="line"><span style="color:#24292e;">            if split[i] == a and split[i + 1] == b:</span></span>
<span class="line"><span style="color:#24292e;">                new_word = a + b[2:] if b.startswith(&#39;##&#39;) else a + b</span></span>
<span class="line"><span style="color:#24292e;">                split = split[:i] + [new_word] + split[i + 2:]</span></span>
<span class="line"><span style="color:#24292e;">            i += 1</span></span>
<span class="line"><span style="color:#24292e;">        word_splits[word] = split</span></span>
<span class="line"><span style="color:#24292e;">    return word_splits</span></span>
<span class="line"><span style="color:#24292e;"></span></span>
<span class="line"><span style="color:#24292e;"></span></span>
<span class="line"><span style="color:#24292e;">def wordpiece_train(vocab_size, word_freqs, word_splits):</span></span>
<span class="line"><span style="color:#24292e;">    vocab = get_base_vocab(word_splits)</span></span>
<span class="line"><span style="color:#24292e;">    while len(vocab) &lt; vocab_size:</span></span>
<span class="line"><span style="color:#24292e;">        pair_scores = get_pair_scores(word_freqs, word_splits)</span></span>
<span class="line"><span style="color:#24292e;">        if not pair_scores:</span></span>
<span class="line"><span style="color:#24292e;">            break</span></span>
<span class="line"><span style="color:#24292e;">        max_score_pair = max(pair_scores, key=pair_scores.get)</span></span>
<span class="line"><span style="color:#24292e;">        word_splits = merge_pair(max_score_pair, word_splits)</span></span>
<span class="line"><span style="color:#24292e;">        a, b = max_score_pair</span></span>
<span class="line"><span style="color:#24292e;">        new_word = a + b[2:] if b.startswith(&#39;##&#39;) else a + b</span></span>
<span class="line"><span style="color:#24292e;">        vocab.append(new_word)</span></span>
<span class="line"><span style="color:#24292e;">    return vocab</span></span>
<span class="line"><span style="color:#24292e;"></span></span>
<span class="line"><span style="color:#24292e;"></span></span>
<span class="line"><span style="color:#24292e;"></span></span>
<span class="line"><span style="color:#24292e;">&quot;&quot;&quot;</span></span>
<span class="line"><span style="color:#24292e;">I love you very much</span></span>
<span class="line"><span style="color:#24292e;">I am very hungry</span></span>
<span class="line"><span style="color:#24292e;">&quot;&quot;&quot;</span></span>
<span class="line"><span style="color:#24292e;">corpus = [&#39;I&#39;, &#39;love&#39;, &#39;you&#39;, &#39;very&#39;, &#39;much&#39;, &#39;I&#39;, &#39;am&#39;, &#39;very&#39;, &#39;hungry&#39;]</span></span>
<span class="line"><span style="color:#24292e;">word_freqs = get_word_freqs(corpus)</span></span>
<span class="line"><span style="color:#24292e;">print(word_freqs)</span></span>
<span class="line"><span style="color:#24292e;">word_splits = get_word_splits(word_freqs)</span></span>
<span class="line"><span style="color:#24292e;">print(word_splits)</span></span>
<span class="line"><span style="color:#24292e;">vocab = wordpiece_train(30, word_freqs, word_splits)</span></span>
<span class="line"><span style="color:#24292e;"># print(vocab)</span></span></code></pre></div><h1 id="unigram" tabindex="-1">Unigram <a class="header-anchor" href="#unigram" aria-label="Permalink to &quot;Unigram&quot;">​</a></h1><p><a href="https://blog.csdn.net/raelum/article/details/132225006" target="_blank" rel="noreferrer">https://blog.csdn.net/raelum/article/details/132225006</a></p><p>Unigram 语言建模2018在 《 Improving neural network translation models with multiple subword candidates》 中提出。与WordPiece一样，Unigram Language Model(ULM)同样使用语言模型来挑选子词。不同之处在于，BPE和WordPiece算法的词表大小都是从小到大变化，属于增量法。而Unigram Language Model则是减量法,即先初始化一个大词表，根据评估准则不断丢弃词表，直到满足限定条件。UniLM算法考虑了句子的不同分词可能，因而能够输出带概率的多个子词分段。</p><p>我们接下来看看UniLM是如何操作的。</p><p>假设xi是句子的分词结果，当前分词下句子S的似然值可以表示为：</p><p><img src="`+m+'" alt="image-20231214185027625"></p><p>对于句子S，挑选似然值最大的作为分词结果，则可以表示为</p><p><img src="'+y+'" alt="image-20231214185123914"></p><p>在实际应用中，词表大小有上万个，直接罗列所有可能的分词组合不具有操作性。针对这个问题，可通过<strong>维特比算法</strong>来解决（请参考N-gram模型）。</p><p><strong>那怎么求解每个子词的概率P(xi)？</strong></p><p>ULM通过EM算法来估计。假设当前词表V, 则M步最大化的对象是如下似然函数：</p><p><img src="'+Q+`" alt="image-20231214185251951"></p><p>其中，|D|是语料库中语料数量。上述公式的一个直观理解是，将语料库中所有句子的所有分词组合形成的概率相加。</p><p>但是，初始时，词表V并不存在。因而，ULM算法采用不断迭代的方法来构造词表以及求解分词概率：</p><div class="language-text vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">text</span><pre class="shiki github-dark vp-code-dark"><code><span class="line"><span style="color:#e1e4e8;">1.初始时，建立一个足够大的词表。一般，可用语料中的所有字符加上常见的子字符串初始化词表，也可以通过BPE算法初始化。</span></span>
<span class="line"><span style="color:#e1e4e8;">2.针对当前词表，用EM算法求解每个子词在语料上的概率。</span></span>
<span class="line"><span style="color:#e1e4e8;">3.对于每个子词，计算当该子词被从词表中移除时，总的loss降低了多少，记为该子词的loss。</span></span>
<span class="line"><span style="color:#e1e4e8;">4.将子词按照loss大小进行排序，丢弃一定比例loss最小的子词(比如20%)，保留下来的子词生成新的词表。这里需要注意的是，单字符不能被丢弃，这是为了避免OOV情况。</span></span>
<span class="line"><span style="color:#e1e4e8;">5.重复步骤2到4，直到词表大小减少到设定范围。</span></span></code></pre><pre class="shiki github-light vp-code-light"><code><span class="line"><span style="color:#24292e;">1.初始时，建立一个足够大的词表。一般，可用语料中的所有字符加上常见的子字符串初始化词表，也可以通过BPE算法初始化。</span></span>
<span class="line"><span style="color:#24292e;">2.针对当前词表，用EM算法求解每个子词在语料上的概率。</span></span>
<span class="line"><span style="color:#24292e;">3.对于每个子词，计算当该子词被从词表中移除时，总的loss降低了多少，记为该子词的loss。</span></span>
<span class="line"><span style="color:#24292e;">4.将子词按照loss大小进行排序，丢弃一定比例loss最小的子词(比如20%)，保留下来的子词生成新的词表。这里需要注意的是，单字符不能被丢弃，这是为了避免OOV情况。</span></span>
<span class="line"><span style="color:#24292e;">5.重复步骤2到4，直到词表大小减少到设定范围。</span></span></code></pre></div><p>可以看出，ULM会保留那些以较高频率出现在很多句子的分词结果中的子词，因为这些子词如果被丢弃，其损失会很大。</p>`,32),L={class:"MathJax",jax:"SVG",style:{direction:"ltr",position:"relative"}},V={style:{overflow:"visible","min-height":"1px","min-width":"1px","vertical-align":"-0.357ex"},xmlns:"http://www.w3.org/2000/svg",width:"2.36ex",height:"1.359ex",role:"img",focusable:"false",viewBox:"0 -443 1043 600.8","aria-hidden":"true"},P=e('<g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(749,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z" style="stroke-width:3;"></path></g></g></g></g>',1),M=[P],O=s("mjx-assistive-mml",{unselectable:"on",display:"inline",style:{top:"0px",left:"0px",clip:"rect(1px, 1px, 1px, 1px)","-webkit-touch-callout":"none","-webkit-user-select":"none","-khtml-user-select":"none","-moz-user-select":"none","-ms-user-select":"none","user-select":"none",position:"absolute",padding:"1px 0px 0px 0px",border:"0px",display:"block",width:"auto",overflow:"hidden"}},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("msub",null,[s("mi",null,"w"),s("mi",null,"i")])])],-1),Z={class:"MathJax",jax:"SVG",style:{direction:"ltr",position:"relative"}},H={style:{overflow:"visible","min-height":"1px","min-width":"1px","vertical-align":"-0.592ex"},xmlns:"http://www.w3.org/2000/svg",width:"2.36ex",height:"2.309ex",role:"img",focusable:"false",viewBox:"0 -759 1043 1020.7","aria-hidden":"true"},A=e('<g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msubsup"><g data-mml-node="mi"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(749,363) scale(0.707)"><path data-c="2032" d="M79 43Q73 43 52 49T30 61Q30 68 85 293T146 528Q161 560 198 560Q218 560 240 545T262 501Q262 496 260 486Q259 479 173 263T84 45T79 43Z" style="stroke-width:3;"></path></g><g data-mml-node="TeXAtom" transform="translate(749,-254) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z" style="stroke-width:3;"></path></g></g></g></g></g>',1),N=[A],C=s("mjx-assistive-mml",{unselectable:"on",display:"inline",style:{top:"0px",left:"0px",clip:"rect(1px, 1px, 1px, 1px)","-webkit-touch-callout":"none","-webkit-user-select":"none","-khtml-user-select":"none","-moz-user-select":"none","-ms-user-select":"none","user-select":"none",position:"absolute",padding:"1px 0px 0px 0px",border:"0px",display:"block",width:"auto",overflow:"hidden"}},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("msubsup",null,[s("mi",null,"w"),s("mrow",{"data-mjx-texclass":"ORD"},[s("mi",null,"i")]),s("mo",{"data-mjx-alternate":"1"},"′")])])],-1),z=s("p",null,[s("img",{src:h,alt:"image-20231214191606352"})],-1),F=s("p",null,"接下来，Unigram会为vocab中的每个子词计算一个score，这个score等于从vocab中移除该子词后loss的变化值。可以证明，这个值一定是非负的，因此score越小说明这个子词越不重要，故可以移除。",-1),W=s("p",null,"将vocab的子词按照score从小到大进行排序，并移除前10-20%，然后重新计算loss，反复执行，直到vocab大小符合要求。",-1);function E(D,I,S,B,R,U){return a(),l("div",null,[f,s("p",null,[n("假设句子"),s("mjx-container",_,[(a(),l("svg",b,v)),q]),n("，则句子S的语言模型似然值是：")]),k,s("p",null,[n("设语料库中单词为"),s("mjx-container",L,[(a(),l("svg",V,M)),O]),n("，分词后的单词为"),s("mjx-container",Z,[(a(),l("svg",H,N)),C]),n(", 那么当该子词被从词表中移除时，整个语料库的loss可以表示为：")]),z,F,W])}const G=p(u,[["render",E]]);export{Y as __pageData,G as default};
