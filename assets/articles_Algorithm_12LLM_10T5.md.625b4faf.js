import{_ as e,a as r,b as o,c as t}from"./chunks/640-1574825187478.fea00b2f.js";import{_ as a,o as l,c as s,Q as i}from"./chunks/framework.2516552c.js";const D=JSON.parse('{"title":"","description":"","frontmatter":{},"headers":[],"relativePath":"articles/Algorithm/12LLM/10T5.md","filePath":"articles/Algorithm/12LLM/10T5.md","lastUpdated":null}'),n={name:"articles/Algorithm/12LLM/10T5.md"},p=i('<p>参考：</p><ul><li><a href="https://mp.weixin.qq.com/s/ifZ_wnJkgRhu0Aa8-6u11Q" target="_blank" rel="noreferrer">https://mp.weixin.qq.com/s/ifZ_wnJkgRhu0Aa8-6u11Q</a></li></ul><p>2019/10 T5 (from Google)</p><p>Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</p><p><a href="https://arxiv.org/abs/1910.10683v1" target="_blank" rel="noreferrer">https://arxiv.org/abs/1910.10683v1</a></p><p>论文50页，是一个综述。</p><p>T5是一个超大预训练模型，把所有的任务都转化成一种形式。</p><p><img src="'+e+'" alt="img"></p><p>上面是T5完成的四个任务：翻译、情感分类、文本语义相似度任务（1-5分）、摘要</p><h3 id="data-c4" tabindex="-1">Data: C4 <a class="header-anchor" href="#data-c4" aria-label="Permalink to &quot;Data: C4&quot;">​</a></h3><p>作者从 Common Crawl（一个公开的网页存档数据集，每个月大概抓取 20TB 文本数据） 里清出了 750 GB 的训练数据，然后取名为 ” Colossal Clean Crawled Corpus （超大型干净爬取数据）“，简称 C4，论作者取名之恶趣味。</p><h3 id="architecture" tabindex="-1">Architecture <a class="header-anchor" href="#architecture" aria-label="Permalink to &quot;Architecture&quot;">​</a></h3><p>作者对多种模型架构进行了对比，主要是如下三种：</p><p><img src="'+r+'" alt="img"></p><p>第一种，<strong>Encoder-Decoder 型</strong>，即 Seq2Seq 常用模型，分成 Encoder 和 Decoder 两部分，对于 Encoder 部分，输入可以看到全体，之后结果输给 Decoder，而 Decoder 因为输出方式只能看到之前的。此架构代表是 MASS（今年WMT的胜者），而 BERT 可以看作是其中 Encoder 部分。</p><p>第二种， 相当于上面的 <strong>Decoder 部分</strong>，当前时间步只能看到之前时间步信息。典型代表是 GPT2 还有最近 CTRL 这样的。</p><p>第三种，<strong>Prefix LM（Language Model） 型</strong>，可看作是上面 Encoder 和 Decoder 的融合体，一部分如 Encoder 一样能看到全体信息，一部分如 Decoder 一样只能看到过去信息。最近开源的 UniLM 便是此结构。</p><p>上面这些模型架构都是 Transformer 构成，之所以有这些变换，主要是<strong>对其中注意力机制的 Mask 操作</strong>。</p><p><img src="'+o+'" alt="img"></p><p>通过实验作者们发现，在提出的这个 Text-to-Text 架构中，Encoder-Decoder 模型效果最好。于是乎，就把它定为 T5 模型，因此<strong>所谓的 T5 模型其实就是个 Transformer 的 Encoder-Decoder 模型</strong>。</p><h3 id="objectives-重点" tabindex="-1">Objectives 重点 <a class="header-anchor" href="#objectives-重点" aria-label="Permalink to &quot;Objectives 重点&quot;">​</a></h3><p>之后是对预训练目标的大范围探索，具体做了哪些实验，下面这张图就能一目了然。</p><p><img src="'+t+'" alt="img"></p><p>总共从四方面来进行比较。</p><p>第一个方面，<strong>高层次方法（自监督的预训练方法）对比</strong>，总共三种方式。</p><ol><li><strong>语言模型式</strong>，就是 GPT-2 那种方式，从左到右预测；</li><li><strong>BERT-style 式</strong>，就是像 BERT 一样将一部分给破坏掉，然后还原出来；</li><li><strong>Deshuffling （顺序还原）式</strong>，就是将文本打乱，然后还原出来。</li></ol><p>其中发现 Bert-style 最好，进入下一轮。</p><p>第二方面，对文本一部分进行<strong>破坏时的策略</strong>，也分三种方法。</p><ol><li><strong>Mask 法</strong>，如现在大多模型的做法，将被破坏 token 换成特殊符如 [M]；</li><li><strong>replace span（小段替换）法</strong>，可以把它当作是把上面 Mask 法中相邻 [M] 都合成了一个特殊符，每一小段替换一个特殊符，提高计算效率；</li><li><strong>Drop 法</strong>，没有替换操作，直接随机丢弃一些字符。</li></ol><p>此轮获胜的是 <strong>Replace Span 法</strong>，类似做法如 SpanBERT 也证明了有效性。</p><p>第三方面，到底该<strong>对文本百分之多少进行破坏</strong>呢，挑了 4 个值，10%，15%，25%，50%，最后发现 BERT 的 <strong>15%</strong> 就很 ok了。这时不得不感叹 BERT 作者 Devlin 这个技术老司机直觉的厉害。</p><p>接着进入更细节，第四方面，因为 Replace Span 需要决定<strong>对大概多长的小段进行破坏</strong>，于是对不同长度进行探索，2，3，5，10 这四个值，最后发现 <strong>3</strong> 结果最好。</p><p>终于获得了完整的 T5 模型，还有它的训练方法。</p><ul><li>Transformer Encoder-Decoder 模型；</li><li>BERT-style 式的破坏方法；</li><li>Replace Span 的破坏策略；</li><li>15 %的破坏比；</li><li>3 的破坏时小段长度。</li></ul><p>到此基本上 T5 预训练就大致说完了，之后是些细碎探索。</p><h3 id="models" tabindex="-1">Models <a class="header-anchor" href="#models" aria-label="Permalink to &quot;Models&quot;">​</a></h3><p>训练了不同规模的模型：</p><ul><li><p>Small，Encoder 和 Decoder 都只有 6 层，隐维度 512，8 头；</p></li><li><p>Base，相当于 Encoder 和 Decoder 都用 BERT-base；</p></li><li><p>Large，Encoder 和 Decoder 都用 BERT-large 设置，除了层数只用 12 层；</p></li><li><p>3B（Billion）和11B，层数都用 24 层，不同的是其中头数量和前向层的维度。</p></li></ul><p>11B 的模型最后在 GLUE，SuperGLUE，SQuAD，还有 CNN/DM 上取得了 SOTA，而 WMT(翻译) 则没有。</p><h3 id="总结" tabindex="-1">总结 <a class="header-anchor" href="#总结" aria-label="Permalink to &quot;总结&quot;">​</a></h3><ol><li>本文对多模型多参数进行了对比，找到了一个较好的模型</li><li>大模型大数据表现越好</li></ol>',41),c=[p];function d(g,h,m,T,_,u){return l(),s("div",null,c)}const b=a(n,[["render",d]]);export{D as __pageData,b as default};
