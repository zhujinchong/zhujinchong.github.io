import{_ as a,o as e,c as t,Q as r}from"./chunks/framework.2516552c.js";const s="/assets/image-20191108155555629.832915bb.png",i="/assets/image-20191108160021083.2816cc89.png",o="/assets/v2-a6ef4d4451b4425eb713e57ddc2e33cc_hd.5bb9b43f.jpg",n="/assets/image-20191108160608150.317def5d.png",P=JSON.parse('{"title":"","description":"","frontmatter":{},"headers":[],"relativePath":"articles/算法/12LLM/05GPT01论文.md","filePath":"articles/算法/12LLM/05GPT01论文.md","lastUpdated":1698149129000}'),_={name:"articles/算法/12LLM/05GPT01论文.md"},c=r('<p>2018年OpenAI 《Improving Language Understanding by Generative Pre-Training》</p><h3 id="_1-前言" tabindex="-1">1. 前言 <a class="header-anchor" href="#_1-前言" aria-label="Permalink to &quot;1. 前言&quot;">​</a></h3><p>本文中，我们为自然语言理解任务提出了一种新的半监督方式，包含半监督预训练和有监督微调。我们的目标是学到一种通用的表示，然后在不同任务上微调。 我们的应用方式包含两步，第一步在语言模型上使用无标注的数据预训练，然后在目标任务上直接使用预训练模型的参数作为初始化参数，再加上合适的目标函数。</p><p>模型结构使用Transformer的Decoder结构。</p><h3 id="_2-相关介绍" tabindex="-1">2. 相关介绍 <a class="header-anchor" href="#_2-相关介绍" aria-label="Permalink to &quot;2. 相关介绍&quot;">​</a></h3><h3 id="_3-架构" tabindex="-1">3. 架构 <a class="header-anchor" href="#_3-架构" aria-label="Permalink to &quot;3. 架构&quot;">​</a></h3><p>我们的训练流程包含两个阶段，即半监督预训练和有监督微调。</p><h3 id="_3-1-半监督预训练" tabindex="-1">3.1 半监督预训练 <a class="header-anchor" href="#_3-1-半监督预训练" aria-label="Permalink to &quot;3.1 半监督预训练&quot;">​</a></h3><p><img src="'+s+'" alt="image-20191108155555629"></p><h3 id="_3-2-有监督微调" tabindex="-1">3.2 有监督微调 <a class="header-anchor" href="#_3-2-有监督微调" aria-label="Permalink to &quot;3.2 有监督微调&quot;">​</a></h3><p><img src="'+i+'" alt="image-20191108160021083"></p><p><img src="'+o+'" alt="img"></p><h3 id="_3-3-特殊任务的输入变换" tabindex="-1">3.3 特殊任务的输入变换 <a class="header-anchor" href="#_3-3-特殊任务的输入变换" aria-label="Permalink to &quot;3.3 特殊任务的输入变换&quot;">​</a></h3><p>对于有些任务，像文本分类，我们能够直接用上文的模型进行微调。另外的任务，问答系统，需要构造输入的句子对，或者三个文本。由于我们的预训练模型需要连续的文本序列，我们需要改变这种多句文本的输入。</p><p><img src="'+n+'" alt="image-20191108160608150"></p><h3 id="_4-结论" tabindex="-1">4. 结论 <a class="header-anchor" href="#_4-结论" aria-label="Permalink to &quot;4. 结论&quot;">​</a></h3><p>通过预训练是不同的长文本连续的数据集，模型能够有能力去处理长而广的依赖关系，这个是解决问答系统、语义相似度、文本分类中的关键点。</p>',17),d=[c];function l(h,p,m,b,g,u){return e(),t("div",null,d)}const q=a(_,[["render",l]]);export{P as __pageData,q as default};
