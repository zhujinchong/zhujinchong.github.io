import{_ as a,o as e,c as n,Q as r}from"./chunks/framework.2516552c.js";const s="/assets/upload_8f9f065ff0511f969cf5fe279862555f.45095cfa.png",t="/assets/image-20230701165731045.f77859ef.png",g=JSON.parse('{"title":"综述","description":"","frontmatter":{},"headers":[],"relativePath":"articles/Algorithm/13LLM/00LLM发展2017-2013.md","filePath":"articles/Algorithm/13LLM/00LLM发展2017-2013.md","lastUpdated":null}'),p={name:"articles/Algorithm/13LLM/00LLM发展2017-2013.md"},l=r(`<h1 id="综述" tabindex="-1">综述 <a class="header-anchor" href="#综述" aria-label="Permalink to &quot;综述&quot;">​</a></h1><h2 id="文字版" tabindex="-1">文字版 <a class="header-anchor" href="#文字版" aria-label="Permalink to &quot;文字版&quot;">​</a></h2><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki github-dark vp-code-dark"><code><span class="line"><span style="color:#e1e4e8;">2017.06 Google发布Transformaer；DeepMind提出RLHF方法</span></span>
<span class="line"><span style="color:#e1e4e8;">2018.06 OpenAI发布基于Transformer Decoder的GPT-1，1.1亿参数</span></span>
<span class="line"><span style="color:#e1e4e8;">2018.10 Google发布基于Transformer Encoder的BERT，最大3.5亿参数</span></span>
<span class="line"><span style="color:#e1e4e8;">2019.02 OpenAI发布GPT-2，15亿参数</span></span>
<span class="line"><span style="color:#e1e4e8;">2019.10 Google发布基于Encoder-Decoder的T5,兼容BERT和GPT的下游任务</span></span>
<span class="line"><span style="color:#e1e4e8;">2020.05 OpenAI发布GPT-3，1750亿参数</span></span>
<span class="line"><span style="color:#e1e4e8;">2021.10 Google发布FLAN，转向Decoder-only,提出Instruction Tuning</span></span>
<span class="line"><span style="color:#e1e4e8;">2021.11 DeepMind发布Gopher, 2800亿参数，加入LLM大战</span></span>
<span class="line"><span style="color:#e1e4e8;">2022.01 Google发布LaMDA，1380亿参数</span></span>
<span class="line"><span style="color:#e1e4e8;">2022.03 OpenAI发布InstructGPT，用到Instruction Tuning和RLHF</span></span>
<span class="line"><span style="color:#e1e4e8;">2022.04 Google发布PaLM，5400亿参数</span></span>
<span class="line"><span style="color:#e1e4e8;">2022.09 Tsinghua发布GLM-130B，开源</span></span>
<span class="line"><span style="color:#e1e4e8;">2022.09 DeepMind发布Sparrow，加入RHLF和Retrival</span></span>
<span class="line"><span style="color:#e1e4e8;">2022.12 OpenAI发布ChatGPT</span></span>
<span class="line"><span style="color:#e1e4e8;">2023.02 Google发布Bard</span></span>
<span class="line"><span style="color:#e1e4e8;">2023.02 Meta发布LLaMA， 开源</span></span></code></pre><pre class="shiki github-light vp-code-light"><code><span class="line"><span style="color:#24292e;">2017.06 Google发布Transformaer；DeepMind提出RLHF方法</span></span>
<span class="line"><span style="color:#24292e;">2018.06 OpenAI发布基于Transformer Decoder的GPT-1，1.1亿参数</span></span>
<span class="line"><span style="color:#24292e;">2018.10 Google发布基于Transformer Encoder的BERT，最大3.5亿参数</span></span>
<span class="line"><span style="color:#24292e;">2019.02 OpenAI发布GPT-2，15亿参数</span></span>
<span class="line"><span style="color:#24292e;">2019.10 Google发布基于Encoder-Decoder的T5,兼容BERT和GPT的下游任务</span></span>
<span class="line"><span style="color:#24292e;">2020.05 OpenAI发布GPT-3，1750亿参数</span></span>
<span class="line"><span style="color:#24292e;">2021.10 Google发布FLAN，转向Decoder-only,提出Instruction Tuning</span></span>
<span class="line"><span style="color:#24292e;">2021.11 DeepMind发布Gopher, 2800亿参数，加入LLM大战</span></span>
<span class="line"><span style="color:#24292e;">2022.01 Google发布LaMDA，1380亿参数</span></span>
<span class="line"><span style="color:#24292e;">2022.03 OpenAI发布InstructGPT，用到Instruction Tuning和RLHF</span></span>
<span class="line"><span style="color:#24292e;">2022.04 Google发布PaLM，5400亿参数</span></span>
<span class="line"><span style="color:#24292e;">2022.09 Tsinghua发布GLM-130B，开源</span></span>
<span class="line"><span style="color:#24292e;">2022.09 DeepMind发布Sparrow，加入RHLF和Retrival</span></span>
<span class="line"><span style="color:#24292e;">2022.12 OpenAI发布ChatGPT</span></span>
<span class="line"><span style="color:#24292e;">2023.02 Google发布Bard</span></span>
<span class="line"><span style="color:#24292e;">2023.02 Meta发布LLaMA， 开源</span></span></code></pre></div><h2 id="树形图" tabindex="-1">树形图 <a class="header-anchor" href="#树形图" aria-label="Permalink to &quot;树形图&quot;">​</a></h2><p>综述：Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond</p><p>这篇论文就这张图重要</p><p><img src="`+s+'" alt="img"></p><h2 id="时间图" tabindex="-1">时间图 <a class="header-anchor" href="#时间图" aria-label="Permalink to &quot;时间图&quot;">​</a></h2><p>综述：A Survey of large language models</p><p><img src="'+t+'" alt="image-20230701165731045"></p><h2 id="文章" tabindex="-1">文章 <a class="header-anchor" href="#文章" aria-label="Permalink to &quot;文章&quot;">​</a></h2><p>张俊林的LLM技术精要：</p><p>[通向AGI之路：大型语言模型（LLM）技术精要 - 知乎 (zhihu.com)]：</p><p><a href="https://zhuanlan.zhihu.com/p/597586623" target="_blank" rel="noreferrer">https://zhuanlan.zhihu.com/p/597586623</a></p><p>笔记：把一些重要知识点都写出来，比综述性的论文要牛！必读！</p><h1 id="transformer" tabindex="-1">Transformer <a class="header-anchor" href="#transformer" aria-label="Permalink to &quot;Transformer&quot;">​</a></h1><h2 id="注意力机制" tabindex="-1">注意力机制 <a class="header-anchor" href="#注意力机制" aria-label="Permalink to &quot;注意力机制&quot;">​</a></h2><p>什么是注意力机制：<a href="https://blog.csdn.net/Tink1995/article/details/105012972" target="_blank" rel="noreferrer">https://blog.csdn.net/Tink1995/article/details/105012972</a></p><p>笔记：</p><p>1、基于RNN的Encoder-Decoder结构，缺点：预测长序列效果不好；</p><p>2、将Encoder-Decoder引入注意力机制Attention：</p><ul><li><p>优点：Decoder可以并行训练；注意力机制效果好。</p></li><li><p>缺点：Encoder不能并行训练；Encoder处理长序列效果不好。</p></li></ul><h2 id="transformer-1" tabindex="-1">Transformer <a class="header-anchor" href="#transformer-1" aria-label="Permalink to &quot;Transformer&quot;">​</a></h2><p>Transformer的结构的详细讲解：<a href="https://zhuanlan.zhihu.com/p/338817680" target="_blank" rel="noreferrer">https://zhuanlan.zhihu.com/p/338817680</a></p><p>Transformer论文翻译解读：<a href="https://blog.csdn.net/nocml/article/details/103082600" target="_blank" rel="noreferrer">https://blog.csdn.net/nocml/article/details/103082600</a></p><p>现有问题：RNN不能并行训练</p><p>论文创新：</p><p>1、Transformer提出的出发点：RNN结果不能并行训练。</p><h1 id="大模型" tabindex="-1">大模型 <a class="header-anchor" href="#大模型" aria-label="Permalink to &quot;大模型&quot;">​</a></h1><h2 id="bert" tabindex="-1">BERT <a class="header-anchor" href="#bert" aria-label="Permalink to &quot;BERT&quot;">​</a></h2><p>论文解读：<a href="https://www.cnblogs.com/anai/p/11645953.html" target="_blank" rel="noreferrer">https://www.cnblogs.com/anai/p/11645953.html</a></p><p>笔记：</p><p>1、BERT是Transformer的Encoder部分</p><p>2、BERT是一个双向的预训练语言模型，采用Masked（掩码/完形填空）和NSP（判断两个句子的时序）方式训练模型。</p><p>3、提出“预训练+微调”，这里微调指全量参数微调。</p><h2 id="gpt-1" tabindex="-1">GPT-1 <a class="header-anchor" href="#gpt-1" aria-label="Permalink to &quot;GPT-1&quot;">​</a></h2><p>论文解读：<a href="https://aigonna.com/2020/12/03/NLP" target="_blank" rel="noreferrer">https://aigonna.com/2020/12/03/NLP</a> Paper 12.1 GPT-1 论文笔记/</p><p>现有问题：</p><p>1、如何在没有标注的数据集上进行预训练；</p><p>2、如何做微调；</p><p>3、如何在每个子任务上表示其输入。</p><p>论文笔记：</p><p>1、GPT是Transformer的Decoder部分（且去掉了Decoder的第二个Multi-Head）</p><p>2、无监督的预训练和监督的微调组合成半监督学习</p><h2 id="gpt-2" tabindex="-1">GPT-2 <a class="header-anchor" href="#gpt-2" aria-label="Permalink to &quot;GPT-2&quot;">​</a></h2><p>论文解读：</p><p>现有问题：</p><p>1、当前系统是狭隘的专家而不是称职的全才（下游需要微调）</p><p>论文笔记：</p><p>1、模型参数15亿</p><p>2、zero-shot，零样本学习，即不用下游微调，直接用预训练模型在多个任务上表现很好。</p><h2 id="gpt-3" tabindex="-1">GPT-3 <a class="header-anchor" href="#gpt-3" aria-label="Permalink to &quot;GPT-3&quot;">​</a></h2><p>论文解读：<a href="https://zhuanlan.zhihu.com/p/200978538" target="_blank" rel="noreferrer">https://zhuanlan.zhihu.com/p/200978538</a></p><p>现有问题：</p><p>1、和GPT2一样，探索不需要微调的大模型</p><p>2、如果不微调，可以使用情景学习In-Context Learning：即few-shot的能力</p><p>论文笔记：</p><p>1、模型参数1750亿</p><p>2、评估用了三种方法：</p><ul><li>zero-shot：一个样本都不提供</li><li>one-shot：提供一个样本</li><li>few-shot：提供大概10-100个样本</li></ul><p>3、大模型更适合In-Context Learning</p><h2 id="xlnet" tabindex="-1">XLNet <a class="header-anchor" href="#xlnet" aria-label="Permalink to &quot;XLNet&quot;">​</a></h2><p>论文解读：<a href="https://zhuanlan.zhihu.com/p/71916499" target="_blank" rel="noreferrer">https://zhuanlan.zhihu.com/p/71916499</a></p><p>论文：谷歌</p><p>现有问题：</p><p>BERT是自编码模型，GPT是自回归模型，如何将两者结合起来？</p><p>论文笔记：</p><p>1、Permutation Language Model：将训练的token打乱(Permutation)，然后掩码进行预测</p><p>2、使用了Transformer-XL作为元模型</p><h2 id="unilm2-0" tabindex="-1">UniLM2.0 <a class="header-anchor" href="#unilm2-0" aria-label="Permalink to &quot;UniLM2.0&quot;">​</a></h2><p>论文解读：<a href="https://zhuanlan.zhihu.com/p/122999153" target="_blank" rel="noreferrer">https://zhuanlan.zhihu.com/p/122999153</a></p><p>论文：微软</p><p>现有问题：</p><p>BERT更适用于NLG（自然语言生成），GPT更适用于NLU（自然语言理解），如何将两者结合起来？</p><p>论文笔记：</p><p>1、将NLG和NLU结合起来了。</p><h2 id="t5" tabindex="-1">T5 <a class="header-anchor" href="#t5" aria-label="Permalink to &quot;T5&quot;">​</a></h2><p>论文解读：<a href="https://zhuanlan.zhihu.com/p/88438851" target="_blank" rel="noreferrer">https://zhuanlan.zhihu.com/p/88438851</a></p><p>时间：2019</p><p>现有问题：</p><p>这时候已经有了GPT2，都在想着做大模型</p><p>论文笔记：</p><p>1、模型最大的11B</p><p>2、基础架构就是Tranformer，为什么呢，因为作者对比了Encoder，Decoder，Encoder-Decoder，发现Encoder-Decoder效果最好。</p><h2 id="flan" tabindex="-1">FLAN <a class="header-anchor" href="#flan" aria-label="Permalink to &quot;FLAN&quot;">​</a></h2><p>论文解读：<a href="https://zhuanlan.zhihu.com/p/580468546" target="_blank" rel="noreferrer">https://zhuanlan.zhihu.com/p/580468546</a></p><h2 id="webgpt" tabindex="-1">WebGPT <a class="header-anchor" href="#webgpt" aria-label="Permalink to &quot;WebGPT&quot;">​</a></h2><h2 id="instructgpt" tabindex="-1">InstructGPT <a class="header-anchor" href="#instructgpt" aria-label="Permalink to &quot;InstructGPT&quot;">​</a></h2><p>论文解读：<a href="https://zhuanlan.zhihu.com/p/590311003" target="_blank" rel="noreferrer">https://zhuanlan.zhihu.com/p/590311003</a></p><p>论文笔记：</p><p>1、指示学习（Instruct Learning）和提示学习（Prompt Learning）</p><p>2、RLHF（人类反馈的强化学习）</p><h2 id="opt" tabindex="-1">OPT <a class="header-anchor" href="#opt" aria-label="Permalink to &quot;OPT&quot;">​</a></h2><h2 id="palm" tabindex="-1">PaLM <a class="header-anchor" href="#palm" aria-label="Permalink to &quot;PaLM&quot;">​</a></h2><h2 id="flan-t5" tabindex="-1">Flan-T5 <a class="header-anchor" href="#flan-t5" aria-label="Permalink to &quot;Flan-T5&quot;">​</a></h2><h2 id="bloom" tabindex="-1">BLOOM <a class="header-anchor" href="#bloom" aria-label="Permalink to &quot;BLOOM&quot;">​</a></h2><h2 id="chatgpt" tabindex="-1">ChatGPT <a class="header-anchor" href="#chatgpt" aria-label="Permalink to &quot;ChatGPT&quot;">​</a></h2><h1 id="大模型2023" tabindex="-1">大模型2023 <a class="header-anchor" href="#大模型2023" aria-label="Permalink to &quot;大模型2023&quot;">​</a></h1><h2 id="bart" tabindex="-1">BART <a class="header-anchor" href="#bart" aria-label="Permalink to &quot;BART&quot;">​</a></h2><p>论文笔记：<a href="https://blog.csdn.net/sinat_41506268/article/details/131189683" target="_blank" rel="noreferrer">https://blog.csdn.net/sinat_41506268/article/details/131189683</a></p><p>1、BART使用了Encoder-Decoder结构</p><p>2、类似于BEAR的放大版</p><h2 id="llama" tabindex="-1">LlaMA <a class="header-anchor" href="#llama" aria-label="Permalink to &quot;LlaMA&quot;">​</a></h2><h2 id="vicuna" tabindex="-1">Vicuna <a class="header-anchor" href="#vicuna" aria-label="Permalink to &quot;Vicuna&quot;">​</a></h2><h2 id="tulu" tabindex="-1">TULU <a class="header-anchor" href="#tulu" aria-label="Permalink to &quot;TULU&quot;">​</a></h2><p>论文笔记：<a href="https://mp.weixin.qq.com/s/9nrHh_JKSBbuILeVxCkmlA" target="_blank" rel="noreferrer">https://mp.weixin.qq.com/s/9nrHh_JKSBbuILeVxCkmlA</a></p><h1 id="其他" tabindex="-1">其他 <a class="header-anchor" href="#其他" aria-label="Permalink to &quot;其他&quot;">​</a></h1><p>杨丽坤的世界模型</p><p>Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture</p><p>解锁世界模型</p><p><a href="https://mp.weixin.qq.com/s/wEN6lj1bMJxvnAWACV9BCw" target="_blank" rel="noreferrer">https://mp.weixin.qq.com/s/wEN6lj1bMJxvnAWACV9BCw</a></p><p>指令调优模型评测</p><p>INSTRUCTEVAL: Towards Holistic Evaluation of Instruction-Tuned Large Language Models</p><p>Meta的开源的文本音乐生成模型</p><p>Simple and Controllable Music Generation</p><p>旋转位置编码</p><p><a href="https://mp.weixin.qq.com/s/e8bLx43NNSIKuyomVq2SZA" target="_blank" rel="noreferrer">https://mp.weixin.qq.com/s/e8bLx43NNSIKuyomVq2SZA</a></p>',117),o=[l];function i(h,c,d,u,m,f){return e(),n("div",null,o)}const T=a(p,[["render",i]]);export{g as __pageData,T as default};
