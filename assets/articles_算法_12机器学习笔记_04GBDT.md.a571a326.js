import{_ as s,o as a,c as n,Q as p}from"./chunks/framework.2516552c.js";const l="/assets/1569743683925.74a1dd7c.png",o="/assets/1569743805843.b44a918b.png",e="/assets/1570688109651.d0f5a94c.png",t="/assets/1570688198317.655566a3.png",r="/assets/1570688421569.78f1801a.png",c="data:image/png;base64,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",i="/assets/1569745789836.25fa85ac.png",y="/assets/1569745884749.8ce1f2c0.png",E="data:image/png;base64,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",m="/assets/1569746410666.1fee8d0f.png",d="/assets/1569746461416.94ca173a.png",A="/assets/1569746621976.97a9d055.png",g="/assets/1569746658585.2f6c2ac7.png",u="/assets/1570689728416.6145e12c.png",D="/assets/1570689774048.0b150143.png",h="data:image/png;base64,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",I=JSON.parse('{"title":"提升树","description":"","frontmatter":{},"headers":[],"relativePath":"articles/算法/12机器学习笔记/04GBDT.md","filePath":"articles/算法/12机器学习笔记/04GBDT.md","lastUpdated":1698165159000}'),F={name:"articles/算法/12机器学习笔记/04GBDT.md"},f=p('<p>Gradient Boosting Regression Tree梯度提升树</p><p>Adaboost用前一轮弱学习器的误差率来更新样本权重</p><p>GBDT要找到一个决策树，让样本的损失尽量变得更小，限制只使用CART回归树。</p><p>GBDT无论分类和回归，都采用回归树，分类问题最终将拟合值转化为概率来进行分类。</p><h1 id="提升树" tabindex="-1">提升树 <a class="header-anchor" href="#提升树" aria-label="Permalink to &quot;提升树&quot;">​</a></h1><p>梯度提升树是提升树的一种改进。所以先讲一下提升树。</p><p><img src="'+l+'" alt="1569743683925"></p><p>上面残差是什么？</p><p><img src="'+o+'" alt="1569743805843"></p><h1 id="gbdt" tabindex="-1">GBDT <a class="header-anchor" href="#gbdt" aria-label="Permalink to &quot;GBDT&quot;">​</a></h1><p>当损失函数是平方损失和指数损失函数时，梯度提升树每一步优化是很简单的，但是对于一般损失函数而言，往往每一步优化起来不那么容易，针对这一问题，Friedman提出了梯度提升树算法，这是利用最速下降的近似方法，<strong>其关键是利用损失函数的负梯度作为提升树算法中的残差的近似值。</strong></p><p>第t轮第i个样本的损失为负梯度：</p><p><img src="'+e+'" alt="1570688109651"></p><p><img src="'+t+'" alt="1570688198317"></p><p>通过损失函数的负梯度来拟合，这是一种通用的方法。</p><h1 id="gbdt回归" tabindex="-1">GBDT回归 <a class="header-anchor" href="#gbdt回归" aria-label="Permalink to &quot;GBDT回归&quot;">​</a></h1><p>为什么没有加上分类算法一起？那是因为分类算法的输出是不连续的类别值，需要一些处理才能使用负梯度。</p><p><img src="'+r+'" alt="1570688421569"></p><h1 id="gbdt分类" tabindex="-1">GBDT分类 <a class="header-anchor" href="#gbdt分类" aria-label="Permalink to &quot;GBDT分类&quot;">​</a></h1><p>样本输出是离散的，怎么办？一是使用指数损失函数，此时GBDT退化为Adaboost算法；另一种是使用类似于逻辑回归的对数似然损失函数的方法。也就是说，我们用的是类别的预测概率值和真实概率值的差来拟合损失。</p><h3 id="_1-二元分类" tabindex="-1">1. 二元分类 <a class="header-anchor" href="#_1-二元分类" aria-label="Permalink to &quot;1. 二元分类&quot;">​</a></h3><p>类似于对数似然损失函数：</p><p><img src="'+c+'" alt="1569745766303"></p><p>此时，y为{-1, +1}，负梯度误差为：</p><p><img src="'+i+'" alt="1569745789836"></p><p>对于生成的决策树，我们的每个叶子结点的最佳负梯度拟合值为：</p><p><img src="'+y+'" alt="1569745884749"></p><p>由于上式比较难优化，一般使用近似值代替：</p><p><img src="'+E+'" alt="1569745969325"></p><p>除了负梯度计算和叶子节点的最佳负梯度拟合的线性搜索，二元GBDT分类和GBDT回归算法过程相同。</p><h3 id="_2-多元分类" tabindex="-1">2. 多元分类 <a class="header-anchor" href="#_2-多元分类" aria-label="Permalink to &quot;2. 多元分类&quot;">​</a></h3><p>假设类别为K，此时对数似然损失函数为：</p><p><img src="'+m+'" alt="1569746410666"></p><p>其中，第k类的概率为：</p><p><img src="'+d+'" alt="1569746461416"></p><p>结合上面两式，我们可以计算出第t轮的第i个样本对应类别l的负梯度误差为：</p><p>观察上式可以看出，其实这里的误差就是样本i对应类别ll的真实概率和t−1轮预测概率的差值。</p><p>对于生成的决策树，我们各个叶子节点的最佳负梯度拟合值为：</p><p><img src="'+A+'" alt="1569746621976"></p><p>由于上式比较难优化，我们一般使用近似值代替：</p><p><img src="'+g+'" alt="1569746658585"></p><p>除了负梯度计算和叶子节点的最佳负梯度拟合的线性搜索，多元GBDT分类和二元GBDT分类以及GBDT回归算法过程相同。</p><h1 id="gbdt常用损失函数" tabindex="-1">GBDT常用损失函数 <a class="header-anchor" href="#gbdt常用损失函数" aria-label="Permalink to &quot;GBDT常用损失函数&quot;">​</a></h1><p>分类：</p><ol><li>指数损失函数，见Adaboost</li><li>对数损失函数，分为二元分类，和多元分类</li></ol><p>回归：</p><ol><li>均方差</li><li>绝对损失</li><li>Huber损失，它是均方差和绝对损失的折衷产物，对于远离中心的异常点，采用绝对损失，而中心附近的点采用均方差。这个界限一般用分位数点度量。</li></ol><p><img src="'+u+'" alt="1570689728416"></p><ol start="4"><li>分位数损失，它对应的是分位数回归的损失函数，它的表达式为：</li></ol><p><img src="'+D+'" alt="1570689774048"></p><p>对于Huber损失和分位数损失，主要用于健壮回归，也就是减少异常点对损失函数的影响。</p><h1 id="gbdt正则化" tabindex="-1">GBDT正则化 <a class="header-anchor" href="#gbdt正则化" aria-label="Permalink to &quot;GBDT正则化&quot;">​</a></h1><p>三种。</p><p>第一种，和AdaBoost一样，加一个步长v。<img src="'+h+`" alt="1569746932293"></p><p>第二种，无放回采样，增加样本拟合偏差，减少方差，防止过拟合。</p><p>第三种，对CART回归树进行剪枝。</p><h1 id="优缺点" tabindex="-1">优缺点 <a class="header-anchor" href="#优缺点" aria-label="Permalink to &quot;优缺点&quot;">​</a></h1><p>优点：</p><ol><li>可以灵活处理各种类型的数据，包括连续值和离散值。</li><li>在相对少的调参时间情况下，预测的准确率也可以比较高。</li><li>使用一些健壮的损失函数，对异常值的鲁棒性非常强。比如 Huber损失函数和Quantile损失函数。</li></ol><p>缺点：</p><ol><li>难以并行训练数据。</li></ol><h1 id="代码" tabindex="-1">代码 <a class="header-anchor" href="#代码" aria-label="Permalink to &quot;代码&quot;">​</a></h1><p>库：sklearn.ensemble</p><p>方法：GradientBoostingClassifier and GradientBoostingRegressor</p><p>框架参数：</p><ol><li>n_estimators：默认100</li><li>learning_rate：每个弱学习器的权重衰减系数v，默认1</li><li>subsample：子采样比例(0,1]，默认1，不使用子采样。这里子采样是不放回抽样</li><li>loss：损失函数。 <ul><li>分类默认对数似然&quot;deviance&quot;，还有指数损失&quot;exponential&quot;,Adaboost算法；</li><li>回归默认均方差&quot;ls&quot;，还有绝对损失&quot;lad&quot;，Huber损失&quot;huber&quot;，分位数损失&quot;quantile&quot;。如果噪音点较多，用&quot;huber&quot;。而如果我们需要对训练集进行分段预测的时候，则采用“quantile”。</li></ul></li><li>alpha：只有回归模型有，当使用&quot;huber&quot;和&quot;quantile&quot;损失函数时，需要指定分位数的值，默认0.9，如果噪音点较多，可以适当降低这个分位数的值。</li></ol><p>学习器参数：</p><ol><li>max_feautres：默认&quot;None&quot;考虑所有，还有&quot;sqrt&quot;和&quot;auto&quot;一样，&quot;log&quot;，如果是浮点数，代表百分比。</li><li>max_depth：默认3</li><li>min_samples_split：节点划分最小样本数，默认2</li><li>min_samples_leaf：叶子节点最少样本数，默认1</li><li>max_leaf_nodes：最大叶子节点数，默认不限制</li><li>min_weight_fraction_leaf：叶子结点最小的样本权重和，默认不考虑。如果分类样本分布有偏差，再注意。</li><li>min_impurity_split：划分节点最小不纯度，默认就好1e-7。</li></ol><p>代码，分类</p><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki github-dark vp-code-dark"><code><span class="line"><span style="color:#F97583;">import</span><span style="color:#E1E4E8;"> pandas </span><span style="color:#F97583;">as</span><span style="color:#E1E4E8;"> pd</span></span>
<span class="line"><span style="color:#F97583;">import</span><span style="color:#E1E4E8;"> numpy </span><span style="color:#F97583;">as</span><span style="color:#E1E4E8;"> np</span></span>
<span class="line"><span style="color:#F97583;">from</span><span style="color:#E1E4E8;"> sklearn.ensemble </span><span style="color:#F97583;">import</span><span style="color:#E1E4E8;"> GradientBoostingClassifier</span></span>
<span class="line"><span style="color:#F97583;">from</span><span style="color:#E1E4E8;"> sklearn </span><span style="color:#F97583;">import</span><span style="color:#E1E4E8;"> metrics</span></span>
<span class="line"><span style="color:#F97583;">from</span><span style="color:#E1E4E8;"> sklearn.model_selection </span><span style="color:#F97583;">import</span><span style="color:#E1E4E8;"> GridSearchCV</span></span>
<span class="line"><span style="color:#F97583;">from</span><span style="color:#E1E4E8;"> sklearn.model_selection </span><span style="color:#F97583;">import</span><span style="color:#E1E4E8;"> cross_val_predict</span></span>
<span class="line"><span style="color:#F97583;">import</span><span style="color:#E1E4E8;"> matplotlib.pyplot </span><span style="color:#F97583;">as</span><span style="color:#E1E4E8;"> plt</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;"># data</span></span>
<span class="line"><span style="color:#E1E4E8;">train </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> pd.read_csv(</span><span style="color:#9ECBFF;">&#39;train_modified.csv&#39;</span><span style="color:#E1E4E8;">)</span></span>
<span class="line"><span style="color:#E1E4E8;">target </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> </span><span style="color:#9ECBFF;">&#39;Disbursed&#39;</span></span>
<span class="line"><span style="color:#E1E4E8;">IDcol </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> </span><span style="color:#9ECBFF;">&#39;ID&#39;</span></span>
<span class="line"><span style="color:#E1E4E8;">x_columns </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> [x </span><span style="color:#F97583;">for</span><span style="color:#E1E4E8;"> x </span><span style="color:#F97583;">in</span><span style="color:#E1E4E8;"> train.columns </span><span style="color:#F97583;">if</span><span style="color:#E1E4E8;"> x </span><span style="color:#F97583;">not</span><span style="color:#E1E4E8;"> </span><span style="color:#F97583;">in</span><span style="color:#E1E4E8;"> [target, IDcol]]</span></span>
<span class="line"><span style="color:#E1E4E8;">X </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> train[x_columns]</span></span>
<span class="line"><span style="color:#E1E4E8;">y </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> train[target]</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;"># 网格</span></span>
<span class="line"><span style="color:#6A737D;"># grid = GridSearchCV(estimator=GradientBoostingClassifier(n_estimators=60,</span></span>
<span class="line"><span style="color:#6A737D;">#                                                          learning_rate=0.1,</span></span>
<span class="line"><span style="color:#6A737D;">#                                                          max_depth=7,</span></span>
<span class="line"><span style="color:#6A737D;">#                                                          min_samples_split=1200,</span></span>
<span class="line"><span style="color:#6A737D;">#                                                          min_samples_leaf=60,</span></span>
<span class="line"><span style="color:#6A737D;">#                                                         max_features=9,</span></span>
<span class="line"><span style="color:#6A737D;">#                                                         subsample=0.7,</span></span>
<span class="line"><span style="color:#6A737D;">#                                                         random_state=10),</span></span>
<span class="line"><span style="color:#6A737D;">#                    param_grid={</span></span>
<span class="line"><span style="color:#6A737D;"># #                        &#39;n_estimators&#39;: range(20, 81, 10),</span></span>
<span class="line"><span style="color:#E1E4E8;">                       </span></span>
<span class="line"><span style="color:#6A737D;"># #                        &#39;max_depth&#39;: range(3, 14, 2),</span></span>
<span class="line"><span style="color:#6A737D;"># #                        &#39;min_samples_split&#39;: range(100, 801, 200),</span></span>
<span class="line"><span style="color:#E1E4E8;">                       </span></span>
<span class="line"><span style="color:#6A737D;"># #                        &#39;min_samples_split&#39;: range(800, 1900, 200),</span></span>
<span class="line"><span style="color:#6A737D;"># #                        &#39;min_samples_leaf&#39;: range(60, 101, 10),</span></span>
<span class="line"><span style="color:#E1E4E8;">                       </span></span>
<span class="line"><span style="color:#6A737D;"># #                        &#39;max_features&#39;: range(7, 20, 2),</span></span>
<span class="line"><span style="color:#E1E4E8;">                       </span></span>
<span class="line"><span style="color:#6A737D;">#                          &#39;subsample&#39;: [0.6,0.7,0.75,0.8,0.85,0.9],</span></span>
<span class="line"><span style="color:#6A737D;">#                    },</span></span>
<span class="line"><span style="color:#6A737D;">#                    scoring=&#39;roc_auc&#39;,</span></span>
<span class="line"><span style="color:#6A737D;">#                    cv=5)</span></span>
<span class="line"><span style="color:#6A737D;"># grid.fit(X, y)</span></span>
<span class="line"><span style="color:#6A737D;"># print(grid.best_params_, grid.best_score_)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;"># model</span></span>
<span class="line"><span style="color:#6A737D;"># 缩小步长10倍，增大迭代次数10倍，增加模型泛化能力</span></span>
<span class="line"><span style="color:#E1E4E8;">clf </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> GradientBoostingClassifier(</span><span style="color:#FFAB70;">learning_rate</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">0.05</span><span style="color:#E1E4E8;">,</span></span>
<span class="line"><span style="color:#E1E4E8;">                                </span><span style="color:#FFAB70;">n_estimators</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">600</span><span style="color:#E1E4E8;">,</span></span>
<span class="line"><span style="color:#E1E4E8;">                                </span><span style="color:#FFAB70;">max_depth</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">7</span><span style="color:#E1E4E8;">,</span></span>
<span class="line"><span style="color:#E1E4E8;">                                </span><span style="color:#FFAB70;">min_samples_leaf</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">60</span><span style="color:#E1E4E8;">,</span></span>
<span class="line"><span style="color:#E1E4E8;">                                </span><span style="color:#FFAB70;">min_samples_split</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">1200</span><span style="color:#E1E4E8;">,</span></span>
<span class="line"><span style="color:#E1E4E8;">                                </span><span style="color:#FFAB70;">max_features</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">9</span><span style="color:#E1E4E8;">,</span></span>
<span class="line"><span style="color:#E1E4E8;">                                </span><span style="color:#FFAB70;">subsample</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">0.7</span><span style="color:#E1E4E8;">,</span></span>
<span class="line"><span style="color:#E1E4E8;">                                </span><span style="color:#FFAB70;">random_state</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">10</span><span style="color:#E1E4E8;">)</span></span>
<span class="line"><span style="color:#E1E4E8;">clf.fit(X, y)</span></span>
<span class="line"><span style="color:#E1E4E8;">y_pred </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> clf.predict(X)</span></span>
<span class="line"><span style="color:#E1E4E8;">y_predprob </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> clf.predict_proba(X)[:, </span><span style="color:#79B8FF;">1</span><span style="color:#E1E4E8;">]</span></span>
<span class="line"><span style="color:#79B8FF;">print</span><span style="color:#E1E4E8;">(metrics.accuracy_score(y.values, y_pred))</span></span>
<span class="line"><span style="color:#79B8FF;">print</span><span style="color:#E1E4E8;">(metrics.roc_auc_score(y, y_predprob))</span></span></code></pre><pre class="shiki github-light vp-code-light"><code><span class="line"><span style="color:#D73A49;">import</span><span style="color:#24292E;"> pandas </span><span style="color:#D73A49;">as</span><span style="color:#24292E;"> pd</span></span>
<span class="line"><span style="color:#D73A49;">import</span><span style="color:#24292E;"> numpy </span><span style="color:#D73A49;">as</span><span style="color:#24292E;"> np</span></span>
<span class="line"><span style="color:#D73A49;">from</span><span style="color:#24292E;"> sklearn.ensemble </span><span style="color:#D73A49;">import</span><span style="color:#24292E;"> GradientBoostingClassifier</span></span>
<span class="line"><span style="color:#D73A49;">from</span><span style="color:#24292E;"> sklearn </span><span style="color:#D73A49;">import</span><span style="color:#24292E;"> metrics</span></span>
<span class="line"><span style="color:#D73A49;">from</span><span style="color:#24292E;"> sklearn.model_selection </span><span style="color:#D73A49;">import</span><span style="color:#24292E;"> GridSearchCV</span></span>
<span class="line"><span style="color:#D73A49;">from</span><span style="color:#24292E;"> sklearn.model_selection </span><span style="color:#D73A49;">import</span><span style="color:#24292E;"> cross_val_predict</span></span>
<span class="line"><span style="color:#D73A49;">import</span><span style="color:#24292E;"> matplotlib.pyplot </span><span style="color:#D73A49;">as</span><span style="color:#24292E;"> plt</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;"># data</span></span>
<span class="line"><span style="color:#24292E;">train </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> pd.read_csv(</span><span style="color:#032F62;">&#39;train_modified.csv&#39;</span><span style="color:#24292E;">)</span></span>
<span class="line"><span style="color:#24292E;">target </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> </span><span style="color:#032F62;">&#39;Disbursed&#39;</span></span>
<span class="line"><span style="color:#24292E;">IDcol </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> </span><span style="color:#032F62;">&#39;ID&#39;</span></span>
<span class="line"><span style="color:#24292E;">x_columns </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> [x </span><span style="color:#D73A49;">for</span><span style="color:#24292E;"> x </span><span style="color:#D73A49;">in</span><span style="color:#24292E;"> train.columns </span><span style="color:#D73A49;">if</span><span style="color:#24292E;"> x </span><span style="color:#D73A49;">not</span><span style="color:#24292E;"> </span><span style="color:#D73A49;">in</span><span style="color:#24292E;"> [target, IDcol]]</span></span>
<span class="line"><span style="color:#24292E;">X </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> train[x_columns]</span></span>
<span class="line"><span style="color:#24292E;">y </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> train[target]</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;"># 网格</span></span>
<span class="line"><span style="color:#6A737D;"># grid = GridSearchCV(estimator=GradientBoostingClassifier(n_estimators=60,</span></span>
<span class="line"><span style="color:#6A737D;">#                                                          learning_rate=0.1,</span></span>
<span class="line"><span style="color:#6A737D;">#                                                          max_depth=7,</span></span>
<span class="line"><span style="color:#6A737D;">#                                                          min_samples_split=1200,</span></span>
<span class="line"><span style="color:#6A737D;">#                                                          min_samples_leaf=60,</span></span>
<span class="line"><span style="color:#6A737D;">#                                                         max_features=9,</span></span>
<span class="line"><span style="color:#6A737D;">#                                                         subsample=0.7,</span></span>
<span class="line"><span style="color:#6A737D;">#                                                         random_state=10),</span></span>
<span class="line"><span style="color:#6A737D;">#                    param_grid={</span></span>
<span class="line"><span style="color:#6A737D;"># #                        &#39;n_estimators&#39;: range(20, 81, 10),</span></span>
<span class="line"><span style="color:#24292E;">                       </span></span>
<span class="line"><span style="color:#6A737D;"># #                        &#39;max_depth&#39;: range(3, 14, 2),</span></span>
<span class="line"><span style="color:#6A737D;"># #                        &#39;min_samples_split&#39;: range(100, 801, 200),</span></span>
<span class="line"><span style="color:#24292E;">                       </span></span>
<span class="line"><span style="color:#6A737D;"># #                        &#39;min_samples_split&#39;: range(800, 1900, 200),</span></span>
<span class="line"><span style="color:#6A737D;"># #                        &#39;min_samples_leaf&#39;: range(60, 101, 10),</span></span>
<span class="line"><span style="color:#24292E;">                       </span></span>
<span class="line"><span style="color:#6A737D;"># #                        &#39;max_features&#39;: range(7, 20, 2),</span></span>
<span class="line"><span style="color:#24292E;">                       </span></span>
<span class="line"><span style="color:#6A737D;">#                          &#39;subsample&#39;: [0.6,0.7,0.75,0.8,0.85,0.9],</span></span>
<span class="line"><span style="color:#6A737D;">#                    },</span></span>
<span class="line"><span style="color:#6A737D;">#                    scoring=&#39;roc_auc&#39;,</span></span>
<span class="line"><span style="color:#6A737D;">#                    cv=5)</span></span>
<span class="line"><span style="color:#6A737D;"># grid.fit(X, y)</span></span>
<span class="line"><span style="color:#6A737D;"># print(grid.best_params_, grid.best_score_)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;"># model</span></span>
<span class="line"><span style="color:#6A737D;"># 缩小步长10倍，增大迭代次数10倍，增加模型泛化能力</span></span>
<span class="line"><span style="color:#24292E;">clf </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> GradientBoostingClassifier(</span><span style="color:#E36209;">learning_rate</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">0.05</span><span style="color:#24292E;">,</span></span>
<span class="line"><span style="color:#24292E;">                                </span><span style="color:#E36209;">n_estimators</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">600</span><span style="color:#24292E;">,</span></span>
<span class="line"><span style="color:#24292E;">                                </span><span style="color:#E36209;">max_depth</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">7</span><span style="color:#24292E;">,</span></span>
<span class="line"><span style="color:#24292E;">                                </span><span style="color:#E36209;">min_samples_leaf</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">60</span><span style="color:#24292E;">,</span></span>
<span class="line"><span style="color:#24292E;">                                </span><span style="color:#E36209;">min_samples_split</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">1200</span><span style="color:#24292E;">,</span></span>
<span class="line"><span style="color:#24292E;">                                </span><span style="color:#E36209;">max_features</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">9</span><span style="color:#24292E;">,</span></span>
<span class="line"><span style="color:#24292E;">                                </span><span style="color:#E36209;">subsample</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">0.7</span><span style="color:#24292E;">,</span></span>
<span class="line"><span style="color:#24292E;">                                </span><span style="color:#E36209;">random_state</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">10</span><span style="color:#24292E;">)</span></span>
<span class="line"><span style="color:#24292E;">clf.fit(X, y)</span></span>
<span class="line"><span style="color:#24292E;">y_pred </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> clf.predict(X)</span></span>
<span class="line"><span style="color:#24292E;">y_predprob </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> clf.predict_proba(X)[:, </span><span style="color:#005CC5;">1</span><span style="color:#24292E;">]</span></span>
<span class="line"><span style="color:#005CC5;">print</span><span style="color:#24292E;">(metrics.accuracy_score(y.values, y_pred))</span></span>
<span class="line"><span style="color:#005CC5;">print</span><span style="color:#24292E;">(metrics.roc_auc_score(y, y_predprob))</span></span></code></pre></div>`,70),b=[f];function C(R,B,G,T,x,z){return a(),n("div",null,b)}const O=s(F,[["render",C]]);export{I as __pageData,O as default};
