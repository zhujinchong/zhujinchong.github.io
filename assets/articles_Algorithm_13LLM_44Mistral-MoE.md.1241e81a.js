import{_ as t,o as a,c as r,Q as o}from"./chunks/framework.2516552c.js";const e="/assets/image-20231212113350193.812b510b.png",s="/assets/image-20231212114056759.b6090935.png",i="/assets/image-20231212114618463.27586283.png",b=JSON.parse('{"title":"","description":"","frontmatter":{},"headers":[],"relativePath":"articles/Algorithm/13LLM/44Mistral-MoE.md","filePath":"articles/Algorithm/13LLM/44Mistral-MoE.md","lastUpdated":null}'),l={name:"articles/Algorithm/13LLM/44Mistral-MoE.md"},n=o('<p>时间：20231208</p><p>混合专家模型（Mixture of Experts，MoE）</p><h2 id="mistral-7b×8-moe的特点" tabindex="-1">Mistral-7B×8-MoE的特点 <a class="header-anchor" href="#mistral-7b×8-moe的特点" aria-label="Permalink to &quot;Mistral-7B×8-MoE的特点&quot;">​</a></h2><p>根据官方的介绍，Mistral-7B×8-MoE是一个高质量稀疏型的专家混合模型。是8个70亿参数规模大模型的混合。它的主要特点如下：</p><ul><li>它可以非常优雅地处理<strong>32K上下文</strong>数据</li><li>除了英语外，在<strong>法语、德语、意大利语和西班牙</strong>语表现也很好</li><li>在代码能力上表现很强</li><li>指令微调后MT-Bench的得分8.3分（GPT-3.5是8.32、LLaMA2 70B是6.86）</li></ul><p>在MoE模型中，有两个关键组件：</p><ul><li>专家（Experts）：这些是网络中的小型子网络，每个专家通常专注于处理一种特定类型的数据或任务。专家的设计可以是多种形式，如完全连接的网络、卷积网络等。</li><li>门控机制（Gating Mechanism）：这是一个智能路由系统，负责决定哪些专家应该被激活来处理当前的输入数据。门控机制基于输入数据的特性，动态地将数据分配给不同的专家。</li></ul><p>官方介绍，这个模型是基于网络数据预训练的，其中，专<strong>家网络和门控路由是同时训练的</strong>。</p><h2 id="mistral-7b×8-moe评估效果" tabindex="-1">Mistral-7B×8-MoE评估效果 <a class="header-anchor" href="#mistral-7b×8-moe评估效果" aria-label="Permalink to &quot;Mistral-7B×8-MoE评估效果&quot;">​</a></h2><p>此次，官方详细公布了Mistral-7B×8-MoE在各个评测数据集上的评测效果。结果如下：</p><p><img src="'+e+'" alt="image-20231212113350193"></p><p><img src="'+s+'" alt="image-20231212114056759"></p><p>Mistral-7B×8-MoE模型在各方面的指标都很不错，几乎与LLaMA2-70B在一个水平，但是由于每次只有120亿参数在工作，这意味着它的成本要远低于LLaMA2 70B，官方的说法是推理速度<strong>比LLaMA2 70B快6倍</strong>！</p><p>根据官方的介绍，Mistral-7B×8-MoE<strong>实际的参数为450亿</strong>，但是<strong>每次运行只会利用其中120亿参数</strong>（单个模型在56.5亿，但是可能有共享参数，<strong>每个token会被2个模型处理</strong>）。因此，这个模型的推理速度和成本与120亿参数规模的大模型是一样的。</p><h2 id="模型架构" tabindex="-1">模型架构 <a class="header-anchor" href="#模型架构" aria-label="Permalink to &quot;模型架构&quot;">​</a></h2><p><a href="https://github.com/open-compass/MixtralKit/blob/main/README_zh-CN.md#-%E6%A8%A1%E5%9E%8B%E6%9E%B6%E6%9E%84" target="_blank" rel="noreferrer">https://github.com/open-compass/MixtralKit/blob/main/README_zh-CN.md#-模型架构</a></p><blockquote><p>Mixtral-8x7B-32K MoE模型主要由32个相同的MoE transformer block组成。MoE transformer block与普通的transformer block的最大差别在于其FFN层替换为了<strong>MoE FFN</strong>层。在MoE FFN层，tensor首先会经过一个gate layer计算每个expert的得分，并根据expert得分从8个expert中挑出top-k个expert，将tensor经过这top-k个expert的输出后聚合起来，从而得到MoE FFN层的最终输出，其中的每个expert由3个Linear层组成。值得注意的是，mixtral MoE的所有Norm Layer也采用了和LLama一样的RMSNorm，而在attention layer中，mixtral MoE的QKV矩阵中的Q矩阵shape为(4096,4096)，K和V矩阵shape则为(4096,1024)。</p></blockquote><p><img src="'+i+'" alt="image-20231212114618463"></p>',18),p=[n];function m(M,c,g,E,_,h){return a(),r("div",null,p)}const u=t(l,[["render",m]]);export{b as __pageData,u as default};
