import{_ as a,o as e,c as t,Q as l}from"./chunks/framework.2516552c.js";const B=JSON.parse('{"title":"","description":"","frontmatter":{},"headers":[],"relativePath":"articles/Algorithm/13LLM/07RoBERTa.md","filePath":"articles/Algorithm/13LLM/07RoBERTa.md","lastUpdated":null}'),r={name:"articles/Algorithm/13LLM/07RoBERTa.md"},o=l('<p>2019/7 RoBERTa</p><p>RoBERTa: A Robustly Optimized BERT Pretraining Approach</p><p><a href="https://arxiv.org/abs/1907.11692" target="_blank" rel="noreferrer">https://arxiv.org/abs/1907.11692</a></p><p>第三章：</p><ol><li>Adam的参数由BERT的0.99调为0.98；</li><li>BERT使用16G数据；本文使用160G;</li><li>BERT会使用短文本训练；本文使用全长文本训练。</li></ol><p>第四章：</p><ol><li>BERT每个掩码训练四次，称为静态掩码；本文每个训练样本动态掩码;</li><li>不适用NSP loss，使用全长句子512tokens，两个文档之间的首尾句子加separator token。</li><li>过去研究表明提高学习率和batch_size会变好，BERT也同样，原来256batch_size，提高到2K</li><li>larger byte-level BPE编码</li></ol><p>第五章：</p><ul><li>RoBERTa_larger使用160GB_data+500K_steps最终登顶（模仿的BERT_larger）。在后文中，三个benchmarks评估，都是用这个larger模型。</li></ul>',9),i=[o];function s(_,p,c,n,R,T){return e(),t("div",null,i)}const E=a(r,[["render",s]]);export{B as __pageData,E as default};
