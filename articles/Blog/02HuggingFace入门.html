<!DOCTYPE html>
<html lang="en-US" dir="ltr">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>入门 | Make each day count, Make learning a habit.</title>
    <meta name="description" content="一只程序猿">
    <link rel="preload stylesheet" href="/assets/style.f33cd555.css" as="style">
    
    <script type="module" src="/assets/app.dc5e2f50.js"></script>
    <link rel="preload" href="/assets/inter-roman-latin.2ed14f66.woff2" as="font" type="font/woff2" crossorigin="">
    <link rel="modulepreload" href="/assets/chunks/framework.2516552c.js">
    <link rel="modulepreload" href="/assets/chunks/theme.2359dc4a.js">
    <link rel="modulepreload" href="/assets/articles_Blog_02HuggingFace入门.md.f84c50bc.lean.js">
    <link rel="icon" href="/img/home.svg">
    <script id="check-dark-mode">(()=>{const e=localStorage.getItem("vitepress-theme-appearance")||"auto",a=window.matchMedia("(prefers-color-scheme: dark)").matches;(!e||e==="auto"?a:e==="dark")&&document.documentElement.classList.add("dark")})();</script>
    <script id="check-mac-os">document.documentElement.classList.toggle("mac",/Mac|iPhone|iPod|iPad/i.test(navigator.platform));</script>
  </head>
  <body>
    <div id="app"><div class="Layout" data-v-5a346dfe><!--[--><!--]--><!--[--><span tabindex="-1" data-v-0f60ec36></span><a href="#VPContent" class="VPSkipLink visually-hidden" data-v-0f60ec36> Skip to content </a><!--]--><!----><header class="VPNav" data-v-5a346dfe data-v-ae24b3ad><div class="VPNavBar has-sidebar" data-v-ae24b3ad data-v-a0fd61f4><div class="container" data-v-a0fd61f4><div class="title" data-v-a0fd61f4><div class="VPNavBarTitle has-sidebar" data-v-a0fd61f4 data-v-86d1bed8><a class="title" href="/" data-v-86d1bed8><!--[--><!--]--><!----><!--[-->Home<!--]--><!--[--><!--]--></a></div></div><div class="content" data-v-a0fd61f4><div class="curtain" data-v-a0fd61f4></div><div class="content-body" data-v-a0fd61f4><!--[--><!--]--><div class="VPNavBarSearch search" data-v-a0fd61f4><!----></div><nav aria-labelledby="main-nav-aria-label" class="VPNavBarMenu menu" data-v-a0fd61f4 data-v-7f418b0f><span id="main-nav-aria-label" class="visually-hidden" data-v-7f418b0f>Main Navigation</span><!--[--><!--[--><div class="VPFlyout VPNavBarMenuGroup" data-v-7f418b0f data-v-9c007e85><button type="button" class="button" aria-haspopup="true" aria-expanded="false" data-v-9c007e85><span class="text" data-v-9c007e85><!----><span data-v-9c007e85>Algorithm</span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="text-icon" data-v-9c007e85><path d="M12,16c-0.3,0-0.5-0.1-0.7-0.3l-6-6c-0.4-0.4-0.4-1,0-1.4s1-0.4,1.4,0l5.3,5.3l5.3-5.3c0.4-0.4,1-0.4,1.4,0s0.4,1,0,1.4l-6,6C12.5,15.9,12.3,16,12,16z"></path></svg></span></button><div class="menu" data-v-9c007e85><div class="VPMenu" data-v-9c007e85 data-v-e7ea1737><div class="items" data-v-e7ea1737><!--[--><!--[--><div class="VPMenuLink" data-v-e7ea1737 data-v-43f1e123><a class="VPLink link" href="/articles/Algorithm/00数学基础/" data-v-43f1e123><!--[-->00数学基础<!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-e7ea1737 data-v-43f1e123><a class="VPLink link" href="/articles/Algorithm/01python语法&amp;工具/" data-v-43f1e123><!--[-->01python语法&amp;工具<!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-e7ea1737 data-v-43f1e123><a class="VPLink link" href="/articles/Algorithm/10白板推导系列/" data-v-43f1e123><!--[-->10白板推导系列<!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-e7ea1737 data-v-43f1e123><a class="VPLink link" href="/articles/Algorithm/11强化学习/" data-v-43f1e123><!--[-->11强化学习<!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-e7ea1737 data-v-43f1e123><a class="VPLink link" href="/articles/Algorithm/12机器学习笔记/" data-v-43f1e123><!--[-->12机器学习笔记<!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-e7ea1737 data-v-43f1e123><a class="VPLink link" href="/articles/Algorithm/13LLM/" data-v-43f1e123><!--[-->13LLM<!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-e7ea1737 data-v-43f1e123><a class="VPLink link" href="/articles/Algorithm/14LLMVideo/" data-v-43f1e123><!--[-->14LLMVideo<!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-e7ea1737 data-v-43f1e123><a class="VPLink link" href="/articles/Algorithm/14LLMVision/" data-v-43f1e123><!--[-->14LLMVision<!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-e7ea1737 data-v-43f1e123><a class="VPLink link" href="/articles/Algorithm/15Emodied/" data-v-43f1e123><!--[-->15Emodied<!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-e7ea1737 data-v-43f1e123><a class="VPLink link" href="/articles/Algorithm/21模型部署/" data-v-43f1e123><!--[-->21模型部署<!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-e7ea1737 data-v-43f1e123><a class="VPLink link" href="/articles/Algorithm/22模型训练和微调/" data-v-43f1e123><!--[-->22模型训练和微调<!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-e7ea1737 data-v-43f1e123><a class="VPLink link" href="/articles/Algorithm/23LangChain/" data-v-43f1e123><!--[-->23LangChain<!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-e7ea1737 data-v-43f1e123><a class="VPLink link" href="/articles/Algorithm/24Agent/" data-v-43f1e123><!--[-->24Agent<!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-e7ea1737 data-v-43f1e123><a class="VPLink link" href="/articles/Algorithm/99deeplab/" data-v-43f1e123><!--[-->99deeplab<!--]--></a></div><!--]--><!--]--></div><!--[--><!--]--></div></div></div><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/articles/Blog/" tabindex="0" data-v-7f418b0f data-v-42ef59de><!--[--><span data-v-42ef59de>Blog</span><!--]--></a><!--]--><!--[--><div class="VPFlyout VPNavBarMenuGroup" data-v-7f418b0f data-v-9c007e85><button type="button" class="button" aria-haspopup="true" aria-expanded="false" data-v-9c007e85><span class="text" data-v-9c007e85><!----><span data-v-9c007e85>Java</span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="text-icon" data-v-9c007e85><path d="M12,16c-0.3,0-0.5-0.1-0.7-0.3l-6-6c-0.4-0.4-0.4-1,0-1.4s1-0.4,1.4,0l5.3,5.3l5.3-5.3c0.4-0.4,1-0.4,1.4,0s0.4,1,0,1.4l-6,6C12.5,15.9,12.3,16,12,16z"></path></svg></span></button><div class="menu" data-v-9c007e85><div class="VPMenu" data-v-9c007e85 data-v-e7ea1737><div class="items" data-v-e7ea1737><!--[--><!--[--><div class="VPMenuLink" data-v-e7ea1737 data-v-43f1e123><a class="VPLink link" href="/articles/Java/01Java语法/" data-v-43f1e123><!--[-->01Java语法<!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-e7ea1737 data-v-43f1e123><a class="VPLink link" href="/articles/Java/02HTML+CSS+JS+XML/" data-v-43f1e123><!--[-->02HTML+CSS+JS+XML<!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-e7ea1737 data-v-43f1e123><a class="VPLink link" href="/articles/Java/03JavaWeb基础tomcat servlet jsp/" data-v-43f1e123><!--[-->03JavaWeb基础tomcat servlet jsp<!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-e7ea1737 data-v-43f1e123><a class="VPLink link" href="/articles/Java/05Maven/" data-v-43f1e123><!--[-->05Maven<!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-e7ea1737 data-v-43f1e123><a class="VPLink link" href="/articles/Java/20mysql/" data-v-43f1e123><!--[-->20mysql<!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-e7ea1737 data-v-43f1e123><a class="VPLink link" href="/articles/Java/22JanusGraph/" data-v-43f1e123><!--[-->22JanusGraph<!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-e7ea1737 data-v-43f1e123><a class="VPLink link" href="/articles/Java/30设计模式/" data-v-43f1e123><!--[-->30设计模式<!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-e7ea1737 data-v-43f1e123><a class="VPLink link" href="/articles/Java/31Java并发编程/" data-v-43f1e123><!--[-->31Java并发编程<!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-e7ea1737 data-v-43f1e123><a class="VPLink link" href="/articles/Java/51SpringBoot/" data-v-43f1e123><!--[-->51SpringBoot<!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-e7ea1737 data-v-43f1e123><a class="VPLink link" href="/articles/Java/52SpringCloud/" data-v-43f1e123><!--[-->52SpringCloud<!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-e7ea1737 data-v-43f1e123><a class="VPLink link" href="/articles/Java/53Arthus/" data-v-43f1e123><!--[-->53Arthus<!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-e7ea1737 data-v-43f1e123><a class="VPLink link" href="/articles/Java/60kafka/" data-v-43f1e123><!--[-->60kafka<!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-e7ea1737 data-v-43f1e123><a class="VPLink link" href="/articles/Java/61ElasticSearch/" data-v-43f1e123><!--[-->61ElasticSearch<!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-e7ea1737 data-v-43f1e123><a class="VPLink link" href="/articles/Java/65redis/" data-v-43f1e123><!--[-->65redis<!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-e7ea1737 data-v-43f1e123><a class="VPLink link" href="/articles/Java/65vue/" data-v-43f1e123><!--[-->65vue<!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-e7ea1737 data-v-43f1e123><a class="VPLink link" href="/articles/Java/66jenkins/" data-v-43f1e123><!--[-->66jenkins<!--]--></a></div><!--]--><!--]--></div><!--[--><!--]--></div></div></div><!--]--><!--[--><div class="VPFlyout VPNavBarMenuGroup" data-v-7f418b0f data-v-9c007e85><button type="button" class="button" aria-haspopup="true" aria-expanded="false" data-v-9c007e85><span class="text" data-v-9c007e85><!----><span data-v-9c007e85>Ops</span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="text-icon" data-v-9c007e85><path d="M12,16c-0.3,0-0.5-0.1-0.7-0.3l-6-6c-0.4-0.4-0.4-1,0-1.4s1-0.4,1.4,0l5.3,5.3l5.3-5.3c0.4-0.4,1-0.4,1.4,0s0.4,1,0,1.4l-6,6C12.5,15.9,12.3,16,12,16z"></path></svg></span></button><div class="menu" data-v-9c007e85><div class="VPMenu" data-v-9c007e85 data-v-e7ea1737><div class="items" data-v-e7ea1737><!--[--><!--[--><div class="VPMenuLink" data-v-e7ea1737 data-v-43f1e123><a class="VPLink link" href="/articles/Ops/Docker/" data-v-43f1e123><!--[-->Docker<!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-e7ea1737 data-v-43f1e123><a class="VPLink link" href="/articles/Ops/Git/" data-v-43f1e123><!--[-->Git<!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-e7ea1737 data-v-43f1e123><a class="VPLink link" href="/articles/Ops/Linux/" data-v-43f1e123><!--[-->Linux<!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-e7ea1737 data-v-43f1e123><a class="VPLink link" href="/articles/Ops/Nginx/" data-v-43f1e123><!--[-->Nginx<!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-e7ea1737 data-v-43f1e123><a class="VPLink link" href="/articles/Ops/k8s/" data-v-43f1e123><!--[-->k8s<!--]--></a></div><!--]--><!--]--></div><!--[--><!--]--></div></div></div><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/articles/Python/" tabindex="0" data-v-7f418b0f data-v-42ef59de><!--[--><span data-v-42ef59de>Python</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/articles/VitePress/" tabindex="0" data-v-7f418b0f data-v-42ef59de><!--[--><span data-v-42ef59de>VitePress</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/articles/windows/" tabindex="0" data-v-7f418b0f data-v-42ef59de><!--[--><span data-v-42ef59de>windows</span><!--]--></a><!--]--><!--]--></nav><!----><div class="VPNavBarAppearance appearance" data-v-a0fd61f4 data-v-e6aabb21><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" title="toggle dark mode" aria-checked="false" data-v-e6aabb21 data-v-ce54a7d1 data-v-b1685198><span class="check" data-v-b1685198><span class="icon" data-v-b1685198><!--[--><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="sun" data-v-ce54a7d1><path d="M12,18c-3.3,0-6-2.7-6-6s2.7-6,6-6s6,2.7,6,6S15.3,18,12,18zM12,8c-2.2,0-4,1.8-4,4c0,2.2,1.8,4,4,4c2.2,0,4-1.8,4-4C16,9.8,14.2,8,12,8z"></path><path d="M12,4c-0.6,0-1-0.4-1-1V1c0-0.6,0.4-1,1-1s1,0.4,1,1v2C13,3.6,12.6,4,12,4z"></path><path d="M12,24c-0.6,0-1-0.4-1-1v-2c0-0.6,0.4-1,1-1s1,0.4,1,1v2C13,23.6,12.6,24,12,24z"></path><path d="M5.6,6.6c-0.3,0-0.5-0.1-0.7-0.3L3.5,4.9c-0.4-0.4-0.4-1,0-1.4s1-0.4,1.4,0l1.4,1.4c0.4,0.4,0.4,1,0,1.4C6.2,6.5,5.9,6.6,5.6,6.6z"></path><path d="M19.8,20.8c-0.3,0-0.5-0.1-0.7-0.3l-1.4-1.4c-0.4-0.4-0.4-1,0-1.4s1-0.4,1.4,0l1.4,1.4c0.4,0.4,0.4,1,0,1.4C20.3,20.7,20,20.8,19.8,20.8z"></path><path d="M3,13H1c-0.6,0-1-0.4-1-1s0.4-1,1-1h2c0.6,0,1,0.4,1,1S3.6,13,3,13z"></path><path d="M23,13h-2c-0.6,0-1-0.4-1-1s0.4-1,1-1h2c0.6,0,1,0.4,1,1S23.6,13,23,13z"></path><path d="M4.2,20.8c-0.3,0-0.5-0.1-0.7-0.3c-0.4-0.4-0.4-1,0-1.4l1.4-1.4c0.4-0.4,1-0.4,1.4,0s0.4,1,0,1.4l-1.4,1.4C4.7,20.7,4.5,20.8,4.2,20.8z"></path><path d="M18.4,6.6c-0.3,0-0.5-0.1-0.7-0.3c-0.4-0.4-0.4-1,0-1.4l1.4-1.4c0.4-0.4,1-0.4,1.4,0s0.4,1,0,1.4l-1.4,1.4C18.9,6.5,18.6,6.6,18.4,6.6z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="moon" data-v-ce54a7d1><path d="M12.1,22c-0.3,0-0.6,0-0.9,0c-5.5-0.5-9.5-5.4-9-10.9c0.4-4.8,4.2-8.6,9-9c0.4,0,0.8,0.2,1,0.5c0.2,0.3,0.2,0.8-0.1,1.1c-2,2.7-1.4,6.4,1.3,8.4c2.1,1.6,5,1.6,7.1,0c0.3-0.2,0.7-0.3,1.1-0.1c0.3,0.2,0.5,0.6,0.5,1c-0.2,2.7-1.5,5.1-3.6,6.8C16.6,21.2,14.4,22,12.1,22zM9.3,4.4c-2.9,1-5,3.6-5.2,6.8c-0.4,4.4,2.8,8.3,7.2,8.7c2.1,0.2,4.2-0.4,5.8-1.8c1.1-0.9,1.9-2.1,2.4-3.4c-2.5,0.9-5.3,0.5-7.5-1.1C9.2,11.4,8.1,7.7,9.3,4.4z"></path></svg><!--]--></span></span></button></div><!----><div class="VPFlyout VPNavBarExtra extra" data-v-a0fd61f4 data-v-40855f84 data-v-9c007e85><button type="button" class="button" aria-haspopup="true" aria-expanded="false" aria-label="extra navigation" data-v-9c007e85><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="icon" data-v-9c007e85><circle cx="12" cy="12" r="2"></circle><circle cx="19" cy="12" r="2"></circle><circle cx="5" cy="12" r="2"></circle></svg></button><div class="menu" data-v-9c007e85><div class="VPMenu" data-v-9c007e85 data-v-e7ea1737><!----><!--[--><!--[--><!----><div class="group" data-v-40855f84><div class="item appearance" data-v-40855f84><p class="label" data-v-40855f84>Appearance</p><div class="appearance-action" data-v-40855f84><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" title="toggle dark mode" aria-checked="false" data-v-40855f84 data-v-ce54a7d1 data-v-b1685198><span class="check" data-v-b1685198><span class="icon" data-v-b1685198><!--[--><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="sun" data-v-ce54a7d1><path d="M12,18c-3.3,0-6-2.7-6-6s2.7-6,6-6s6,2.7,6,6S15.3,18,12,18zM12,8c-2.2,0-4,1.8-4,4c0,2.2,1.8,4,4,4c2.2,0,4-1.8,4-4C16,9.8,14.2,8,12,8z"></path><path d="M12,4c-0.6,0-1-0.4-1-1V1c0-0.6,0.4-1,1-1s1,0.4,1,1v2C13,3.6,12.6,4,12,4z"></path><path d="M12,24c-0.6,0-1-0.4-1-1v-2c0-0.6,0.4-1,1-1s1,0.4,1,1v2C13,23.6,12.6,24,12,24z"></path><path d="M5.6,6.6c-0.3,0-0.5-0.1-0.7-0.3L3.5,4.9c-0.4-0.4-0.4-1,0-1.4s1-0.4,1.4,0l1.4,1.4c0.4,0.4,0.4,1,0,1.4C6.2,6.5,5.9,6.6,5.6,6.6z"></path><path d="M19.8,20.8c-0.3,0-0.5-0.1-0.7-0.3l-1.4-1.4c-0.4-0.4-0.4-1,0-1.4s1-0.4,1.4,0l1.4,1.4c0.4,0.4,0.4,1,0,1.4C20.3,20.7,20,20.8,19.8,20.8z"></path><path d="M3,13H1c-0.6,0-1-0.4-1-1s0.4-1,1-1h2c0.6,0,1,0.4,1,1S3.6,13,3,13z"></path><path d="M23,13h-2c-0.6,0-1-0.4-1-1s0.4-1,1-1h2c0.6,0,1,0.4,1,1S23.6,13,23,13z"></path><path d="M4.2,20.8c-0.3,0-0.5-0.1-0.7-0.3c-0.4-0.4-0.4-1,0-1.4l1.4-1.4c0.4-0.4,1-0.4,1.4,0s0.4,1,0,1.4l-1.4,1.4C4.7,20.7,4.5,20.8,4.2,20.8z"></path><path d="M18.4,6.6c-0.3,0-0.5-0.1-0.7-0.3c-0.4-0.4-0.4-1,0-1.4l1.4-1.4c0.4-0.4,1-0.4,1.4,0s0.4,1,0,1.4l-1.4,1.4C18.9,6.5,18.6,6.6,18.4,6.6z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="moon" data-v-ce54a7d1><path d="M12.1,22c-0.3,0-0.6,0-0.9,0c-5.5-0.5-9.5-5.4-9-10.9c0.4-4.8,4.2-8.6,9-9c0.4,0,0.8,0.2,1,0.5c0.2,0.3,0.2,0.8-0.1,1.1c-2,2.7-1.4,6.4,1.3,8.4c2.1,1.6,5,1.6,7.1,0c0.3-0.2,0.7-0.3,1.1-0.1c0.3,0.2,0.5,0.6,0.5,1c-0.2,2.7-1.5,5.1-3.6,6.8C16.6,21.2,14.4,22,12.1,22zM9.3,4.4c-2.9,1-5,3.6-5.2,6.8c-0.4,4.4,2.8,8.3,7.2,8.7c2.1,0.2,4.2-0.4,5.8-1.8c1.1-0.9,1.9-2.1,2.4-3.4c-2.5,0.9-5.3,0.5-7.5-1.1C9.2,11.4,8.1,7.7,9.3,4.4z"></path></svg><!--]--></span></span></button></div></div></div><!----><!--]--><!--]--></div></div></div><!--[--><!--]--><button type="button" class="VPNavBarHamburger hamburger" aria-label="mobile navigation" aria-expanded="false" aria-controls="VPNavScreen" data-v-a0fd61f4 data-v-e5dd9c1c><span class="container" data-v-e5dd9c1c><span class="top" data-v-e5dd9c1c></span><span class="middle" data-v-e5dd9c1c></span><span class="bottom" data-v-e5dd9c1c></span></span></button></div></div></div></div><!----></header><div class="VPLocalNav reached-top" data-v-5a346dfe data-v-79c8c1df><button class="menu" aria-expanded="false" aria-controls="VPSidebarNav" data-v-79c8c1df><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="menu-icon" data-v-79c8c1df><path d="M17,11H3c-0.6,0-1-0.4-1-1s0.4-1,1-1h14c0.6,0,1,0.4,1,1S17.6,11,17,11z"></path><path d="M21,7H3C2.4,7,2,6.6,2,6s0.4-1,1-1h18c0.6,0,1,0.4,1,1S21.6,7,21,7z"></path><path d="M21,15H3c-0.6,0-1-0.4-1-1s0.4-1,1-1h18c0.6,0,1,0.4,1,1S21.6,15,21,15z"></path><path d="M17,19H3c-0.6,0-1-0.4-1-1s0.4-1,1-1h14c0.6,0,1,0.4,1,1S17.6,19,17,19z"></path></svg><span class="menu-text" data-v-79c8c1df>Menu</span></button><div class="VPLocalNavOutlineDropdown" style="--vp-vh:0px;" data-v-79c8c1df data-v-1c15a60a><button data-v-1c15a60a>Return to top</button><!----></div></div><aside class="VPSidebar" data-v-5a346dfe data-v-b00e2fdd><div class="curtain" data-v-b00e2fdd></div><nav class="nav" id="VPSidebarNav" aria-labelledby="sidebar-aria-label" tabindex="-1" data-v-b00e2fdd><span class="visually-hidden" id="sidebar-aria-label" data-v-b00e2fdd> Sidebar Navigation </span><!--[--><!--]--><!--[--><div class="group" data-v-b00e2fdd><section class="VPSidebarItem level-0 has-active" data-v-b00e2fdd data-v-e31bd47b><div class="item" role="button" tabindex="0" data-v-e31bd47b><div class="indicator" data-v-e31bd47b></div><h2 class="text" data-v-e31bd47b>Blog</h2><!----></div><div class="items" data-v-e31bd47b><!--[--><div class="VPSidebarItem level-1 is-link" data-v-e31bd47b data-v-e31bd47b><div class="item" data-v-e31bd47b><div class="indicator" data-v-e31bd47b></div><a class="VPLink link link" href="/articles/Blog/01%E6%8C%87%E4%BB%A4.html" data-v-e31bd47b><!--[--><p class="text" data-v-e31bd47b>01指令</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-e31bd47b data-v-e31bd47b><div class="item" data-v-e31bd47b><div class="indicator" data-v-e31bd47b></div><a class="VPLink link link" href="/articles/Blog/02HuggingFace%E5%85%A5%E9%97%A8.html" data-v-e31bd47b><!--[--><p class="text" data-v-e31bd47b>02HuggingFace入门</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-e31bd47b data-v-e31bd47b><div class="item" data-v-e31bd47b><div class="indicator" data-v-e31bd47b></div><a class="VPLink link link" href="/articles/Blog/03%E5%81%A5%E8%BA%AB.html" data-v-e31bd47b><!--[--><p class="text" data-v-e31bd47b>03健身</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-e31bd47b data-v-e31bd47b><div class="item" data-v-e31bd47b><div class="indicator" data-v-e31bd47b></div><a class="VPLink link link" href="/articles/Blog/04%E5%88%86%E8%AF%8D%E6%96%B9%E6%B3%95.html" data-v-e31bd47b><!--[--><p class="text" data-v-e31bd47b>04分词方法</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-e31bd47b data-v-e31bd47b><div class="item" data-v-e31bd47b><div class="indicator" data-v-e31bd47b></div><a class="VPLink link link" href="/articles/Blog/05%E6%89%A9%E5%85%85%E8%AF%8D%E8%A1%A8.html" data-v-e31bd47b><!--[--><p class="text" data-v-e31bd47b>05扩充词表</p><!--]--></a><!----></div><!----></div><!--]--></div></section></div><!--]--><!--[--><!--]--></nav></aside><div class="VPContent has-sidebar" id="VPContent" data-v-5a346dfe data-v-669faec9><div class="VPDoc has-sidebar has-aside" data-v-669faec9 data-v-6b87e69f><!--[--><!--]--><div class="container" data-v-6b87e69f><div class="aside" data-v-6b87e69f><div class="aside-curtain" data-v-6b87e69f></div><div class="aside-container" data-v-6b87e69f><div class="aside-content" data-v-6b87e69f><div class="VPDocAside" data-v-6b87e69f data-v-3f215769><!--[--><!--]--><!--[--><!--]--><div class="VPDocAsideOutline" role="navigation" data-v-3f215769 data-v-d330b1bb><div class="content" data-v-d330b1bb><div class="outline-marker" data-v-d330b1bb></div><div class="outline-title" role="heading" aria-level="2" data-v-d330b1bb>On this page</div><nav aria-labelledby="doc-outline-aria-label" data-v-d330b1bb><span class="visually-hidden" id="doc-outline-aria-label" data-v-d330b1bb> Table of Contents for current page </span><ul class="root" data-v-d330b1bb data-v-d0ee3533><!--[--><!--]--></ul></nav></div></div><!--[--><!--]--><div class="spacer" data-v-3f215769></div><!--[--><!--]--><!----><!--[--><!--]--><!--[--><!--]--></div></div></div></div><div class="content" data-v-6b87e69f><div class="content-container" data-v-6b87e69f><!--[--><!--]--><!----><main class="main" data-v-6b87e69f><div style="position:relative;" class="vp-doc _articles_Blog_02HuggingFace%E5%85%A5%E9%97%A8" data-v-6b87e69f><div><h1 id="入门" tabindex="-1">入门 <a class="header-anchor" href="#入门" aria-label="Permalink to &quot;入门&quot;">​</a></h1><p>huggingface官网教程</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki github-dark vp-code-dark"><code><span class="line"><span style="color:#e1e4e8;">https://huggingface.co/learn</span></span></code></pre><pre class="shiki github-light vp-code-light"><code><span class="line"><span style="color:#24292e;">https://huggingface.co/learn</span></span></code></pre></div><p>安装</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki github-dark vp-code-dark"><code><span class="line"><span style="color:#e1e4e8;">pip install transformers</span></span></code></pre><pre class="shiki github-light vp-code-light"><code><span class="line"><span style="color:#24292e;">pip install transformers</span></span></code></pre></div><h2 id="pipline函数" tabindex="-1">pipline函数 <a class="header-anchor" href="#pipline函数" aria-label="Permalink to &quot;pipline函数&quot;">​</a></h2><p>pipline把所有东西都封装起来，一个函数即可用，但实际不会这么用。</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki github-dark vp-code-dark"><code><span class="line"><span style="color:#e1e4e8;">from transformers import pipeline</span></span>
<span class="line"><span style="color:#e1e4e8;"></span></span>
<span class="line"><span style="color:#e1e4e8;"></span></span>
<span class="line"><span style="color:#e1e4e8;"># 情感分析</span></span>
<span class="line"><span style="color:#e1e4e8;">classifier = pipeline(&quot;sentiment-analysis&quot;)</span></span>
<span class="line"><span style="color:#e1e4e8;">classifier(&quot;I&#39;ve been waiting for a HuggingFace course my whole life.&quot;)</span></span>
<span class="line"><span style="color:#e1e4e8;"># 文本生成</span></span>
<span class="line"><span style="color:#e1e4e8;"># model：指定模型</span></span>
<span class="line"><span style="color:#e1e4e8;">generator = pipeline(&quot;text-generation&quot;, model=&quot;distilgpt2&quot;)</span></span>
<span class="line"><span style="color:#e1e4e8;">generator(&quot;In this course, we will teach you how to&quot;, max_length=30)</span></span>
<span class="line"><span style="color:#e1e4e8;"># 命名实体识别</span></span>
<span class="line"><span style="color:#e1e4e8;">ner = pipeline(&quot;ner&quot;, grouped_entities=True)</span></span>
<span class="line"><span style="color:#e1e4e8;">ner(&quot;My name is Sylvain and I work at Hugging Face in Brooklyn.&quot;)</span></span></code></pre><pre class="shiki github-light vp-code-light"><code><span class="line"><span style="color:#24292e;">from transformers import pipeline</span></span>
<span class="line"><span style="color:#24292e;"></span></span>
<span class="line"><span style="color:#24292e;"></span></span>
<span class="line"><span style="color:#24292e;"># 情感分析</span></span>
<span class="line"><span style="color:#24292e;">classifier = pipeline(&quot;sentiment-analysis&quot;)</span></span>
<span class="line"><span style="color:#24292e;">classifier(&quot;I&#39;ve been waiting for a HuggingFace course my whole life.&quot;)</span></span>
<span class="line"><span style="color:#24292e;"># 文本生成</span></span>
<span class="line"><span style="color:#24292e;"># model：指定模型</span></span>
<span class="line"><span style="color:#24292e;">generator = pipeline(&quot;text-generation&quot;, model=&quot;distilgpt2&quot;)</span></span>
<span class="line"><span style="color:#24292e;">generator(&quot;In this course, we will teach you how to&quot;, max_length=30)</span></span>
<span class="line"><span style="color:#24292e;"># 命名实体识别</span></span>
<span class="line"><span style="color:#24292e;">ner = pipeline(&quot;ner&quot;, grouped_entities=True)</span></span>
<span class="line"><span style="color:#24292e;">ner(&quot;My name is Sylvain and I work at Hugging Face in Brooklyn.&quot;)</span></span></code></pre></div><h2 id="model" tabindex="-1">Model <a class="header-anchor" href="#model" aria-label="Permalink to &quot;Model&quot;">​</a></h2><p>加载模型，模型参数随机初始化，即，未训练的模型：</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki github-dark vp-code-dark"><code><span class="line"><span style="color:#e1e4e8;">from transformers import BertConfig, BertModel</span></span>
<span class="line"><span style="color:#e1e4e8;"></span></span>
<span class="line"><span style="color:#e1e4e8;">config = BertConfig()</span></span>
<span class="line"><span style="color:#e1e4e8;">model = BertModel(config)</span></span>
<span class="line"><span style="color:#e1e4e8;">print(config)	# config定义了模型结构</span></span></code></pre><pre class="shiki github-light vp-code-light"><code><span class="line"><span style="color:#24292e;">from transformers import BertConfig, BertModel</span></span>
<span class="line"><span style="color:#24292e;"></span></span>
<span class="line"><span style="color:#24292e;">config = BertConfig()</span></span>
<span class="line"><span style="color:#24292e;">model = BertModel(config)</span></span>
<span class="line"><span style="color:#24292e;">print(config)	# config定义了模型结构</span></span></code></pre></div><p>从checkpoint加载模型：</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki github-dark vp-code-dark"><code><span class="line"><span style="color:#e1e4e8;">model = BertModel.from_pretrained(&quot;bert-base-cased&quot;)</span></span></code></pre><pre class="shiki github-light vp-code-light"><code><span class="line"><span style="color:#24292e;">model = BertModel.from_pretrained(&quot;bert-base-cased&quot;)</span></span></code></pre></div><p>模型保存：</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki github-dark vp-code-dark"><code><span class="line"><span style="color:#e1e4e8;">model.save_pretrained(&quot;directory_on_my_computer&quot;)</span></span>
<span class="line"><span style="color:#e1e4e8;"># 保存后会有两个文件：</span></span>
<span class="line"><span style="color:#e1e4e8;"># config.json 配置+模型结构</span></span>
<span class="line"><span style="color:#e1e4e8;"># pytorch_model.bin 权重字典</span></span></code></pre><pre class="shiki github-light vp-code-light"><code><span class="line"><span style="color:#24292e;">model.save_pretrained(&quot;directory_on_my_computer&quot;)</span></span>
<span class="line"><span style="color:#24292e;"># 保存后会有两个文件：</span></span>
<span class="line"><span style="color:#24292e;"># config.json 配置+模型结构</span></span>
<span class="line"><span style="color:#24292e;"># pytorch_model.bin 权重字典</span></span></code></pre></div><h2 id="tokenizer" tabindex="-1">Tokenizer <a class="header-anchor" href="#tokenizer" aria-label="Permalink to &quot;Tokenizer&quot;">​</a></h2><p>input数据需要变成batch形式，才可以输入模型。</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki github-dark vp-code-dark"><code><span class="line"><span style="color:#e1e4e8;">import torch</span></span>
<span class="line"><span style="color:#e1e4e8;">from transformers import AutoTokenizer, AutoModelForSequenceClassification</span></span>
<span class="line"><span style="color:#e1e4e8;"></span></span>
<span class="line"><span style="color:#e1e4e8;">checkpoint = &quot;distilbert-base-uncased-finetuned-sst-2-english&quot;</span></span>
<span class="line"><span style="color:#e1e4e8;">tokenizer = AutoTokenizer.from_pretrained(checkpoint)</span></span>
<span class="line"><span style="color:#e1e4e8;">model = AutoModelForSequenceClassification.from_pretrained(checkpoint)</span></span>
<span class="line"><span style="color:#e1e4e8;"></span></span>
<span class="line"><span style="color:#e1e4e8;">sequence = &quot;I&#39;ve been waiting for a HuggingFace course my whole life.&quot;</span></span>
<span class="line"><span style="color:#e1e4e8;">tokens = tokenizer.tokenize(sequence)</span></span>
<span class="line"><span style="color:#e1e4e8;">ids = tokenizer.convert_tokens_to_ids(tokens)</span></span>
<span class="line"><span style="color:#e1e4e8;">input_ids = torch.tensor([ids])</span></span>
<span class="line"><span style="color:#e1e4e8;">print(input_ids)</span></span>
<span class="line"><span style="color:#e1e4e8;"># [[ 1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,  2607, 2026,  2878,  2166,  1012]]</span></span>
<span class="line"><span style="color:#e1e4e8;"></span></span>
<span class="line"><span style="color:#e1e4e8;">output = model(input_ids)</span></span>
<span class="line"><span style="color:#e1e4e8;">print(&quot;Logits:&quot;, output.logits)</span></span>
<span class="line"><span style="color:#e1e4e8;"># [[-2.7276,  2.8789]]</span></span></code></pre><pre class="shiki github-light vp-code-light"><code><span class="line"><span style="color:#24292e;">import torch</span></span>
<span class="line"><span style="color:#24292e;">from transformers import AutoTokenizer, AutoModelForSequenceClassification</span></span>
<span class="line"><span style="color:#24292e;"></span></span>
<span class="line"><span style="color:#24292e;">checkpoint = &quot;distilbert-base-uncased-finetuned-sst-2-english&quot;</span></span>
<span class="line"><span style="color:#24292e;">tokenizer = AutoTokenizer.from_pretrained(checkpoint)</span></span>
<span class="line"><span style="color:#24292e;">model = AutoModelForSequenceClassification.from_pretrained(checkpoint)</span></span>
<span class="line"><span style="color:#24292e;"></span></span>
<span class="line"><span style="color:#24292e;">sequence = &quot;I&#39;ve been waiting for a HuggingFace course my whole life.&quot;</span></span>
<span class="line"><span style="color:#24292e;">tokens = tokenizer.tokenize(sequence)</span></span>
<span class="line"><span style="color:#24292e;">ids = tokenizer.convert_tokens_to_ids(tokens)</span></span>
<span class="line"><span style="color:#24292e;">input_ids = torch.tensor([ids])</span></span>
<span class="line"><span style="color:#24292e;">print(input_ids)</span></span>
<span class="line"><span style="color:#24292e;"># [[ 1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,  2607, 2026,  2878,  2166,  1012]]</span></span>
<span class="line"><span style="color:#24292e;"></span></span>
<span class="line"><span style="color:#24292e;">output = model(input_ids)</span></span>
<span class="line"><span style="color:#24292e;">print(&quot;Logits:&quot;, output.logits)</span></span>
<span class="line"><span style="color:#24292e;"># [[-2.7276,  2.8789]]</span></span></code></pre></div><p>batch填充：tokenizer中都有pad_token_id</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki github-dark vp-code-dark"><code><span class="line"><span style="color:#e1e4e8;">batched_ids = [</span></span>
<span class="line"><span style="color:#e1e4e8;">    [200, 200, 200],</span></span>
<span class="line"><span style="color:#e1e4e8;">    [200, 200, tokenizer.pad_token_id],</span></span>
<span class="line"><span style="color:#e1e4e8;">]</span></span></code></pre><pre class="shiki github-light vp-code-light"><code><span class="line"><span style="color:#24292e;">batched_ids = [</span></span>
<span class="line"><span style="color:#24292e;">    [200, 200, 200],</span></span>
<span class="line"><span style="color:#24292e;">    [200, 200, tokenizer.pad_token_id],</span></span>
<span class="line"><span style="color:#24292e;">]</span></span></code></pre></div><p>自动填充：进一步简化代码</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki github-dark vp-code-dark"><code><span class="line"><span style="color:#e1e4e8;"># 按最长序列填充 （建议）</span></span>
<span class="line"><span style="color:#e1e4e8;">model_inputs = tokenizer(sequences, padding=&quot;longest&quot;)</span></span>
<span class="line"><span style="color:#e1e4e8;"></span></span>
<span class="line"><span style="color:#e1e4e8;"># 模型支持多长，填充到最长（不建议）</span></span>
<span class="line"><span style="color:#e1e4e8;">model_inputs = tokenizer(sequences, padding=&quot;max_length&quot;)</span></span>
<span class="line"><span style="color:#e1e4e8;"></span></span>
<span class="line"><span style="color:#e1e4e8;"># 填充到自定义的长度（不建议）</span></span>
<span class="line"><span style="color:#e1e4e8;">model_inputs = tokenizer(sequences, padding=&quot;max_length&quot;, max_length=8)</span></span></code></pre><pre class="shiki github-light vp-code-light"><code><span class="line"><span style="color:#24292e;"># 按最长序列填充 （建议）</span></span>
<span class="line"><span style="color:#24292e;">model_inputs = tokenizer(sequences, padding=&quot;longest&quot;)</span></span>
<span class="line"><span style="color:#24292e;"></span></span>
<span class="line"><span style="color:#24292e;"># 模型支持多长，填充到最长（不建议）</span></span>
<span class="line"><span style="color:#24292e;">model_inputs = tokenizer(sequences, padding=&quot;max_length&quot;)</span></span>
<span class="line"><span style="color:#24292e;"></span></span>
<span class="line"><span style="color:#24292e;"># 填充到自定义的长度（不建议）</span></span>
<span class="line"><span style="color:#24292e;">model_inputs = tokenizer(sequences, padding=&quot;max_length&quot;, max_length=8)</span></span></code></pre></div><p>句子截断</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki github-dark vp-code-dark"><code><span class="line"><span style="color:#e1e4e8;"># 超过最大长度，就会截断</span></span>
<span class="line"><span style="color:#e1e4e8;">model_inputs = tokenizer(sequences, truncation=True)</span></span></code></pre><pre class="shiki github-light vp-code-light"><code><span class="line"><span style="color:#24292e;"># 超过最大长度，就会截断</span></span>
<span class="line"><span style="color:#24292e;">model_inputs = tokenizer(sequences, truncation=True)</span></span></code></pre></div><p>自动返回张量：简化代码，不用再写转换张量代码</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki github-dark vp-code-dark"><code><span class="line"><span style="color:#e1e4e8;"># Returns PyTorch tensors</span></span>
<span class="line"><span style="color:#e1e4e8;">model_inputs = tokenizer(sequences, padding=True, return_tensors=&quot;pt&quot;)</span></span>
<span class="line"><span style="color:#e1e4e8;"></span></span>
<span class="line"><span style="color:#e1e4e8;"># Returns TensorFlow tensors</span></span>
<span class="line"><span style="color:#e1e4e8;">model_inputs = tokenizer(sequences, padding=True, return_tensors=&quot;tf&quot;)</span></span></code></pre><pre class="shiki github-light vp-code-light"><code><span class="line"><span style="color:#24292e;"># Returns PyTorch tensors</span></span>
<span class="line"><span style="color:#24292e;">model_inputs = tokenizer(sequences, padding=True, return_tensors=&quot;pt&quot;)</span></span>
<span class="line"><span style="color:#24292e;"></span></span>
<span class="line"><span style="color:#24292e;"># Returns TensorFlow tensors</span></span>
<span class="line"><span style="color:#24292e;">model_inputs = tokenizer(sequences, padding=True, return_tensors=&quot;tf&quot;)</span></span></code></pre></div><p>自动填充特殊token：比如BERT会在句子开头和模型添加特殊字符，此时Tokenizer也做了</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki github-dark vp-code-dark"><code><span class="line"><span style="color:#e1e4e8;">model_inputs = tokenizer(sequence)</span></span>
<span class="line"><span style="color:#e1e4e8;">print(tokenizer.decode(model_inputs[&quot;input_ids&quot;])) # 发现tokenizer自动做了特殊字符填充</span></span>
<span class="line"><span style="color:#e1e4e8;"></span></span>
<span class="line"><span style="color:#e1e4e8;">tokens = tokenizer.tokenize(sequence)</span></span>
<span class="line"><span style="color:#e1e4e8;">ids = tokenizer.convert_tokens_to_ids(tokens)</span></span>
<span class="line"><span style="color:#e1e4e8;">print(tokenizer.decode(ids))  # 自己调用 convert_tokens_to_ids并不会填充</span></span>
<span class="line"><span style="color:#e1e4e8;"></span></span>
<span class="line"><span style="color:#e1e4e8;"># &quot;[CLS] i&#39;ve been waiting for a huggingface course my whole life. [SEP]&quot;</span></span>
<span class="line"><span style="color:#e1e4e8;"># &quot;i&#39;ve been waiting for a huggingface course my whole life.&quot;</span></span></code></pre><pre class="shiki github-light vp-code-light"><code><span class="line"><span style="color:#24292e;">model_inputs = tokenizer(sequence)</span></span>
<span class="line"><span style="color:#24292e;">print(tokenizer.decode(model_inputs[&quot;input_ids&quot;])) # 发现tokenizer自动做了特殊字符填充</span></span>
<span class="line"><span style="color:#24292e;"></span></span>
<span class="line"><span style="color:#24292e;">tokens = tokenizer.tokenize(sequence)</span></span>
<span class="line"><span style="color:#24292e;">ids = tokenizer.convert_tokens_to_ids(tokens)</span></span>
<span class="line"><span style="color:#24292e;">print(tokenizer.decode(ids))  # 自己调用 convert_tokens_to_ids并不会填充</span></span>
<span class="line"><span style="color:#24292e;"></span></span>
<span class="line"><span style="color:#24292e;"># &quot;[CLS] i&#39;ve been waiting for a huggingface course my whole life. [SEP]&quot;</span></span>
<span class="line"><span style="color:#24292e;"># &quot;i&#39;ve been waiting for a huggingface course my whole life.&quot;</span></span></code></pre></div><p>总结：</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki github-dark vp-code-dark"><code><span class="line"><span style="color:#e1e4e8;"># 填充、截断、返回张量</span></span>
<span class="line"><span style="color:#e1e4e8;">tokens = tokenizer(sequences, padding=True, truncation=True, return_tensors=&quot;pt&quot;)</span></span>
<span class="line"><span style="color:#e1e4e8;">output = model(**tokens)</span></span></code></pre><pre class="shiki github-light vp-code-light"><code><span class="line"><span style="color:#24292e;"># 填充、截断、返回张量</span></span>
<span class="line"><span style="color:#24292e;">tokens = tokenizer(sequences, padding=True, truncation=True, return_tensors=&quot;pt&quot;)</span></span>
<span class="line"><span style="color:#24292e;">output = model(**tokens)</span></span></code></pre></div><h2 id="准备数据集" tabindex="-1">准备数据集 <a class="header-anchor" href="#准备数据集" aria-label="Permalink to &quot;准备数据集&quot;">​</a></h2><p>加载数据集：此命令下载并缓存数据集，默认位于 ~/.cache/huggingface/datasets 中。您可以通过设置 HF_HOME 环境变量来自定义缓存文件夹。</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki github-dark vp-code-dark"><code><span class="line"><span style="color:#e1e4e8;">from datasets import load_dataset</span></span>
<span class="line"><span style="color:#e1e4e8;"></span></span>
<span class="line"><span style="color:#e1e4e8;">raw_datasets = load_dataset(&quot;glue&quot;, &quot;mrpc&quot;)</span></span>
<span class="line"><span style="color:#e1e4e8;">raw_datasets</span></span></code></pre><pre class="shiki github-light vp-code-light"><code><span class="line"><span style="color:#24292e;">from datasets import load_dataset</span></span>
<span class="line"><span style="color:#24292e;"></span></span>
<span class="line"><span style="color:#24292e;">raw_datasets = load_dataset(&quot;glue&quot;, &quot;mrpc&quot;)</span></span>
<span class="line"><span style="color:#24292e;">raw_datasets</span></span></code></pre></div><p>批量转换数据集</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki github-dark vp-code-dark"><code><span class="line"><span style="color:#e1e4e8;">def tokenize_function(example):</span></span>
<span class="line"><span style="color:#e1e4e8;">    return tokenizer(example[&quot;sentence1&quot;], example[&quot;sentence2&quot;], truncation=True)</span></span>
<span class="line"><span style="color:#e1e4e8;"></span></span>
<span class="line"><span style="color:#e1e4e8;"></span></span>
<span class="line"><span style="color:#e1e4e8;">raw_datasets = load_dataset(&quot;glue&quot;, &quot;mrpc&quot;)</span></span>
<span class="line"><span style="color:#e1e4e8;">tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)</span></span></code></pre><pre class="shiki github-light vp-code-light"><code><span class="line"><span style="color:#24292e;">def tokenize_function(example):</span></span>
<span class="line"><span style="color:#24292e;">    return tokenizer(example[&quot;sentence1&quot;], example[&quot;sentence2&quot;], truncation=True)</span></span>
<span class="line"><span style="color:#24292e;"></span></span>
<span class="line"><span style="color:#24292e;"></span></span>
<span class="line"><span style="color:#24292e;">raw_datasets = load_dataset(&quot;glue&quot;, &quot;mrpc&quot;)</span></span>
<span class="line"><span style="color:#24292e;">tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)</span></span></code></pre></div><p>动态填充<strong>dynamic padding</strong></p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki github-dark vp-code-dark"><code><span class="line"><span style="color:#e1e4e8;">from transformers import DataCollatorWithPadding</span></span>
<span class="line"><span style="color:#e1e4e8;"></span></span>
<span class="line"><span style="color:#e1e4e8;">data_collator = DataCollatorWithPadding(tokenizer=tokenizer)</span></span>
<span class="line"><span style="color:#e1e4e8;">samples = tokenized_datasets[&quot;train&quot;][:8]</span></span>
<span class="line"><span style="color:#e1e4e8;">batch = data_collator(samples)</span></span></code></pre><pre class="shiki github-light vp-code-light"><code><span class="line"><span style="color:#24292e;">from transformers import DataCollatorWithPadding</span></span>
<span class="line"><span style="color:#24292e;"></span></span>
<span class="line"><span style="color:#24292e;">data_collator = DataCollatorWithPadding(tokenizer=tokenizer)</span></span>
<span class="line"><span style="color:#24292e;">samples = tokenized_datasets[&quot;train&quot;][:8]</span></span>
<span class="line"><span style="color:#24292e;">batch = data_collator(samples)</span></span></code></pre></div><h2 id="微调" tabindex="-1">微调 <a class="header-anchor" href="#微调" aria-label="Permalink to &quot;微调&quot;">​</a></h2><p>Trainer类就是来帮你微调的，你需要：</p><p>1、准备数据</p><p>2、定义参数TrainingArguments</p><p>3、开启微调trainer.train()</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki github-dark vp-code-dark"><code><span class="line"><span style="color:#e1e4e8;">from datasets import load_dataset</span></span>
<span class="line"><span style="color:#e1e4e8;">from transformers import AutoTokenizer, DataCollatorWithPadding, TrainingArguments, AutoModelForSequenceClassification, Trainer</span></span>
<span class="line"><span style="color:#e1e4e8;">import evaluate</span></span>
<span class="line"><span style="color:#e1e4e8;"></span></span>
<span class="line"><span style="color:#e1e4e8;"></span></span>
<span class="line"><span style="color:#e1e4e8;">checkpoint = &quot;bert-base-uncased&quot;</span></span>
<span class="line"><span style="color:#e1e4e8;">tokenizer = AutoTokenizer.from_pretrained(checkpoint)</span></span>
<span class="line"><span style="color:#e1e4e8;"># 注意1：这里修改了BERT模型的输出，定义了分类输出，会出现警告日志</span></span>
<span class="line"><span style="color:#e1e4e8;">model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)</span></span>
<span class="line"><span style="color:#e1e4e8;"></span></span>
<span class="line"><span style="color:#e1e4e8;">raw_datasets = load_dataset(&quot;glue&quot;, &quot;mrpc&quot;)</span></span>
<span class="line"><span style="color:#e1e4e8;">tokenized_datasets = raw_datasets.map(lambda example: tokenizer(example[&quot;sentence1&quot;], example[&quot;sentence2&quot;], truncation=True), batched=True)</span></span>
<span class="line"><span style="color:#e1e4e8;">data_collator = DataCollatorWithPadding(tokenizer=tokenizer)</span></span>
<span class="line"><span style="color:#e1e4e8;"></span></span>
<span class="line"><span style="color:#e1e4e8;"># 注意2：训练和评估的超参数（这里保留默认）</span></span>
<span class="line"><span style="color:#e1e4e8;">training_args = TrainingArguments(checkpoint, evaluation_strategy=&quot;epoch&quot;)</span></span>
<span class="line"><span style="color:#e1e4e8;"></span></span>
<span class="line"><span style="color:#e1e4e8;"></span></span>
<span class="line"><span style="color:#e1e4e8;"># 注意3：这里定义了一个评估方法，参数就是模型的输出</span></span>
<span class="line"><span style="color:#e1e4e8;">def compute_metrics(eval_preds):</span></span>
<span class="line"><span style="color:#e1e4e8;">    metric = evaluate.load(&quot;glue&quot;, &quot;mrpc&quot;)</span></span>
<span class="line"><span style="color:#e1e4e8;">    logits, labels = eval_preds</span></span>
<span class="line"><span style="color:#e1e4e8;">    predictions = np.argmax(logits, axis=-1)</span></span>
<span class="line"><span style="color:#e1e4e8;">    return metric.compute(predictions=predictions, references=labels)</span></span>
<span class="line"><span style="color:#e1e4e8;"></span></span>
<span class="line"><span style="color:#e1e4e8;"></span></span>
<span class="line"><span style="color:#e1e4e8;">trainer = Trainer(</span></span>
<span class="line"><span style="color:#e1e4e8;">    model,</span></span>
<span class="line"><span style="color:#e1e4e8;">    training_args,</span></span>
<span class="line"><span style="color:#e1e4e8;">    train_dataset=tokenized_datasets[&quot;train&quot;],</span></span>
<span class="line"><span style="color:#e1e4e8;">    eval_dataset=tokenized_datasets[&quot;validation&quot;],</span></span>
<span class="line"><span style="color:#e1e4e8;">    data_collator=data_collator,</span></span>
<span class="line"><span style="color:#e1e4e8;">    tokenizer=tokenizer,</span></span>
<span class="line"><span style="color:#e1e4e8;">    compute_metrics=compute_metrics,</span></span>
<span class="line"><span style="color:#e1e4e8;">)</span></span>
<span class="line"><span style="color:#e1e4e8;">trainer.train()</span></span></code></pre><pre class="shiki github-light vp-code-light"><code><span class="line"><span style="color:#24292e;">from datasets import load_dataset</span></span>
<span class="line"><span style="color:#24292e;">from transformers import AutoTokenizer, DataCollatorWithPadding, TrainingArguments, AutoModelForSequenceClassification, Trainer</span></span>
<span class="line"><span style="color:#24292e;">import evaluate</span></span>
<span class="line"><span style="color:#24292e;"></span></span>
<span class="line"><span style="color:#24292e;"></span></span>
<span class="line"><span style="color:#24292e;">checkpoint = &quot;bert-base-uncased&quot;</span></span>
<span class="line"><span style="color:#24292e;">tokenizer = AutoTokenizer.from_pretrained(checkpoint)</span></span>
<span class="line"><span style="color:#24292e;"># 注意1：这里修改了BERT模型的输出，定义了分类输出，会出现警告日志</span></span>
<span class="line"><span style="color:#24292e;">model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)</span></span>
<span class="line"><span style="color:#24292e;"></span></span>
<span class="line"><span style="color:#24292e;">raw_datasets = load_dataset(&quot;glue&quot;, &quot;mrpc&quot;)</span></span>
<span class="line"><span style="color:#24292e;">tokenized_datasets = raw_datasets.map(lambda example: tokenizer(example[&quot;sentence1&quot;], example[&quot;sentence2&quot;], truncation=True), batched=True)</span></span>
<span class="line"><span style="color:#24292e;">data_collator = DataCollatorWithPadding(tokenizer=tokenizer)</span></span>
<span class="line"><span style="color:#24292e;"></span></span>
<span class="line"><span style="color:#24292e;"># 注意2：训练和评估的超参数（这里保留默认）</span></span>
<span class="line"><span style="color:#24292e;">training_args = TrainingArguments(checkpoint, evaluation_strategy=&quot;epoch&quot;)</span></span>
<span class="line"><span style="color:#24292e;"></span></span>
<span class="line"><span style="color:#24292e;"></span></span>
<span class="line"><span style="color:#24292e;"># 注意3：这里定义了一个评估方法，参数就是模型的输出</span></span>
<span class="line"><span style="color:#24292e;">def compute_metrics(eval_preds):</span></span>
<span class="line"><span style="color:#24292e;">    metric = evaluate.load(&quot;glue&quot;, &quot;mrpc&quot;)</span></span>
<span class="line"><span style="color:#24292e;">    logits, labels = eval_preds</span></span>
<span class="line"><span style="color:#24292e;">    predictions = np.argmax(logits, axis=-1)</span></span>
<span class="line"><span style="color:#24292e;">    return metric.compute(predictions=predictions, references=labels)</span></span>
<span class="line"><span style="color:#24292e;"></span></span>
<span class="line"><span style="color:#24292e;"></span></span>
<span class="line"><span style="color:#24292e;">trainer = Trainer(</span></span>
<span class="line"><span style="color:#24292e;">    model,</span></span>
<span class="line"><span style="color:#24292e;">    training_args,</span></span>
<span class="line"><span style="color:#24292e;">    train_dataset=tokenized_datasets[&quot;train&quot;],</span></span>
<span class="line"><span style="color:#24292e;">    eval_dataset=tokenized_datasets[&quot;validation&quot;],</span></span>
<span class="line"><span style="color:#24292e;">    data_collator=data_collator,</span></span>
<span class="line"><span style="color:#24292e;">    tokenizer=tokenizer,</span></span>
<span class="line"><span style="color:#24292e;">    compute_metrics=compute_metrics,</span></span>
<span class="line"><span style="color:#24292e;">)</span></span>
<span class="line"><span style="color:#24292e;">trainer.train()</span></span></code></pre></div><h2 id="训练" tabindex="-1">训练 <a class="header-anchor" href="#训练" aria-label="Permalink to &quot;训练&quot;">​</a></h2><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki github-dark vp-code-dark"><code><span class="line"><span style="color:#e1e4e8;">from datasets import load_dataset</span></span>
<span class="line"><span style="color:#e1e4e8;">from transformers import AutoTokenizer, DataCollatorWithPadding</span></span>
<span class="line"><span style="color:#e1e4e8;">from torch.utils.data import DataLoader</span></span>
<span class="line"><span style="color:#e1e4e8;">from transformers import AdamW, get_scheduler</span></span>
<span class="line"><span style="color:#e1e4e8;">from tqdm.auto import tqdm</span></span>
<span class="line"><span style="color:#e1e4e8;">import evaluate</span></span>
<span class="line"><span style="color:#e1e4e8;"></span></span>
<span class="line"><span style="color:#e1e4e8;"></span></span>
<span class="line"><span style="color:#e1e4e8;">device = torch.device(&quot;cuda&quot;) if torch.cuda.is_available() else torch.device(&quot;cpu&quot;)</span></span>
<span class="line"><span style="color:#e1e4e8;">checkpoint = &quot;bert-base-uncased&quot;</span></span>
<span class="line"><span style="color:#e1e4e8;">tokenizer = AutoTokenizer.from_pretrained(checkpoint)</span></span>
<span class="line"><span style="color:#e1e4e8;">model = AutoModel.from_pretrained(checkpoint, num_labels=2)</span></span>
<span class="line"><span style="color:#e1e4e8;">model.to(device)</span></span>
<span class="line"><span style="color:#e1e4e8;"></span></span>
<span class="line"><span style="color:#e1e4e8;">raw_datasets = load_dataset(&quot;glue&quot;, &quot;mrpc&quot;)</span></span>
<span class="line"><span style="color:#e1e4e8;">tokenized_datasets = raw_datasets.map(lambda example: tokenizer(example[&quot;sentence1&quot;], example[&quot;sentence2&quot;], truncation=True), batched=True)</span></span>
<span class="line"><span style="color:#e1e4e8;"># 删除不用的列</span></span>
<span class="line"><span style="color:#e1e4e8;">tokenized_datasets = tokenized_datasets.remove_columns([&quot;sentence1&quot;, &quot;sentence2&quot;, &quot;idx&quot;])</span></span>
<span class="line"><span style="color:#e1e4e8;"># 重命名标签</span></span>
<span class="line"><span style="color:#e1e4e8;">tokenized_datasets = tokenized_datasets.rename_column(&quot;label&quot;, &quot;labels&quot;)</span></span>
<span class="line"><span style="color:#e1e4e8;"># 转为张量</span></span>
<span class="line"><span style="color:#e1e4e8;">tokenized_datasets.set_format(&quot;torch&quot;)</span></span>
<span class="line"><span style="color:#e1e4e8;"># 动态填充</span></span>
<span class="line"><span style="color:#e1e4e8;">data_collator = DataCollatorWithPadding(tokenizer=tokenizer)</span></span>
<span class="line"><span style="color:#e1e4e8;"># 加载器</span></span>
<span class="line"><span style="color:#e1e4e8;">train_dataloader = DataLoader(</span></span>
<span class="line"><span style="color:#e1e4e8;">    tokenized_datasets[&quot;train&quot;], shuffle=True, batch_size=8, collate_fn=data_collator</span></span>
<span class="line"><span style="color:#e1e4e8;">)</span></span>
<span class="line"><span style="color:#e1e4e8;">eval_dataloader = DataLoader(</span></span>
<span class="line"><span style="color:#e1e4e8;">    tokenized_datasets[&quot;validation&quot;], batch_size=8, collate_fn=data_collator</span></span>
<span class="line"><span style="color:#e1e4e8;">)</span></span>
<span class="line"><span style="color:#e1e4e8;"></span></span>
<span class="line"><span style="color:#e1e4e8;"># 优化器</span></span>
<span class="line"><span style="color:#e1e4e8;">optimizer = AdamW(model.parameters(), lr=5e-5)</span></span>
<span class="line"><span style="color:#e1e4e8;">num_epochs = 3</span></span>
<span class="line"><span style="color:#e1e4e8;"># 学习率线性衰减至0</span></span>
<span class="line"><span style="color:#e1e4e8;">num_training_steps = num_epochs * len(train_dataloader)</span></span>
<span class="line"><span style="color:#e1e4e8;">lr_scheduler = get_scheduler(</span></span>
<span class="line"><span style="color:#e1e4e8;">    &quot;linear&quot;,</span></span>
<span class="line"><span style="color:#e1e4e8;">    optimizer=optimizer,</span></span>
<span class="line"><span style="color:#e1e4e8;">    num_warmup_steps=0,</span></span>
<span class="line"><span style="color:#e1e4e8;">    num_training_steps=num_training_steps,</span></span>
<span class="line"><span style="color:#e1e4e8;">)</span></span>
<span class="line"><span style="color:#e1e4e8;"></span></span>
<span class="line"><span style="color:#e1e4e8;">progress_bar = tqdm(range(num_training_steps))</span></span>
<span class="line"><span style="color:#e1e4e8;">for epoch in range(num_epochs):</span></span>
<span class="line"><span style="color:#e1e4e8;">    model.train()</span></span>
<span class="line"><span style="color:#e1e4e8;">    for batch in train_dataloader:</span></span>
<span class="line"><span style="color:#e1e4e8;">        batch = {k: v.to(device) for k, v in batch.items()}</span></span>
<span class="line"><span style="color:#e1e4e8;">        outputs = model(**batch)</span></span>
<span class="line"><span style="color:#e1e4e8;">        loss = outputs.loss</span></span>
<span class="line"><span style="color:#e1e4e8;">        loss.backward()</span></span>
<span class="line"><span style="color:#e1e4e8;"></span></span>
<span class="line"><span style="color:#e1e4e8;">        optimizer.step()</span></span>
<span class="line"><span style="color:#e1e4e8;">        lr_scheduler.step()</span></span>
<span class="line"><span style="color:#e1e4e8;">        optimizer.zero_grad()</span></span>
<span class="line"><span style="color:#e1e4e8;">        progress_bar.update(1)</span></span>
<span class="line"><span style="color:#e1e4e8;"></span></span>
<span class="line"><span style="color:#e1e4e8;">    model.eval()</span></span>
<span class="line"><span style="color:#e1e4e8;">    metric = evaluate.load(&quot;glue&quot;, &quot;mrpc&quot;)</span></span>
<span class="line"><span style="color:#e1e4e8;">    for batch in eval_dataloader:</span></span>
<span class="line"><span style="color:#e1e4e8;">        batch = {k: v.to(device) for k, v in batch.items()}</span></span>
<span class="line"><span style="color:#e1e4e8;">        with torch.no_grad():</span></span>
<span class="line"><span style="color:#e1e4e8;">            outputs = model(**batch)</span></span>
<span class="line"><span style="color:#e1e4e8;">        logits = outputs.logits</span></span>
<span class="line"><span style="color:#e1e4e8;">        predictions = torch.argmax(logits, dim=-1)</span></span>
<span class="line"><span style="color:#e1e4e8;">        metric.add_batch(predictions=predictions, references=batch[&quot;labels&quot;])</span></span>
<span class="line"><span style="color:#e1e4e8;">    metric.compute()</span></span></code></pre><pre class="shiki github-light vp-code-light"><code><span class="line"><span style="color:#24292e;">from datasets import load_dataset</span></span>
<span class="line"><span style="color:#24292e;">from transformers import AutoTokenizer, DataCollatorWithPadding</span></span>
<span class="line"><span style="color:#24292e;">from torch.utils.data import DataLoader</span></span>
<span class="line"><span style="color:#24292e;">from transformers import AdamW, get_scheduler</span></span>
<span class="line"><span style="color:#24292e;">from tqdm.auto import tqdm</span></span>
<span class="line"><span style="color:#24292e;">import evaluate</span></span>
<span class="line"><span style="color:#24292e;"></span></span>
<span class="line"><span style="color:#24292e;"></span></span>
<span class="line"><span style="color:#24292e;">device = torch.device(&quot;cuda&quot;) if torch.cuda.is_available() else torch.device(&quot;cpu&quot;)</span></span>
<span class="line"><span style="color:#24292e;">checkpoint = &quot;bert-base-uncased&quot;</span></span>
<span class="line"><span style="color:#24292e;">tokenizer = AutoTokenizer.from_pretrained(checkpoint)</span></span>
<span class="line"><span style="color:#24292e;">model = AutoModel.from_pretrained(checkpoint, num_labels=2)</span></span>
<span class="line"><span style="color:#24292e;">model.to(device)</span></span>
<span class="line"><span style="color:#24292e;"></span></span>
<span class="line"><span style="color:#24292e;">raw_datasets = load_dataset(&quot;glue&quot;, &quot;mrpc&quot;)</span></span>
<span class="line"><span style="color:#24292e;">tokenized_datasets = raw_datasets.map(lambda example: tokenizer(example[&quot;sentence1&quot;], example[&quot;sentence2&quot;], truncation=True), batched=True)</span></span>
<span class="line"><span style="color:#24292e;"># 删除不用的列</span></span>
<span class="line"><span style="color:#24292e;">tokenized_datasets = tokenized_datasets.remove_columns([&quot;sentence1&quot;, &quot;sentence2&quot;, &quot;idx&quot;])</span></span>
<span class="line"><span style="color:#24292e;"># 重命名标签</span></span>
<span class="line"><span style="color:#24292e;">tokenized_datasets = tokenized_datasets.rename_column(&quot;label&quot;, &quot;labels&quot;)</span></span>
<span class="line"><span style="color:#24292e;"># 转为张量</span></span>
<span class="line"><span style="color:#24292e;">tokenized_datasets.set_format(&quot;torch&quot;)</span></span>
<span class="line"><span style="color:#24292e;"># 动态填充</span></span>
<span class="line"><span style="color:#24292e;">data_collator = DataCollatorWithPadding(tokenizer=tokenizer)</span></span>
<span class="line"><span style="color:#24292e;"># 加载器</span></span>
<span class="line"><span style="color:#24292e;">train_dataloader = DataLoader(</span></span>
<span class="line"><span style="color:#24292e;">    tokenized_datasets[&quot;train&quot;], shuffle=True, batch_size=8, collate_fn=data_collator</span></span>
<span class="line"><span style="color:#24292e;">)</span></span>
<span class="line"><span style="color:#24292e;">eval_dataloader = DataLoader(</span></span>
<span class="line"><span style="color:#24292e;">    tokenized_datasets[&quot;validation&quot;], batch_size=8, collate_fn=data_collator</span></span>
<span class="line"><span style="color:#24292e;">)</span></span>
<span class="line"><span style="color:#24292e;"></span></span>
<span class="line"><span style="color:#24292e;"># 优化器</span></span>
<span class="line"><span style="color:#24292e;">optimizer = AdamW(model.parameters(), lr=5e-5)</span></span>
<span class="line"><span style="color:#24292e;">num_epochs = 3</span></span>
<span class="line"><span style="color:#24292e;"># 学习率线性衰减至0</span></span>
<span class="line"><span style="color:#24292e;">num_training_steps = num_epochs * len(train_dataloader)</span></span>
<span class="line"><span style="color:#24292e;">lr_scheduler = get_scheduler(</span></span>
<span class="line"><span style="color:#24292e;">    &quot;linear&quot;,</span></span>
<span class="line"><span style="color:#24292e;">    optimizer=optimizer,</span></span>
<span class="line"><span style="color:#24292e;">    num_warmup_steps=0,</span></span>
<span class="line"><span style="color:#24292e;">    num_training_steps=num_training_steps,</span></span>
<span class="line"><span style="color:#24292e;">)</span></span>
<span class="line"><span style="color:#24292e;"></span></span>
<span class="line"><span style="color:#24292e;">progress_bar = tqdm(range(num_training_steps))</span></span>
<span class="line"><span style="color:#24292e;">for epoch in range(num_epochs):</span></span>
<span class="line"><span style="color:#24292e;">    model.train()</span></span>
<span class="line"><span style="color:#24292e;">    for batch in train_dataloader:</span></span>
<span class="line"><span style="color:#24292e;">        batch = {k: v.to(device) for k, v in batch.items()}</span></span>
<span class="line"><span style="color:#24292e;">        outputs = model(**batch)</span></span>
<span class="line"><span style="color:#24292e;">        loss = outputs.loss</span></span>
<span class="line"><span style="color:#24292e;">        loss.backward()</span></span>
<span class="line"><span style="color:#24292e;"></span></span>
<span class="line"><span style="color:#24292e;">        optimizer.step()</span></span>
<span class="line"><span style="color:#24292e;">        lr_scheduler.step()</span></span>
<span class="line"><span style="color:#24292e;">        optimizer.zero_grad()</span></span>
<span class="line"><span style="color:#24292e;">        progress_bar.update(1)</span></span>
<span class="line"><span style="color:#24292e;"></span></span>
<span class="line"><span style="color:#24292e;">    model.eval()</span></span>
<span class="line"><span style="color:#24292e;">    metric = evaluate.load(&quot;glue&quot;, &quot;mrpc&quot;)</span></span>
<span class="line"><span style="color:#24292e;">    for batch in eval_dataloader:</span></span>
<span class="line"><span style="color:#24292e;">        batch = {k: v.to(device) for k, v in batch.items()}</span></span>
<span class="line"><span style="color:#24292e;">        with torch.no_grad():</span></span>
<span class="line"><span style="color:#24292e;">            outputs = model(**batch)</span></span>
<span class="line"><span style="color:#24292e;">        logits = outputs.logits</span></span>
<span class="line"><span style="color:#24292e;">        predictions = torch.argmax(logits, dim=-1)</span></span>
<span class="line"><span style="color:#24292e;">        metric.add_batch(predictions=predictions, references=batch[&quot;labels&quot;])</span></span>
<span class="line"><span style="color:#24292e;">    metric.compute()</span></span></code></pre></div><h1 id="数据集详解" tabindex="-1">数据集详解 <a class="header-anchor" href="#数据集详解" aria-label="Permalink to &quot;数据集详解&quot;">​</a></h1><h2 id="数据处理" tabindex="-1">数据处理 <a class="header-anchor" href="#数据处理" aria-label="Permalink to &quot;数据处理&quot;">​</a></h2><p>加载本地数据集</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki github-dark vp-code-dark"><code><span class="line"><span style="color:#e1e4e8;">from datasets import load_dataset</span></span>
<span class="line"><span style="color:#e1e4e8;"></span></span>
<span class="line"><span style="color:#e1e4e8;">!wget https://github.com/crux82/squad-it/raw/master/SQuAD_it-train.json.gz</span></span>
<span class="line"><span style="color:#e1e4e8;">!wget https://github.com/crux82/squad-it/raw/master/SQuAD_it-test.json.gz</span></span>
<span class="line"><span style="color:#e1e4e8;">raw_data = load_dataset(&quot;json&quot;, data_files=&quot;SQuAD_it-train.json&quot;, field=&quot;data&quot;)</span></span></code></pre><pre class="shiki github-light vp-code-light"><code><span class="line"><span style="color:#24292e;">from datasets import load_dataset</span></span>
<span class="line"><span style="color:#24292e;"></span></span>
<span class="line"><span style="color:#24292e;">!wget https://github.com/crux82/squad-it/raw/master/SQuAD_it-train.json.gz</span></span>
<span class="line"><span style="color:#24292e;">!wget https://github.com/crux82/squad-it/raw/master/SQuAD_it-test.json.gz</span></span>
<span class="line"><span style="color:#24292e;">raw_data = load_dataset(&quot;json&quot;, data_files=&quot;SQuAD_it-train.json&quot;, field=&quot;data&quot;)</span></span></code></pre></div><p>打乱和选择</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki github-dark vp-code-dark"><code><span class="line"><span style="color:#e1e4e8;">drug_sample = drug_dataset[&quot;train&quot;].shuffle(seed=42).select(range(1000))</span></span>
<span class="line"><span style="color:#e1e4e8;">drug_sample[:3]</span></span></code></pre><pre class="shiki github-light vp-code-light"><code><span class="line"><span style="color:#24292e;">drug_sample = drug_dataset[&quot;train&quot;].shuffle(seed=42).select(range(1000))</span></span>
<span class="line"><span style="color:#24292e;">drug_sample[:3]</span></span></code></pre></div><p>过滤None数据</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki github-dark vp-code-dark"><code><span class="line"><span style="color:#e1e4e8;">drug_dataset = drug_dataset.filter(lambda x: x[&quot;condition&quot;] is not None)</span></span></code></pre><pre class="shiki github-light vp-code-light"><code><span class="line"><span style="color:#24292e;">drug_dataset = drug_dataset.filter(lambda x: x[&quot;condition&quot;] is not None)</span></span></code></pre></div><p>创建新列</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki github-dark vp-code-dark"><code><span class="line"><span style="color:#e1e4e8;">drug_dataset = drug_dataset.map(lambda x: {&quot;review_length&quot;: len(x[&quot;review&quot;].split())})</span></span></code></pre><pre class="shiki github-light vp-code-light"><code><span class="line"><span style="color:#24292e;">drug_dataset = drug_dataset.map(lambda x: {&quot;review_length&quot;: len(x[&quot;review&quot;].split())})</span></span></code></pre></div><p>通过batch操作，加速</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki github-dark vp-code-dark"><code><span class="line"><span style="color:#e1e4e8;">tokenizer = AutoTokenizer.from_pretrained(&quot;bert-base-cased&quot;)</span></span>
<span class="line"><span style="color:#e1e4e8;"></span></span>
<span class="line"><span style="color:#e1e4e8;"></span></span>
<span class="line"><span style="color:#e1e4e8;">def tokenize_and_split(examples):</span></span>
<span class="line"><span style="color:#e1e4e8;">    return tokenizer(examples[&quot;review&quot;], truncation=True, max_length=128, return_overflowing_tokens=True)</span></span>
<span class="line"><span style="color:#e1e4e8;"></span></span>
<span class="line"><span style="color:#e1e4e8;"></span></span>
<span class="line"><span style="color:#e1e4e8;">tokenized_dataset = drug_dataset.map(tokenize_and_split, batched=True)</span></span></code></pre><pre class="shiki github-light vp-code-light"><code><span class="line"><span style="color:#24292e;">tokenizer = AutoTokenizer.from_pretrained(&quot;bert-base-cased&quot;)</span></span>
<span class="line"><span style="color:#24292e;"></span></span>
<span class="line"><span style="color:#24292e;"></span></span>
<span class="line"><span style="color:#24292e;">def tokenize_and_split(examples):</span></span>
<span class="line"><span style="color:#24292e;">    return tokenizer(examples[&quot;review&quot;], truncation=True, max_length=128, return_overflowing_tokens=True)</span></span>
<span class="line"><span style="color:#24292e;"></span></span>
<span class="line"><span style="color:#24292e;"></span></span>
<span class="line"><span style="color:#24292e;">tokenized_dataset = drug_dataset.map(tokenize_and_split, batched=True)</span></span></code></pre></div><p>上面会报错，因为return_overflowing_tokens会导致多出几列，所以还要删除旧列。</p><p>删除所有旧列</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki github-dark vp-code-dark"><code><span class="line"><span style="color:#e1e4e8;">tokenized_dataset = drug_dataset.map(</span></span>
<span class="line"><span style="color:#e1e4e8;">    tokenize_and_split, batched=True, remove_columns=drug_dataset[&quot;train&quot;].column_names</span></span>
<span class="line"><span style="color:#e1e4e8;">)</span></span></code></pre><pre class="shiki github-light vp-code-light"><code><span class="line"><span style="color:#24292e;">tokenized_dataset = drug_dataset.map(</span></span>
<span class="line"><span style="color:#24292e;">    tokenize_and_split, batched=True, remove_columns=drug_dataset[&quot;train&quot;].column_names</span></span>
<span class="line"><span style="color:#24292e;">)</span></span></code></pre></div><h2 id="数据流" tabindex="-1">数据流 <a class="header-anchor" href="#数据流" aria-label="Permalink to &quot;数据流&quot;">​</a></h2><p>几百G数据怎么处理，流失处理：</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki github-dark vp-code-dark"><code><span class="line"><span style="color:#e1e4e8;">pubmed_dataset_streamed = load_dataset(</span></span>
<span class="line"><span style="color:#e1e4e8;">    &quot;json&quot;, data_files=data_files, split=&quot;train&quot;, streaming=True</span></span>
<span class="line"><span style="color:#e1e4e8;">)</span></span>
<span class="line"><span style="color:#e1e4e8;">next(iter(pubmed_dataset_streamed))</span></span></code></pre><pre class="shiki github-light vp-code-light"><code><span class="line"><span style="color:#24292e;">pubmed_dataset_streamed = load_dataset(</span></span>
<span class="line"><span style="color:#24292e;">    &quot;json&quot;, data_files=data_files, split=&quot;train&quot;, streaming=True</span></span>
<span class="line"><span style="color:#24292e;">)</span></span>
<span class="line"><span style="color:#24292e;">next(iter(pubmed_dataset_streamed))</span></span></code></pre></div><p>打乱数据</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki github-dark vp-code-dark"><code><span class="line"><span style="color:#e1e4e8;">shuffled_dataset = pubmed_dataset_streamed.shuffle(buffer_size=10_000, seed=42)</span></span>
<span class="line"><span style="color:#e1e4e8;">next(iter(shuffled_dataset))</span></span></code></pre><pre class="shiki github-light vp-code-light"><code><span class="line"><span style="color:#24292e;">shuffled_dataset = pubmed_dataset_streamed.shuffle(buffer_size=10_000, seed=42)</span></span>
<span class="line"><span style="color:#24292e;">next(iter(shuffled_dataset))</span></span></code></pre></div><h1 id="peft" tabindex="-1">PEFT <a class="header-anchor" href="#peft" aria-label="Permalink to &quot;PEFT&quot;">​</a></h1><h2 id="微调代码" tabindex="-1">微调代码 <a class="header-anchor" href="#微调代码" aria-label="Permalink to &quot;微调代码&quot;">​</a></h2><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki github-dark vp-code-dark"><code><span class="line"><span style="color:#e1e4e8;">from peft import LoraConfig, TaskType, get_peft_model</span></span>
<span class="line"><span style="color:#e1e4e8;">from transformers import AutoModelForSeq2SeqLM, AutoTokenizer</span></span>
<span class="line"><span style="color:#e1e4e8;"></span></span>
<span class="line"><span style="color:#e1e4e8;"></span></span>
<span class="line"><span style="color:#e1e4e8;">checkpoint = &quot;bigscience/mt0-large&quot;</span></span>
<span class="line"><span style="color:#e1e4e8;">model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)</span></span>
<span class="line"><span style="color:#e1e4e8;">tokenizer = AutoTokenizer.from_pretrained(checkpoint)</span></span>
<span class="line"><span style="color:#e1e4e8;"></span></span>
<span class="line"><span style="color:#e1e4e8;"># peft</span></span>
<span class="line"><span style="color:#e1e4e8;">peft_config = LoraConfig(task_type=TaskType.SEQ_2_SEQ_LM,</span></span>
<span class="line"><span style="color:#e1e4e8;">                         inference_mode=False,</span></span>
<span class="line"><span style="color:#e1e4e8;">                         r=8,</span></span>
<span class="line"><span style="color:#e1e4e8;">                         lora_alpha=32,</span></span>
<span class="line"><span style="color:#e1e4e8;">                         lora_dropout=0.05)</span></span>
<span class="line"><span style="color:#e1e4e8;">model = get_peft_model(model, peft_config)</span></span>
<span class="line"><span style="color:#e1e4e8;">model.print_trainable_parameters()</span></span>
<span class="line"><span style="color:#e1e4e8;"></span></span>
<span class="line"><span style="color:#e1e4e8;">train_dataset=</span></span>
<span class="line"><span style="color:#e1e4e8;">eval_dataset=</span></span>
<span class="line"><span style="color:#e1e4e8;">data_collator=</span></span>
<span class="line"><span style="color:#e1e4e8;"></span></span>
<span class="line"><span style="color:#e1e4e8;">trainer = LoRATrainer(</span></span>
<span class="line"><span style="color:#e1e4e8;">    model=model,</span></span>
<span class="line"><span style="color:#e1e4e8;">    args=hf_train_args,</span></span>
<span class="line"><span style="color:#e1e4e8;">    train_dataset=tfrom peft import LoraConfig, TaskType, get_peft_model</span></span>
<span class="line"><span style="color:#e1e4e8;">trainer.train()</span></span>
<span class="line"><span style="color:#e1e4e8;"># 保存得到 adapter_config.json adapter_model.bin</span></span>
<span class="line"><span style="color:#e1e4e8;">trainer.model.save_pretrained(output_dir)</span></span></code></pre><pre class="shiki github-light vp-code-light"><code><span class="line"><span style="color:#24292e;">from peft import LoraConfig, TaskType, get_peft_model</span></span>
<span class="line"><span style="color:#24292e;">from transformers import AutoModelForSeq2SeqLM, AutoTokenizer</span></span>
<span class="line"><span style="color:#24292e;"></span></span>
<span class="line"><span style="color:#24292e;"></span></span>
<span class="line"><span style="color:#24292e;">checkpoint = &quot;bigscience/mt0-large&quot;</span></span>
<span class="line"><span style="color:#24292e;">model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)</span></span>
<span class="line"><span style="color:#24292e;">tokenizer = AutoTokenizer.from_pretrained(checkpoint)</span></span>
<span class="line"><span style="color:#24292e;"></span></span>
<span class="line"><span style="color:#24292e;"># peft</span></span>
<span class="line"><span style="color:#24292e;">peft_config = LoraConfig(task_type=TaskType.SEQ_2_SEQ_LM,</span></span>
<span class="line"><span style="color:#24292e;">                         inference_mode=False,</span></span>
<span class="line"><span style="color:#24292e;">                         r=8,</span></span>
<span class="line"><span style="color:#24292e;">                         lora_alpha=32,</span></span>
<span class="line"><span style="color:#24292e;">                         lora_dropout=0.05)</span></span>
<span class="line"><span style="color:#24292e;">model = get_peft_model(model, peft_config)</span></span>
<span class="line"><span style="color:#24292e;">model.print_trainable_parameters()</span></span>
<span class="line"><span style="color:#24292e;"></span></span>
<span class="line"><span style="color:#24292e;">train_dataset=</span></span>
<span class="line"><span style="color:#24292e;">eval_dataset=</span></span>
<span class="line"><span style="color:#24292e;">data_collator=</span></span>
<span class="line"><span style="color:#24292e;"></span></span>
<span class="line"><span style="color:#24292e;">trainer = LoRATrainer(</span></span>
<span class="line"><span style="color:#24292e;">    model=model,</span></span>
<span class="line"><span style="color:#24292e;">    args=hf_train_args,</span></span>
<span class="line"><span style="color:#24292e;">    train_dataset=tfrom peft import LoraConfig, TaskType, get_peft_model</span></span>
<span class="line"><span style="color:#24292e;">trainer.train()</span></span>
<span class="line"><span style="color:#24292e;"># 保存得到 adapter_config.json adapter_model.bin</span></span>
<span class="line"><span style="color:#24292e;">trainer.model.save_pretrained(output_dir)</span></span></code></pre></div><h2 id="模型加载" tabindex="-1">模型加载 <a class="header-anchor" href="#模型加载" aria-label="Permalink to &quot;模型加载&quot;">​</a></h2><p>加载peft配置，加载base模型，加载peft+base模型</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki github-dark vp-code-dark"><code><span class="line"><span style="color:#e1e4e8;">from transformers import AutoModelForSeq2SeqLM</span></span>
<span class="line"><span style="color:#e1e4e8;">from peft import PeftModel, PeftConfig</span></span>
<span class="line"><span style="color:#e1e4e8;"></span></span>
<span class="line"><span style="color:#e1e4e8;"></span></span>
<span class="line"><span style="color:#e1e4e8;">peft_checkpoint = &quot;smangrul/twitter_complaints_bigscience_T0_3B_LORA_SEQ_2_SEQ_LM&quot;</span></span>
<span class="line"><span style="color:#e1e4e8;">peft_config = PeftConfig.from_pretrained(peft_checkpoint)</span></span>
<span class="line"><span style="color:#e1e4e8;">base_model = AutoModelForSeq2SeqLM.from_pretrained(peft_config.base_model_name_or_path).cuda()</span></span>
<span class="line"><span style="color:#e1e4e8;">model = PeftModel.from_pretrained(base_model, peft_checkpoint)</span></span>
<span class="line"><span style="color:#e1e4e8;">tokenizer = AutoTokenizer.from_pretrained(peft_config.base_model_name_or_path)</span></span>
<span class="line"><span style="color:#e1e4e8;">model.eval()</span></span>
<span class="line"><span style="color:#e1e4e8;"></span></span>
<span class="line"><span style="color:#e1e4e8;">while True:</span></span>
<span class="line"><span style="color:#e1e4e8;">    try:</span></span>
<span class="line"><span style="color:#e1e4e8;">        query = input(&quot;\nInput: &quot;)</span></span>
<span class="line"><span style="color:#e1e4e8;">    except Exception:</span></span>
<span class="line"><span style="color:#e1e4e8;">        raise</span></span>
<span class="line"><span style="color:#e1e4e8;">    response, history = model.chat(tokenizer=tokenizer, query=query)</span></span>
<span class="line"><span style="color:#e1e4e8;">    print(&quot;Output: &quot; + response)</span></span></code></pre><pre class="shiki github-light vp-code-light"><code><span class="line"><span style="color:#24292e;">from transformers import AutoModelForSeq2SeqLM</span></span>
<span class="line"><span style="color:#24292e;">from peft import PeftModel, PeftConfig</span></span>
<span class="line"><span style="color:#24292e;"></span></span>
<span class="line"><span style="color:#24292e;"></span></span>
<span class="line"><span style="color:#24292e;">peft_checkpoint = &quot;smangrul/twitter_complaints_bigscience_T0_3B_LORA_SEQ_2_SEQ_LM&quot;</span></span>
<span class="line"><span style="color:#24292e;">peft_config = PeftConfig.from_pretrained(peft_checkpoint)</span></span>
<span class="line"><span style="color:#24292e;">base_model = AutoModelForSeq2SeqLM.from_pretrained(peft_config.base_model_name_or_path).cuda()</span></span>
<span class="line"><span style="color:#24292e;">model = PeftModel.from_pretrained(base_model, peft_checkpoint)</span></span>
<span class="line"><span style="color:#24292e;">tokenizer = AutoTokenizer.from_pretrained(peft_config.base_model_name_or_path)</span></span>
<span class="line"><span style="color:#24292e;">model.eval()</span></span>
<span class="line"><span style="color:#24292e;"></span></span>
<span class="line"><span style="color:#24292e;">while True:</span></span>
<span class="line"><span style="color:#24292e;">    try:</span></span>
<span class="line"><span style="color:#24292e;">        query = input(&quot;\nInput: &quot;)</span></span>
<span class="line"><span style="color:#24292e;">    except Exception:</span></span>
<span class="line"><span style="color:#24292e;">        raise</span></span>
<span class="line"><span style="color:#24292e;">    response, history = model.chat(tokenizer=tokenizer, query=query)</span></span>
<span class="line"><span style="color:#24292e;">    print(&quot;Output: &quot; + response)</span></span></code></pre></div><p>也支持一个方法完成加载</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki github-dark vp-code-dark"><code><span class="line"><span style="color:#e1e4e8;">from peft import AutoPeftModelForCausalLM</span></span>
<span class="line"><span style="color:#e1e4e8;">peft_model = AutoPeftModelForCausalLM.from_pretrained(&quot;ybelkada/opt-350m-lora&quot;)</span></span></code></pre><pre class="shiki github-light vp-code-light"><code><span class="line"><span style="color:#24292e;">from peft import AutoPeftModelForCausalLM</span></span>
<span class="line"><span style="color:#24292e;">peft_model = AutoPeftModelForCausalLM.from_pretrained(&quot;ybelkada/opt-350m-lora&quot;)</span></span></code></pre></div><h2 id="模型导出" tabindex="-1">模型导出 <a class="header-anchor" href="#模型导出" aria-label="Permalink to &quot;模型导出&quot;">​</a></h2><h2 id="qlora" tabindex="-1">QLora <a class="header-anchor" href="#qlora" aria-label="Permalink to &quot;QLora&quot;">​</a></h2></div></div></main><footer class="VPDocFooter" data-v-6b87e69f data-v-ef5dee53><!--[--><!--]--><div class="edit-info" data-v-ef5dee53><div class="edit-link" data-v-ef5dee53><a class="VPLink link vp-external-link-icon no-icon edit-link-button" href="https://github.com/mingriyingying/mingriyingying.github.io/blob/master/docs/articles/Blog/02HuggingFace入门.md" target="_blank" rel="noreferrer" data-v-ef5dee53><!--[--><svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24" class="edit-link-icon" aria-label="edit icon" data-v-ef5dee53><path d="M18,23H4c-1.7,0-3-1.3-3-3V6c0-1.7,1.3-3,3-3h7c0.6,0,1,0.4,1,1s-0.4,1-1,1H4C3.4,5,3,5.4,3,6v14c0,0.6,0.4,1,1,1h14c0.6,0,1-0.4,1-1v-7c0-0.6,0.4-1,1-1s1,0.4,1,1v7C21,21.7,19.7,23,18,23z"></path><path d="M8,17c-0.3,0-0.5-0.1-0.7-0.3C7,16.5,6.9,16.1,7,15.8l1-4c0-0.2,0.1-0.3,0.3-0.5l9.5-9.5c1.2-1.2,3.2-1.2,4.4,0c1.2,1.2,1.2,3.2,0,4.4l-9.5,9.5c-0.1,0.1-0.3,0.2-0.5,0.3l-4,1C8.2,17,8.1,17,8,17zM9.9,12.5l-0.5,2.1l2.1-0.5l9.3-9.3c0.4-0.4,0.4-1.1,0-1.6c-0.4-0.4-1.2-0.4-1.6,0l0,0L9.9,12.5z M18.5,2.5L18.5,2.5L18.5,2.5z"></path></svg> Edit this page on GitHub<!--]--></a></div><!----></div><!----></footer><!--[--><!--]--></div></div></div><!--[--><!--]--></div></div><!----><!--[--><!--]--></div></div>
    <script>window.__VP_HASH_MAP__=JSON.parse("{\"articles_algorithm_00数学基础_01线性代数的本质.md\":\"23bedfb0\",\"about_index.md\":\"773a13d2\",\"articles_algorithm_00数学基础_index.md\":\"1f342d0d\",\"articles_algorithm_01python语法_工具_02np.array()和np.mat()区别.md\":\"20f996fb\",\"articles_algorithm_01python语法_工具_index.md\":\"8ad0cc38\",\"articles_algorithm_01python语法_工具_01python3语法.md\":\"6a181f84\",\"articles_algorithm_10白板推导系列_01介绍.md\":\"6ae311a1\",\"articles_algorithm_01python语法_工具_03python常用库.md\":\"8065203a\",\"articles_algorithm_10白板推导系列_03线性回归.md\":\"e23a1c36\",\"articles_algorithm_14llmvision_04instructblip.md\":\"647fe416\",\"articles_algorithm_00数学基础_02概率论.md\":\"9954b3e6\",\"articles_algorithm_10白板推导系列_07指数族分布.md\":\"7e7e9f72\",\"articles_algorithm_10白板推导系列_24approinference.md\":\"813ae68b\",\"articles_algorithm_11强化学习_01深度强化学习-李宏毅.md\":\"a341f12d\",\"articles_algorithm_10白板推导系列_15particlefilter粒子滤波.md\":\"7cb194f0\",\"articles_algorithm_10白板推导系列_18贝叶斯线性回归.md\":\"93509b41\",\"articles_algorithm_10白板推导系列_02高斯分布.md\":\"41ad3dcb\",\"articles_algorithm_13llm_04bert1进化史.md\":\"89a60318\",\"articles_algorithm_13llm_04bert2详解.md\":\"cf590553\",\"articles_algorithm_13llm_06xlnet.md\":\"4ffbcf8e\",\"articles_ops_linux_03重要命令.md\":\"f2bfd1d2\",\"articles_java_65redis_index.md\":\"1a5188c6\",\"articles_vitepress_index.md\":\"eb84204e\",\"articles_java_65vue_01vue3_ts语法.md\":\"0b446e12\",\"articles_java_65vue_index.md\":\"41626696\",\"articles_ops_nginx_index.md\":\"b404ecb4\",\"articles_python_10python操作docx文件.md\":\"a4990092\",\"articles_ops_k8s_index.md\":\"bb4b7c97\",\"articles_windows_14typora.md\":\"5403befa\",\"articles_ops_linux_01linux教程.md\":\"417340bc\",\"articles_ops_docker_index.md\":\"6aeca549\",\"articles_algorithm_14llmvision_04cogvlm.md\":\"74560f3a\",\"articles_ops_linux_index.md\":\"8dd7462a\",\"articles_algorithm_13llm_12gpt3.md\":\"e57f36c0\",\"articles_algorithm_10白板推导系列_04线性分类.md\":\"19875900\",\"articles_java_61elasticsearch_index.md\":\"4386ad02\",\"articles_ops_k8s_k8s教程.md\":\"7510ec7f\",\"articles_java_31java并发编程_index.md\":\"29d2febb\",\"articles_algorithm_13llm_44mistral-moe.md\":\"5d583191\",\"articles_java_30设计模式_index.md\":\"2036be1e\",\"articles_algorithm_13llm_40分词和扩充词表.md\":\"c252de17\",\"articles_algorithm_14llmvideo_index.md\":\"f87b08ec\",\"articles_algorithm_14llmvision_01dall-e.md\":\"15f9b4fc\",\"articles_algorithm_13llm_index.md\":\"e0020df1\",\"articles_java_51springboot_03日志slf4j.md\":\"03ae1162\",\"articles_algorithm_14llmvision_01dall-e2.md\":\"1e0698c4\",\"articles_algorithm_14llmvision_01stablediffusion.md\":\"499a1a3c\",\"articles_algorithm_14llmvision_01stablediffusionxl.md\":\"b7ed9a38\",\"articles_algorithm_14llmvision_01dall-e3.md\":\"081d11e2\",\"articles_java_51springboot_05安全shiro.md\":\"b9311a8e\",\"articles_java_51springboot_06分页pagehelper.md\":\"29b69393\",\"articles_java_51springboot_07任务 之 异步 定时 邮件.md\":\"ee4a3800\",\"articles_java_51springboot_10读文件.md\":\"d839210a\",\"articles_java_51springboot_11日志.md\":\"9b4f3776\",\"articles_java_51springboot_index.md\":\"197072b0\",\"articles_java_51springboot_05安全springsecurity.md\":\"830cd08b\",\"articles_java_51springboot_04缓存redis.md\":\"2503e11d\",\"articles_algorithm_13llm_16opt.md\":\"9d355205\",\"articles_java_60kafka_index.md\":\"c1e239b3\",\"articles_algorithm_13llm_19palm.md\":\"8bc63126\",\"articles_java_51springboot_00springboot.md\":\"9446f56a\",\"articles_algorithm_13llm_17gpt-neox-20b.md\":\"8ac0fa4d\",\"articles_algorithm_13llm_04学习分词技术subword.md\":\"9e10718a\",\"articles_algorithm_13llm_21flan-palm.md\":\"838412c1\",\"articles_java_65vue_02vue3实战.md\":\"7edccd0e\",\"articles_algorithm_13llm_18ul2.md\":\"6c2b9028\",\"articles_algorithm_14llmvision_04minigpt4.md\":\"9143e6b5\",\"articles_algorithm_13llm_05gpt01论文.md\":\"586c5492\",\"articles_java_53arthus_index.md\":\"cff324b3\",\"articles_algorithm_13llm_06mtdnn.md\":\"8e857fc8\",\"articles_algorithm_13llm_22opt-iml.md\":\"9340804b\",\"articles_algorithm_13llm_22bloom.md\":\"00f28f91\",\"articles_algorithm_13llm_05gpt02模型.md\":\"4fbc5006\",\"articles_algorithm_13llm_22mt0.md\":\"da08f63e\",\"articles_java_52springcloud_index.md\":\"f8f5dd71\",\"articles_algorithm_13llm_07roberta.md\":\"863b793e\",\"articles_algorithm_13llm_23gpt4.md\":\"f5856165\",\"articles_algorithm_13llm_09electra.md\":\"296753e6\",\"articles_algorithm_13llm_10t5.md\":\"082bb6dd\",\"articles_ops_linux_02shell脚本.md\":\"39572f23\",\"articles_algorithm_13llm_23llama.md\":\"bf1d4734\",\"articles_algorithm_13llm_39qwen.md\":\"b76aa8fe\",\"articles_algorithm_13llm_08albert.md\":\"26b974a1\",\"articles_java_51springboot_02i18n国际化.md\":\"14b59b8c\",\"articles_algorithm_13llm_22chatgpt.md\":\"630f5044\",\"articles_java_66jenkins_index.md\":\"0874217a\",\"articles_ops_git_index.md\":\"0f8e763f\",\"articles_algorithm_11强化学习_index.md\":\"61535b70\",\"articles_algorithm_13llm_26llama2.md\":\"7bfdfec1\",\"articles_ops_linux_04linux性能监控.md\":\"d53800ef\",\"articles_algorithm_13llm_11t5.md\":\"3dd0f536\",\"articles_algorithm_13llm_11学习attention mask.md\":\"141f3813\",\"articles_algorithm_12机器学习笔记_01概述.md\":\"8303897d\",\"articles_algorithm_13llm_26llama2中的技术.md\":\"7a23239f\",\"articles_algorithm_12机器学习笔记_00算法性能度量.md\":\"7e1b17b0\",\"articles_algorithm_13llm_13flan.md\":\"87f87797\",\"articles_ops_linux_linux开发环境准备.md\":\"b613d8f2\",\"articles_algorithm_13llm_14t0.md\":\"c554e2f2\",\"articles_algorithm_13llm_15lamda.md\":\"9662b3db\",\"articles_algorithm_12机器学习笔记_02梯度消失.md\":\"629ea8c1\",\"articles_algorithm_12机器学习笔记_04gbdt.md\":\"a2f54a46\",\"articles_algorithm_12机器学习笔记_03逻辑回归.md\":\"3ea2e8bf\",\"articles_algorithm_12机器学习笔记_10调参指南.md\":\"8cfbae47\",\"articles_python_11sqlalchemy.md\":\"64e60703\",\"articles_windows_13tvbox.md\":\"6be49e05\",\"articles_algorithm_12机器学习笔记_05adam.md\":\"6fc614d2\",\"articles_ops_k8s_k8s安装文档.md\":\"26fd6f3a\",\"articles_algorithm_13llm_00llm发展2017-2013.md\":\"638c1e7b\",\"articles_algorithm_13llm_00综述.md\":\"908399be\",\"articles_algorithm_12机器学习笔记_11过拟合问题.md\":\"1b068c55\",\"articles_algorithm_13llm_01prtnet.md\":\"6ef4eb8b\",\"articles_windows_16欧拉系统安装gpu驱动.md\":\"166785a3\",\"articles_windows_index.md\":\"1f3bd6a8\",\"articles_algorithm_13llm_01seq2seq和attention.md\":\"39e8f779\",\"index.md\":\"c2511dde\",\"articles_python_index.md\":\"d83303df\",\"articles_python_21fastapi开发请求.md\":\"9b942595\",\"articles_algorithm_13llm_03transformerxl.md\":\"1bdd1c9e\",\"articles_windows_01常用软件.md\":\"c9842742\",\"articles_algorithm_13llm_02word2vec.md\":\"4c41a321\",\"articles_algorithm_13llm_00多模态.md\":\"8f3dfb00\",\"articles_windows_02latex与文献检索.md\":\"7e7d3a86\",\"articles_algorithm_13llm_16instructgpt.md\":\"9cdd551d\",\"articles_windows_10idea.md\":\"9f3f4681\",\"articles_algorithm_14llmvision_04llava.md\":\"ec2c1507\",\"articles_algorithm_14llmvision_04blip-2.md\":\"08855d29\",\"articles_python_20fastapi什么时候用async.md\":\"d7802c92\",\"articles_windows_15miniconda_pycharm_cuda.md\":\"821dea1a\",\"articles_algorithm_13llm_03transformer详解.md\":\"bd4e0b95\",\"articles_algorithm_22模型训练和微调_03chatglm微调学习.md\":\"e385334f\",\"articles_algorithm_13llm_21flan-t5.md\":\"c421e943\",\"articles_algorithm_12机器学习笔记_12bagging和随机森林.md\":\"25cdf836\",\"articles_algorithm_12机器学习笔记_13adaboost.md\":\"e052cc8e\",\"articles_algorithm_14llmvision_04sharegpt4v.md\":\"5963b2f7\",\"articles_algorithm_21模型部署_05推理加速总结.md\":\"ce7680d8\",\"articles_algorithm_14llmvision_index.md\":\"140976d2\",\"articles_algorithm_15emodied_01rt2.md\":\"9f2d6f06\",\"articles_algorithm_15emodied_index.md\":\"0e9b1b7a\",\"articles_algorithm_21模型部署_00调研模型工业化部署.md\":\"5f1fa797\",\"articles_algorithm_10白板推导系列_12mcmc.md\":\"ceebe04f\",\"articles_algorithm_21模型部署_02fastapi.md\":\"77179108\",\"articles_algorithm_21模型部署_02gradio.md\":\"8a053f52\",\"articles_algorithm_21模型部署_04中兴llm加速.md\":\"9f1e4c09\",\"articles_ops_linux_02shell脚本之服务启停脚本.md\":\"a67f47dd\",\"articles_algorithm_10白板推导系列_08概率图模型.md\":\"1a606efa\",\"articles_algorithm_10白板推导系列_11vi变分推断.md\":\"7521e131\",\"articles_algorithm_10白板推导系列_05降维.md\":\"33c05daa\",\"articles_blog_05扩充词表.md\":\"47600ee3\",\"articles_blog_index.md\":\"9250c279\",\"articles_algorithm_22模型训练和微调_10各种包如何用.md\":\"ed1c9aa1\",\"articles_algorithm_23langchain_00基于大语言模型知识问答应用落地实践.md\":\"19517067\",\"articles_algorithm_22模型训练和微调_index.md\":\"6a29de37\",\"articles_blog_04分词方法.md\":\"081c20f8\",\"articles_java_01java语法_10java读大数据量csv.md\":\"30452cef\",\"articles_algorithm_12机器学习笔记_15xgboost.md\":\"1926e6b4\",\"articles_algorithm_12机器学习笔记_14lightgbm.md\":\"f59e6c64\",\"articles_algorithm_23langchain_01langchain入门.md\":\"f19d9991\",\"articles_algorithm_24agent_index.md\":\"429ca301\",\"articles_blog_01指令.md\":\"3e35021a\",\"articles_algorithm_23langchain_02工具总结.md\":\"a1b146b5\",\"articles_algorithm_23langchain_03langchain检索.md\":\"74a116d1\",\"articles_algorithm_99deeplab_01提取语音.md\":\"fcda4e9a\",\"articles_algorithm_23langchain_index.md\":\"38e8d089\",\"articles_algorithm_99deeplab_index.md\":\"bcfe7ef6\",\"articles_algorithm_24agent_01babyagi.md\":\"31250d7f\",\"articles_blog_02huggingface入门.md\":\"f84c50bc\",\"articles_blog_03健身.md\":\"e45e3fe0\",\"articles_algorithm_22模型训练和微调_04deepseepd rlhf.md\":\"78fab5b2\",\"articles_algorithm_22模型训练和微调_11扩充词表.md\":\"9e319fe0\",\"articles_algorithm_12机器学习笔记_17catboost.md\":\"22bab25d\",\"articles_algorithm_12机器学习笔记_16lightgbm.md\":\"b9ec8433\",\"articles_algorithm_13llm_00ai前期发展2016.md\":\"26249a16\",\"articles_algorithm_12机器学习笔记_index.md\":\"f281a514\",\"articles_algorithm_10白板推导系列_20rbm.md\":\"1ee68cc8\",\"articles_algorithm_10白板推导系列_17高斯网络.md\":\"b629dbd5\",\"articles_java_03javaweb基础tomcat servlet jsp_index.md\":\"a423dc3e\",\"articles_java_02html_css_js_xml_index.md\":\"94de74a0\",\"articles_algorithm_21模型部署_12qwenllm.md\":\"e5e76ed7\",\"articles_algorithm_21模型部署_index.md\":\"945f856d\",\"articles_algorithm_22模型训练和微调_00项目学习.md\":\"a303eabc\",\"articles_java_22janusgraph_index.md\":\"82fee626\",\"articles_algorithm_11强化学习_01强化学习环境.md\":\"c31ab779\",\"articles_algorithm_10白板推导系列_index.md\":\"7e03b24d\",\"articles_algorithm_13llm_25stanford alpaca.md\":\"10edecc5\",\"articles_algorithm_22模型训练和微调_02模型微调.md\":\"a343f965\",\"articles_algorithm_13llm_25vicuna.md\":\"fc6ee813\",\"articles_java_20mysql_index.md\":\"e283be17\",\"articles_java_51springboot_01freemarker模板.md\":\"35b979d5\",\"articles_algorithm_10白板推导系列_22nn.md\":\"22f28813\",\"articles_algorithm_14llmvision_04qwenvl.md\":\"25bb58fb\",\"articles_java_05maven_index.md\":\"0fc81f60\",\"articles_java_01java语法_11java线程池的使用.md\":\"e5940b78\",\"articles_java_01java语法_12java正则化.md\":\"d96fb728\",\"articles_java_01java语法_index.md\":\"7829dfc3\",\"articles_algorithm_10白板推导系列_14lds线性动态系统.md\":\"fb943bcc\",\"articles_java_01java语法_10web前后端大文件上传和接收.md\":\"c9393c1d\",\"articles_java_01java语法_10java读大数据量excel.md\":\"1a64b300\",\"articles_algorithm_10白板推导系列_10gmm.md\":\"16b4ef85\",\"articles_algorithm_22模型训练和微调_01并行训练.md\":\"1816afc0\",\"articles_algorithm_10白板推导系列_06支持向量机.md\":\"f6f8da89\",\"articles_java_01java语法_02java8实战.md\":\"466ba27a\",\"articles_algorithm_12机器学习笔记_14adboost.md\":\"8df17e03\",\"articles_algorithm_10白板推导系列_21spectral谱聚类.md\":\"06fd562a\",\"articles_algorithm_10白板推导系列_19高斯过程回归.md\":\"d1425ccf\",\"articles_algorithm_10白板推导系列_16crf.md\":\"606f5a36\",\"articles_algorithm_10白板推导系列_23partitionfunction.md\":\"5076e96d\",\"articles_algorithm_10白板推导系列_09em.md\":\"21c1515b\",\"articles_algorithm_10白板推导系列_13hmm.md\":\"61763bcd\"}");window.__VP_SITE_DATA__=JSON.parse("{\"lang\":\"en-US\",\"dir\":\"ltr\",\"title\":\"明日盈盈\",\"titleTemplate\":\"Make each day count, Make learning a habit.\",\"description\":\"一只程序猿\",\"base\":\"/\",\"head\":[],\"appearance\":true,\"themeConfig\":{\"siteTitle\":\"Home\",\"nav\":[{\"text\":\"Algorithm\",\"items\":[{\"text\":\"00数学基础\",\"link\":\"/articles/Algorithm/00数学基础/\"},{\"text\":\"01python语法&工具\",\"link\":\"/articles/Algorithm/01python语法&工具/\"},{\"text\":\"10白板推导系列\",\"link\":\"/articles/Algorithm/10白板推导系列/\"},{\"text\":\"11强化学习\",\"link\":\"/articles/Algorithm/11强化学习/\"},{\"text\":\"12机器学习笔记\",\"link\":\"/articles/Algorithm/12机器学习笔记/\"},{\"text\":\"13LLM\",\"link\":\"/articles/Algorithm/13LLM/\"},{\"text\":\"14LLMVideo\",\"link\":\"/articles/Algorithm/14LLMVideo/\"},{\"text\":\"14LLMVision\",\"link\":\"/articles/Algorithm/14LLMVision/\"},{\"text\":\"15Emodied\",\"link\":\"/articles/Algorithm/15Emodied/\"},{\"text\":\"21模型部署\",\"link\":\"/articles/Algorithm/21模型部署/\"},{\"text\":\"22模型训练和微调\",\"link\":\"/articles/Algorithm/22模型训练和微调/\"},{\"text\":\"23LangChain\",\"link\":\"/articles/Algorithm/23LangChain/\"},{\"text\":\"24Agent\",\"link\":\"/articles/Algorithm/24Agent/\"},{\"text\":\"99deeplab\",\"link\":\"/articles/Algorithm/99deeplab/\"}]},{\"text\":\"Blog\",\"link\":\"/articles/Blog/\"},{\"text\":\"Java\",\"items\":[{\"text\":\"01Java语法\",\"link\":\"/articles/Java/01Java语法/\"},{\"text\":\"02HTML+CSS+JS+XML\",\"link\":\"/articles/Java/02HTML+CSS+JS+XML/\"},{\"text\":\"03JavaWeb基础tomcat servlet jsp\",\"link\":\"/articles/Java/03JavaWeb基础tomcat servlet jsp/\"},{\"text\":\"05Maven\",\"link\":\"/articles/Java/05Maven/\"},{\"text\":\"20mysql\",\"link\":\"/articles/Java/20mysql/\"},{\"text\":\"22JanusGraph\",\"link\":\"/articles/Java/22JanusGraph/\"},{\"text\":\"30设计模式\",\"link\":\"/articles/Java/30设计模式/\"},{\"text\":\"31Java并发编程\",\"link\":\"/articles/Java/31Java并发编程/\"},{\"text\":\"51SpringBoot\",\"link\":\"/articles/Java/51SpringBoot/\"},{\"text\":\"52SpringCloud\",\"link\":\"/articles/Java/52SpringCloud/\"},{\"text\":\"53Arthus\",\"link\":\"/articles/Java/53Arthus/\"},{\"text\":\"60kafka\",\"link\":\"/articles/Java/60kafka/\"},{\"text\":\"61ElasticSearch\",\"link\":\"/articles/Java/61ElasticSearch/\"},{\"text\":\"65redis\",\"link\":\"/articles/Java/65redis/\"},{\"text\":\"65vue\",\"link\":\"/articles/Java/65vue/\"},{\"text\":\"66jenkins\",\"link\":\"/articles/Java/66jenkins/\"}]},{\"text\":\"Ops\",\"items\":[{\"text\":\"Docker\",\"link\":\"/articles/Ops/Docker/\"},{\"text\":\"Git\",\"link\":\"/articles/Ops/Git/\"},{\"text\":\"Linux\",\"link\":\"/articles/Ops/Linux/\"},{\"text\":\"Nginx\",\"link\":\"/articles/Ops/Nginx/\"},{\"text\":\"k8s\",\"link\":\"/articles/Ops/k8s/\"}]},{\"text\":\"Python\",\"link\":\"/articles/Python/\"},{\"text\":\"VitePress\",\"link\":\"/articles/VitePress/\"},{\"text\":\"windows\",\"link\":\"/articles/windows/\"}],\"sidebar\":{\"/articles/Algorithm/00数学基础\":[{\"text\":\"00数学基础\",\"items\":[{\"text\":\"01线性代数的本质\",\"link\":\"/articles/Algorithm/00数学基础/01线性代数的本质.md\"},{\"text\":\"02概率论\",\"link\":\"/articles/Algorithm/00数学基础/02概率论.md\"}]}],\"/articles/Algorithm/01python语法&工具\":[{\"text\":\"01python语法&工具\",\"items\":[{\"text\":\"01python3语法\",\"link\":\"/articles/Algorithm/01python语法&工具/01python3语法.md\"},{\"text\":\"02np.array()和np.mat()区别\",\"link\":\"/articles/Algorithm/01python语法&工具/02np.array()和np.mat()区别.md\"},{\"text\":\"03python常用库\",\"link\":\"/articles/Algorithm/01python语法&工具/03python常用库.md\"}]}],\"/articles/Algorithm/10白板推导系列\":[{\"text\":\"10白板推导系列\",\"items\":[{\"text\":\"01介绍\",\"link\":\"/articles/Algorithm/10白板推导系列/01介绍.md\"},{\"text\":\"02高斯分布\",\"link\":\"/articles/Algorithm/10白板推导系列/02高斯分布.md\"},{\"text\":\"03线性回归\",\"link\":\"/articles/Algorithm/10白板推导系列/03线性回归.md\"},{\"text\":\"04线性分类\",\"link\":\"/articles/Algorithm/10白板推导系列/04线性分类.md\"},{\"text\":\"05降维\",\"link\":\"/articles/Algorithm/10白板推导系列/05降维.md\"},{\"text\":\"06支持向量机\",\"link\":\"/articles/Algorithm/10白板推导系列/06支持向量机.md\"},{\"text\":\"07指数族分布\",\"link\":\"/articles/Algorithm/10白板推导系列/07指数族分布.md\"},{\"text\":\"08概率图模型\",\"link\":\"/articles/Algorithm/10白板推导系列/08概率图模型.md\"},{\"text\":\"09EM\",\"link\":\"/articles/Algorithm/10白板推导系列/09EM.md\"},{\"text\":\"10GMM\",\"link\":\"/articles/Algorithm/10白板推导系列/10GMM.md\"},{\"text\":\"11VI变分推断\",\"link\":\"/articles/Algorithm/10白板推导系列/11VI变分推断.md\"},{\"text\":\"12MCMC\",\"link\":\"/articles/Algorithm/10白板推导系列/12MCMC.md\"},{\"text\":\"13HMM\",\"link\":\"/articles/Algorithm/10白板推导系列/13HMM.md\"},{\"text\":\"14LDS线性动态系统\",\"link\":\"/articles/Algorithm/10白板推导系列/14LDS线性动态系统.md\"},{\"text\":\"15particleFilter粒子滤波\",\"link\":\"/articles/Algorithm/10白板推导系列/15particleFilter粒子滤波.md\"},{\"text\":\"16CRF\",\"link\":\"/articles/Algorithm/10白板推导系列/16CRF.md\"},{\"text\":\"17高斯网络\",\"link\":\"/articles/Algorithm/10白板推导系列/17高斯网络.md\"},{\"text\":\"18贝叶斯线性回归\",\"link\":\"/articles/Algorithm/10白板推导系列/18贝叶斯线性回归.md\"},{\"text\":\"19高斯过程回归\",\"link\":\"/articles/Algorithm/10白板推导系列/19高斯过程回归.md\"},{\"text\":\"20RBM\",\"link\":\"/articles/Algorithm/10白板推导系列/20RBM.md\"},{\"text\":\"21Spectral谱聚类\",\"link\":\"/articles/Algorithm/10白板推导系列/21Spectral谱聚类.md\"},{\"text\":\"22NN\",\"link\":\"/articles/Algorithm/10白板推导系列/22NN.md\"},{\"text\":\"23PartitionFunction\",\"link\":\"/articles/Algorithm/10白板推导系列/23PartitionFunction.md\"},{\"text\":\"24ApproInference\",\"link\":\"/articles/Algorithm/10白板推导系列/24ApproInference.md\"}]}],\"/articles/Algorithm/11强化学习\":[{\"text\":\"11强化学习\",\"items\":[{\"text\":\"01强化学习环境\",\"link\":\"/articles/Algorithm/11强化学习/01强化学习环境.md\"},{\"text\":\"01深度强化学习-李宏毅\",\"link\":\"/articles/Algorithm/11强化学习/01深度强化学习-李宏毅.md\"}]}],\"/articles/Algorithm/12机器学习笔记\":[{\"text\":\"12机器学习笔记\",\"items\":[{\"text\":\"00算法性能度量\",\"link\":\"/articles/Algorithm/12机器学习笔记/00算法性能度量.md\"},{\"text\":\"01概述\",\"link\":\"/articles/Algorithm/12机器学习笔记/01概述.md\"},{\"text\":\"02梯度消失\",\"link\":\"/articles/Algorithm/12机器学习笔记/02梯度消失.md\"},{\"text\":\"03逻辑回归\",\"link\":\"/articles/Algorithm/12机器学习笔记/03逻辑回归.md\"},{\"text\":\"04GBDT\",\"link\":\"/articles/Algorithm/12机器学习笔记/04GBDT.md\"},{\"text\":\"05Adam\",\"link\":\"/articles/Algorithm/12机器学习笔记/05Adam.md\"},{\"text\":\"10调参指南\",\"link\":\"/articles/Algorithm/12机器学习笔记/10调参指南.md\"},{\"text\":\"11过拟合问题\",\"link\":\"/articles/Algorithm/12机器学习笔记/11过拟合问题.md\"},{\"text\":\"12bagging和随机森林\",\"link\":\"/articles/Algorithm/12机器学习笔记/12bagging和随机森林.md\"},{\"text\":\"13Adaboost\",\"link\":\"/articles/Algorithm/12机器学习笔记/13Adaboost.md\"},{\"text\":\"14Adboost\",\"link\":\"/articles/Algorithm/12机器学习笔记/14Adboost.md\"},{\"text\":\"14LightGBM\",\"link\":\"/articles/Algorithm/12机器学习笔记/14LightGBM.md\"},{\"text\":\"15XGBoost\",\"link\":\"/articles/Algorithm/12机器学习笔记/15XGBoost.md\"},{\"text\":\"16Lightgbm\",\"link\":\"/articles/Algorithm/12机器学习笔记/16Lightgbm.md\"},{\"text\":\"17catboost\",\"link\":\"/articles/Algorithm/12机器学习笔记/17catboost.md\"}]}],\"/articles/Algorithm/13LLM\":[{\"text\":\"13LLM\",\"items\":[{\"text\":\"00AI前期发展2016\",\"link\":\"/articles/Algorithm/13LLM/00AI前期发展2016.md\"},{\"text\":\"00LLM发展2017-2013\",\"link\":\"/articles/Algorithm/13LLM/00LLM发展2017-2013.md\"},{\"text\":\"00多模态\",\"link\":\"/articles/Algorithm/13LLM/00多模态.md\"},{\"text\":\"00综述\",\"link\":\"/articles/Algorithm/13LLM/00综述.md\"},{\"text\":\"01PrtNet\",\"link\":\"/articles/Algorithm/13LLM/01PrtNet.md\"},{\"text\":\"01seq2seq和Attention\",\"link\":\"/articles/Algorithm/13LLM/01seq2seq和Attention.md\"},{\"text\":\"02word2Vec\",\"link\":\"/articles/Algorithm/13LLM/02word2Vec.md\"},{\"text\":\"03TransformerXL\",\"link\":\"/articles/Algorithm/13LLM/03TransformerXL.md\"},{\"text\":\"03Transformer详解\",\"link\":\"/articles/Algorithm/13LLM/03Transformer详解.md\"},{\"text\":\"04bert1进化史\",\"link\":\"/articles/Algorithm/13LLM/04bert1进化史.md\"},{\"text\":\"04bert2详解\",\"link\":\"/articles/Algorithm/13LLM/04bert2详解.md\"},{\"text\":\"04学习分词技术Subword\",\"link\":\"/articles/Algorithm/13LLM/04学习分词技术Subword.md\"},{\"text\":\"05GPT01论文\",\"link\":\"/articles/Algorithm/13LLM/05GPT01论文.md\"},{\"text\":\"05GPT02模型\",\"link\":\"/articles/Algorithm/13LLM/05GPT02模型.md\"},{\"text\":\"06MTDNN\",\"link\":\"/articles/Algorithm/13LLM/06MTDNN.md\"},{\"text\":\"06XLNet\",\"link\":\"/articles/Algorithm/13LLM/06XLNet.md\"},{\"text\":\"07RoBERTa\",\"link\":\"/articles/Algorithm/13LLM/07RoBERTa.md\"},{\"text\":\"08ALBERT\",\"link\":\"/articles/Algorithm/13LLM/08ALBERT.md\"},{\"text\":\"09ELECTRA\",\"link\":\"/articles/Algorithm/13LLM/09ELECTRA.md\"},{\"text\":\"10T5\",\"link\":\"/articles/Algorithm/13LLM/10T5.md\"},{\"text\":\"11T5\",\"link\":\"/articles/Algorithm/13LLM/11T5.md\"},{\"text\":\"11学习attention mask\",\"link\":\"/articles/Algorithm/13LLM/11学习attention mask.md\"},{\"text\":\"12GPT3\",\"link\":\"/articles/Algorithm/13LLM/12GPT3.md\"},{\"text\":\"13FLAN\",\"link\":\"/articles/Algorithm/13LLM/13FLAN.md\"},{\"text\":\"14T0\",\"link\":\"/articles/Algorithm/13LLM/14T0.md\"},{\"text\":\"15LaMDA\",\"link\":\"/articles/Algorithm/13LLM/15LaMDA.md\"},{\"text\":\"16InstructGPT\",\"link\":\"/articles/Algorithm/13LLM/16InstructGPT.md\"},{\"text\":\"16OPT\",\"link\":\"/articles/Algorithm/13LLM/16OPT.md\"},{\"text\":\"17GPT-NeoX-20B\",\"link\":\"/articles/Algorithm/13LLM/17GPT-NeoX-20B.md\"},{\"text\":\"18UL2\",\"link\":\"/articles/Algorithm/13LLM/18UL2.md\"},{\"text\":\"19PaLM\",\"link\":\"/articles/Algorithm/13LLM/19PaLM.md\"},{\"text\":\"21Flan-PaLM\",\"link\":\"/articles/Algorithm/13LLM/21Flan-PaLM.md\"},{\"text\":\"21Flan-T5\",\"link\":\"/articles/Algorithm/13LLM/21Flan-T5.md\"},{\"text\":\"22BLOOM\",\"link\":\"/articles/Algorithm/13LLM/22BLOOM.md\"},{\"text\":\"22ChatGPT\",\"link\":\"/articles/Algorithm/13LLM/22ChatGPT.md\"},{\"text\":\"22OPT-IML\",\"link\":\"/articles/Algorithm/13LLM/22OPT-IML.md\"},{\"text\":\"22mT0\",\"link\":\"/articles/Algorithm/13LLM/22mT0.md\"},{\"text\":\"23GPT4\",\"link\":\"/articles/Algorithm/13LLM/23GPT4.md\"},{\"text\":\"23LLaMA\",\"link\":\"/articles/Algorithm/13LLM/23LLaMA.md\"},{\"text\":\"25Stanford Alpaca\",\"link\":\"/articles/Algorithm/13LLM/25Stanford Alpaca.md\"},{\"text\":\"25Vicuna\",\"link\":\"/articles/Algorithm/13LLM/25Vicuna.md\"},{\"text\":\"26LLaMA2\",\"link\":\"/articles/Algorithm/13LLM/26LLaMA2.md\"},{\"text\":\"26LLaMA2中的技术\",\"link\":\"/articles/Algorithm/13LLM/26LLaMA2中的技术.md\"},{\"text\":\"39Qwen\",\"link\":\"/articles/Algorithm/13LLM/39Qwen.md\"},{\"text\":\"40分词和扩充词表\",\"link\":\"/articles/Algorithm/13LLM/40分词和扩充词表.md\"},{\"text\":\"44Mistral-MoE\",\"link\":\"/articles/Algorithm/13LLM/44Mistral-MoE.md\"}]}],\"/articles/Algorithm/14LLMVision\":[{\"text\":\"14LLMVision\",\"items\":[{\"text\":\"01DALL-E\",\"link\":\"/articles/Algorithm/14LLMVision/01DALL-E.md\"},{\"text\":\"01DALL-E2\",\"link\":\"/articles/Algorithm/14LLMVision/01DALL-E2.md\"},{\"text\":\"01DALL-E3\",\"link\":\"/articles/Algorithm/14LLMVision/01DALL-E3.md\"},{\"text\":\"01StableDiffusion\",\"link\":\"/articles/Algorithm/14LLMVision/01StableDiffusion.md\"},{\"text\":\"01StableDiffusionXL\",\"link\":\"/articles/Algorithm/14LLMVision/01StableDiffusionXL.md\"},{\"text\":\"04BLIP-2\",\"link\":\"/articles/Algorithm/14LLMVision/04BLIP-2.md\"},{\"text\":\"04CogVLM\",\"link\":\"/articles/Algorithm/14LLMVision/04CogVLM.md\"},{\"text\":\"04InstructBLIP\",\"link\":\"/articles/Algorithm/14LLMVision/04InstructBLIP.md\"},{\"text\":\"04LLaVA\",\"link\":\"/articles/Algorithm/14LLMVision/04LLaVA.md\"},{\"text\":\"04MiniGPT4\",\"link\":\"/articles/Algorithm/14LLMVision/04MiniGPT4.md\"},{\"text\":\"04QwenVL\",\"link\":\"/articles/Algorithm/14LLMVision/04QwenVL.md\"},{\"text\":\"04ShareGPT4V\",\"link\":\"/articles/Algorithm/14LLMVision/04ShareGPT4V.md\"}]}],\"/articles/Algorithm/15Emodied\":[{\"text\":\"15Emodied\",\"items\":[{\"text\":\"01RT2\",\"link\":\"/articles/Algorithm/15Emodied/01RT2.md\"}]}],\"/articles/Algorithm/21模型部署\":[{\"text\":\"21模型部署\",\"items\":[{\"text\":\"00调研模型工业化部署\",\"link\":\"/articles/Algorithm/21模型部署/00调研模型工业化部署.md\"},{\"text\":\"02FastApi\",\"link\":\"/articles/Algorithm/21模型部署/02FastApi.md\"},{\"text\":\"02Gradio\",\"link\":\"/articles/Algorithm/21模型部署/02Gradio.md\"},{\"text\":\"04中兴LLM加速\",\"link\":\"/articles/Algorithm/21模型部署/04中兴LLM加速.md\"},{\"text\":\"05推理加速总结\",\"link\":\"/articles/Algorithm/21模型部署/05推理加速总结.md\"},{\"text\":\"12QwenLLM\",\"link\":\"/articles/Algorithm/21模型部署/12QwenLLM.md\"}]}],\"/articles/Algorithm/22模型训练和微调\":[{\"text\":\"22模型训练和微调\",\"items\":[{\"text\":\"00项目学习\",\"link\":\"/articles/Algorithm/22模型训练和微调/00项目学习.md\"},{\"text\":\"01并行训练\",\"link\":\"/articles/Algorithm/22模型训练和微调/01并行训练.md\"},{\"text\":\"02模型微调\",\"link\":\"/articles/Algorithm/22模型训练和微调/02模型微调.md\"},{\"text\":\"03ChatGLM微调学习\",\"link\":\"/articles/Algorithm/22模型训练和微调/03ChatGLM微调学习.md\"},{\"text\":\"04DeepSeepd RLHF\",\"link\":\"/articles/Algorithm/22模型训练和微调/04DeepSeepd RLHF.md\"},{\"text\":\"10各种包如何用\",\"link\":\"/articles/Algorithm/22模型训练和微调/10各种包如何用.md\"},{\"text\":\"11扩充词表\",\"link\":\"/articles/Algorithm/22模型训练和微调/11扩充词表.md\"}]}],\"/articles/Algorithm/23LangChain\":[{\"text\":\"23LangChain\",\"items\":[{\"text\":\"00基于大语言模型知识问答应用落地实践\",\"link\":\"/articles/Algorithm/23LangChain/00基于大语言模型知识问答应用落地实践.md\"},{\"text\":\"01LangChain入门\",\"link\":\"/articles/Algorithm/23LangChain/01LangChain入门.md\"},{\"text\":\"02工具总结\",\"link\":\"/articles/Algorithm/23LangChain/02工具总结.md\"},{\"text\":\"03LangChain检索\",\"link\":\"/articles/Algorithm/23LangChain/03LangChain检索.md\"}]}],\"/articles/Algorithm/24Agent\":[{\"text\":\"24Agent\",\"items\":[{\"text\":\"01babyAGI\",\"link\":\"/articles/Algorithm/24Agent/01babyAGI.md\"}]}],\"/articles/Algorithm/99deeplab\":[{\"text\":\"99deeplab\",\"items\":[{\"text\":\"01提取语音\",\"link\":\"/articles/Algorithm/99deeplab/01提取语音.md\"}]}],\"/articles/Blog\":[{\"text\":\"Blog\",\"items\":[{\"text\":\"01指令\",\"link\":\"/articles/Blog/01指令.md\"},{\"text\":\"02HuggingFace入门\",\"link\":\"/articles/Blog/02HuggingFace入门.md\"},{\"text\":\"03健身\",\"link\":\"/articles/Blog/03健身.md\"},{\"text\":\"04分词方法\",\"link\":\"/articles/Blog/04分词方法.md\"},{\"text\":\"05扩充词表\",\"link\":\"/articles/Blog/05扩充词表.md\"}]}],\"/articles/Java/01Java语法\":[{\"text\":\"01Java语法\",\"items\":[{\"text\":\"02Java8实战\",\"link\":\"/articles/Java/01Java语法/02Java8实战.md\"},{\"text\":\"10Java读大数据量csv\",\"link\":\"/articles/Java/01Java语法/10Java读大数据量csv.md\"},{\"text\":\"10Java读大数据量excel\",\"link\":\"/articles/Java/01Java语法/10Java读大数据量excel.md\"},{\"text\":\"10Web前后端大文件上传和接收\",\"link\":\"/articles/Java/01Java语法/10Web前后端大文件上传和接收.md\"},{\"text\":\"11Java线程池的使用\",\"link\":\"/articles/Java/01Java语法/11Java线程池的使用.md\"},{\"text\":\"12Java正则化\",\"link\":\"/articles/Java/01Java语法/12Java正则化.md\"}]}],\"/articles/Java/51SpringBoot\":[{\"text\":\"51SpringBoot\",\"items\":[{\"text\":\"00SpringBoot\",\"link\":\"/articles/Java/51SpringBoot/00SpringBoot.md\"},{\"text\":\"01Freemarker模板\",\"link\":\"/articles/Java/51SpringBoot/01Freemarker模板.md\"},{\"text\":\"02i18n国际化\",\"link\":\"/articles/Java/51SpringBoot/02i18n国际化.md\"},{\"text\":\"03日志SLF4J\",\"link\":\"/articles/Java/51SpringBoot/03日志SLF4J.md\"},{\"text\":\"04缓存Redis\",\"link\":\"/articles/Java/51SpringBoot/04缓存Redis.md\"},{\"text\":\"05安全Shiro\",\"link\":\"/articles/Java/51SpringBoot/05安全Shiro.md\"},{\"text\":\"05安全SpringSecurity\",\"link\":\"/articles/Java/51SpringBoot/05安全SpringSecurity.md\"},{\"text\":\"06分页PageHelper\",\"link\":\"/articles/Java/51SpringBoot/06分页PageHelper.md\"},{\"text\":\"07任务 之 异步 定时 邮件\",\"link\":\"/articles/Java/51SpringBoot/07任务 之 异步 定时 邮件.md\"},{\"text\":\"10读文件\",\"link\":\"/articles/Java/51SpringBoot/10读文件.md\"},{\"text\":\"11日志\",\"link\":\"/articles/Java/51SpringBoot/11日志.md\"}]}],\"/articles/Java/65vue\":[{\"text\":\"65vue\",\"items\":[{\"text\":\"01vue3+ts语法\",\"link\":\"/articles/Java/65vue/01vue3+ts语法.md\"},{\"text\":\"02vue3实战\",\"link\":\"/articles/Java/65vue/02vue3实战.md\"}]}],\"/articles/Ops/Linux\":[{\"text\":\"Linux\",\"items\":[{\"text\":\"01linux教程\",\"link\":\"/articles/Ops/Linux/01linux教程.md\"},{\"text\":\"02Shell脚本\",\"link\":\"/articles/Ops/Linux/02Shell脚本.md\"},{\"text\":\"02Shell脚本之服务启停脚本\",\"link\":\"/articles/Ops/Linux/02Shell脚本之服务启停脚本.md\"},{\"text\":\"03重要命令\",\"link\":\"/articles/Ops/Linux/03重要命令.md\"},{\"text\":\"04Linux性能监控\",\"link\":\"/articles/Ops/Linux/04Linux性能监控.md\"},{\"text\":\"linux开发环境准备\",\"link\":\"/articles/Ops/Linux/linux开发环境准备.md\"}]}],\"/articles/Ops/k8s\":[{\"text\":\"k8s\",\"items\":[{\"text\":\"k8s安装文档\",\"link\":\"/articles/Ops/k8s/k8s安装文档.md\"},{\"text\":\"k8s教程\",\"link\":\"/articles/Ops/k8s/k8s教程.md\"}]}],\"/articles/Python\":[{\"text\":\"Python\",\"items\":[{\"text\":\"10python操作docx文件\",\"link\":\"/articles/Python/10python操作docx文件.md\"},{\"text\":\"11SQLAlchemy\",\"link\":\"/articles/Python/11SQLAlchemy.md\"},{\"text\":\"20FastAPI什么时候用async\",\"link\":\"/articles/Python/20FastAPI什么时候用async.md\"},{\"text\":\"21FastAPI开发请求\",\"link\":\"/articles/Python/21FastAPI开发请求.md\"}]}],\"/articles/windows\":[{\"text\":\"windows\",\"items\":[{\"text\":\"01常用软件\",\"link\":\"/articles/windows/01常用软件.md\"},{\"text\":\"02Latex与文献检索\",\"link\":\"/articles/windows/02Latex与文献检索.md\"},{\"text\":\"10IDEA\",\"link\":\"/articles/windows/10IDEA.md\"},{\"text\":\"13tvbox\",\"link\":\"/articles/windows/13tvbox.md\"},{\"text\":\"14Typora\",\"link\":\"/articles/windows/14Typora.md\"},{\"text\":\"15miniconda+pycharm+CUDA\",\"link\":\"/articles/windows/15miniconda+pycharm+CUDA.md\"},{\"text\":\"16欧拉系统安装GPU驱动\",\"link\":\"/articles/windows/16欧拉系统安装GPU驱动.md\"}]}]},\"outline\":{\"level\":[1,3]},\"editLink\":{\"pattern\":\"https://github.com/mingriyingying/mingriyingying.github.io/blob/master/docs/:path\",\"text\":\"Edit this page on GitHub\"},\"lastUpdated\":{\"text\":\"Updated at\",\"formatOptions\":{\"dateStyle\":\"short\",\"timeStyle\":\"medium\"}},\"docFooter\":{\"prev\":false,\"next\":false}},\"locales\":{},\"scrollOffset\":90,\"cleanUrls\":false}");</script>
    
  </body>
</html>