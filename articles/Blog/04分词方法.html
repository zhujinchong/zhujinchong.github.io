<!DOCTYPE html>
<html lang="en-US" dir="ltr">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>引言 | Make each day count, Make learning a habit.</title>
    <meta name="description" content="一只程序猿">
    <link rel="preload stylesheet" href="/assets/style.f33cd555.css" as="style">
    
    <script type="module" src="/assets/app.dc5e2f50.js"></script>
    <link rel="preload" href="/assets/inter-roman-latin.2ed14f66.woff2" as="font" type="font/woff2" crossorigin="">
    <link rel="modulepreload" href="/assets/chunks/framework.2516552c.js">
    <link rel="modulepreload" href="/assets/chunks/theme.2359dc4a.js">
    <link rel="modulepreload" href="/assets/articles_Blog_04分词方法.md.081c20f8.lean.js">
    <link rel="icon" href="/img/home.svg">
    <script id="check-dark-mode">(()=>{const e=localStorage.getItem("vitepress-theme-appearance")||"auto",a=window.matchMedia("(prefers-color-scheme: dark)").matches;(!e||e==="auto"?a:e==="dark")&&document.documentElement.classList.add("dark")})();</script>
    <script id="check-mac-os">document.documentElement.classList.toggle("mac",/Mac|iPhone|iPod|iPad/i.test(navigator.platform));</script>
  </head>
  <body>
    <div id="app"><div class="Layout" data-v-5a346dfe><!--[--><!--]--><!--[--><span tabindex="-1" data-v-0f60ec36></span><a href="#VPContent" class="VPSkipLink visually-hidden" data-v-0f60ec36> Skip to content </a><!--]--><!----><header class="VPNav" data-v-5a346dfe data-v-ae24b3ad><div class="VPNavBar has-sidebar" data-v-ae24b3ad data-v-a0fd61f4><div class="container" data-v-a0fd61f4><div class="title" data-v-a0fd61f4><div class="VPNavBarTitle has-sidebar" data-v-a0fd61f4 data-v-86d1bed8><a class="title" href="/" data-v-86d1bed8><!--[--><!--]--><!----><!--[-->Home<!--]--><!--[--><!--]--></a></div></div><div class="content" data-v-a0fd61f4><div class="curtain" data-v-a0fd61f4></div><div class="content-body" data-v-a0fd61f4><!--[--><!--]--><div class="VPNavBarSearch search" data-v-a0fd61f4><!----></div><nav aria-labelledby="main-nav-aria-label" class="VPNavBarMenu menu" data-v-a0fd61f4 data-v-7f418b0f><span id="main-nav-aria-label" class="visually-hidden" data-v-7f418b0f>Main Navigation</span><!--[--><!--[--><div class="VPFlyout VPNavBarMenuGroup" data-v-7f418b0f data-v-9c007e85><button type="button" class="button" aria-haspopup="true" aria-expanded="false" data-v-9c007e85><span class="text" data-v-9c007e85><!----><span data-v-9c007e85>Algorithm</span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="text-icon" data-v-9c007e85><path d="M12,16c-0.3,0-0.5-0.1-0.7-0.3l-6-6c-0.4-0.4-0.4-1,0-1.4s1-0.4,1.4,0l5.3,5.3l5.3-5.3c0.4-0.4,1-0.4,1.4,0s0.4,1,0,1.4l-6,6C12.5,15.9,12.3,16,12,16z"></path></svg></span></button><div class="menu" data-v-9c007e85><div class="VPMenu" data-v-9c007e85 data-v-e7ea1737><div class="items" data-v-e7ea1737><!--[--><!--[--><div class="VPMenuLink" data-v-e7ea1737 data-v-43f1e123><a class="VPLink link" href="/articles/Algorithm/00数学基础/" data-v-43f1e123><!--[-->00数学基础<!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-e7ea1737 data-v-43f1e123><a class="VPLink link" href="/articles/Algorithm/01python语法&amp;工具/" data-v-43f1e123><!--[-->01python语法&amp;工具<!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-e7ea1737 data-v-43f1e123><a class="VPLink link" href="/articles/Algorithm/10白板推导系列/" data-v-43f1e123><!--[-->10白板推导系列<!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-e7ea1737 data-v-43f1e123><a class="VPLink link" href="/articles/Algorithm/11强化学习/" data-v-43f1e123><!--[-->11强化学习<!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-e7ea1737 data-v-43f1e123><a class="VPLink link" href="/articles/Algorithm/12机器学习笔记/" data-v-43f1e123><!--[-->12机器学习笔记<!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-e7ea1737 data-v-43f1e123><a class="VPLink link" href="/articles/Algorithm/13LLM/" data-v-43f1e123><!--[-->13LLM<!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-e7ea1737 data-v-43f1e123><a class="VPLink link" href="/articles/Algorithm/14LMM/" data-v-43f1e123><!--[-->14LMM<!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-e7ea1737 data-v-43f1e123><a class="VPLink link" href="/articles/Algorithm/16RAG/" data-v-43f1e123><!--[-->16RAG<!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-e7ea1737 data-v-43f1e123><a class="VPLink link" href="/articles/Algorithm/17Agent/" data-v-43f1e123><!--[-->17Agent<!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-e7ea1737 data-v-43f1e123><a class="VPLink link" href="/articles/Algorithm/18Emodied/" data-v-43f1e123><!--[-->18Emodied<!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-e7ea1737 data-v-43f1e123><a class="VPLink link" href="/articles/Algorithm/21模型部署/" data-v-43f1e123><!--[-->21模型部署<!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-e7ea1737 data-v-43f1e123><a class="VPLink link" href="/articles/Algorithm/22模型训练和微调/" data-v-43f1e123><!--[-->22模型训练和微调<!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-e7ea1737 data-v-43f1e123><a class="VPLink link" href="/articles/Algorithm/99deeplab/" data-v-43f1e123><!--[-->99deeplab<!--]--></a></div><!--]--><!--]--></div><!--[--><!--]--></div></div></div><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/articles/Blog/" tabindex="0" data-v-7f418b0f data-v-42ef59de><!--[--><span data-v-42ef59de>Blog</span><!--]--></a><!--]--><!--[--><div class="VPFlyout VPNavBarMenuGroup" data-v-7f418b0f data-v-9c007e85><button type="button" class="button" aria-haspopup="true" aria-expanded="false" data-v-9c007e85><span class="text" data-v-9c007e85><!----><span data-v-9c007e85>Java</span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="text-icon" data-v-9c007e85><path d="M12,16c-0.3,0-0.5-0.1-0.7-0.3l-6-6c-0.4-0.4-0.4-1,0-1.4s1-0.4,1.4,0l5.3,5.3l5.3-5.3c0.4-0.4,1-0.4,1.4,0s0.4,1,0,1.4l-6,6C12.5,15.9,12.3,16,12,16z"></path></svg></span></button><div class="menu" data-v-9c007e85><div class="VPMenu" data-v-9c007e85 data-v-e7ea1737><div class="items" data-v-e7ea1737><!--[--><!--[--><div class="VPMenuLink" data-v-e7ea1737 data-v-43f1e123><a class="VPLink link" href="/articles/Java/01Java语法/" data-v-43f1e123><!--[-->01Java语法<!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-e7ea1737 data-v-43f1e123><a class="VPLink link" href="/articles/Java/02HTML+CSS+JS+XML/" data-v-43f1e123><!--[-->02HTML+CSS+JS+XML<!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-e7ea1737 data-v-43f1e123><a class="VPLink link" href="/articles/Java/03JavaWeb基础tomcat servlet jsp/" data-v-43f1e123><!--[-->03JavaWeb基础tomcat servlet jsp<!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-e7ea1737 data-v-43f1e123><a class="VPLink link" href="/articles/Java/05Maven/" data-v-43f1e123><!--[-->05Maven<!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-e7ea1737 data-v-43f1e123><a class="VPLink link" href="/articles/Java/20mysql/" data-v-43f1e123><!--[-->20mysql<!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-e7ea1737 data-v-43f1e123><a class="VPLink link" href="/articles/Java/22JanusGraph/" data-v-43f1e123><!--[-->22JanusGraph<!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-e7ea1737 data-v-43f1e123><a class="VPLink link" href="/articles/Java/30设计模式/" data-v-43f1e123><!--[-->30设计模式<!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-e7ea1737 data-v-43f1e123><a class="VPLink link" href="/articles/Java/31Java并发编程/" data-v-43f1e123><!--[-->31Java并发编程<!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-e7ea1737 data-v-43f1e123><a class="VPLink link" href="/articles/Java/51SpringBoot/" data-v-43f1e123><!--[-->51SpringBoot<!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-e7ea1737 data-v-43f1e123><a class="VPLink link" href="/articles/Java/52SpringCloud/" data-v-43f1e123><!--[-->52SpringCloud<!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-e7ea1737 data-v-43f1e123><a class="VPLink link" href="/articles/Java/53Arthus/" data-v-43f1e123><!--[-->53Arthus<!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-e7ea1737 data-v-43f1e123><a class="VPLink link" href="/articles/Java/60kafka/" data-v-43f1e123><!--[-->60kafka<!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-e7ea1737 data-v-43f1e123><a class="VPLink link" href="/articles/Java/61ElasticSearch/" data-v-43f1e123><!--[-->61ElasticSearch<!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-e7ea1737 data-v-43f1e123><a class="VPLink link" href="/articles/Java/65redis/" data-v-43f1e123><!--[-->65redis<!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-e7ea1737 data-v-43f1e123><a class="VPLink link" href="/articles/Java/65vue/" data-v-43f1e123><!--[-->65vue<!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-e7ea1737 data-v-43f1e123><a class="VPLink link" href="/articles/Java/66jenkins/" data-v-43f1e123><!--[-->66jenkins<!--]--></a></div><!--]--><!--]--></div><!--[--><!--]--></div></div></div><!--]--><!--[--><div class="VPFlyout VPNavBarMenuGroup" data-v-7f418b0f data-v-9c007e85><button type="button" class="button" aria-haspopup="true" aria-expanded="false" data-v-9c007e85><span class="text" data-v-9c007e85><!----><span data-v-9c007e85>Ops</span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="text-icon" data-v-9c007e85><path d="M12,16c-0.3,0-0.5-0.1-0.7-0.3l-6-6c-0.4-0.4-0.4-1,0-1.4s1-0.4,1.4,0l5.3,5.3l5.3-5.3c0.4-0.4,1-0.4,1.4,0s0.4,1,0,1.4l-6,6C12.5,15.9,12.3,16,12,16z"></path></svg></span></button><div class="menu" data-v-9c007e85><div class="VPMenu" data-v-9c007e85 data-v-e7ea1737><div class="items" data-v-e7ea1737><!--[--><!--[--><div class="VPMenuLink" data-v-e7ea1737 data-v-43f1e123><a class="VPLink link" href="/articles/Ops/Docker/" data-v-43f1e123><!--[-->Docker<!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-e7ea1737 data-v-43f1e123><a class="VPLink link" href="/articles/Ops/Git/" data-v-43f1e123><!--[-->Git<!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-e7ea1737 data-v-43f1e123><a class="VPLink link" href="/articles/Ops/Linux/" data-v-43f1e123><!--[-->Linux<!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-e7ea1737 data-v-43f1e123><a class="VPLink link" href="/articles/Ops/Nginx/" data-v-43f1e123><!--[-->Nginx<!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-e7ea1737 data-v-43f1e123><a class="VPLink link" href="/articles/Ops/k8s/" data-v-43f1e123><!--[-->k8s<!--]--></a></div><!--]--><!--]--></div><!--[--><!--]--></div></div></div><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/articles/Python/" tabindex="0" data-v-7f418b0f data-v-42ef59de><!--[--><span data-v-42ef59de>Python</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/articles/VitePress/" tabindex="0" data-v-7f418b0f data-v-42ef59de><!--[--><span data-v-42ef59de>VitePress</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/articles/windows/" tabindex="0" data-v-7f418b0f data-v-42ef59de><!--[--><span data-v-42ef59de>windows</span><!--]--></a><!--]--><!--]--></nav><!----><div class="VPNavBarAppearance appearance" data-v-a0fd61f4 data-v-e6aabb21><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" title="toggle dark mode" aria-checked="false" data-v-e6aabb21 data-v-ce54a7d1 data-v-b1685198><span class="check" data-v-b1685198><span class="icon" data-v-b1685198><!--[--><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="sun" data-v-ce54a7d1><path d="M12,18c-3.3,0-6-2.7-6-6s2.7-6,6-6s6,2.7,6,6S15.3,18,12,18zM12,8c-2.2,0-4,1.8-4,4c0,2.2,1.8,4,4,4c2.2,0,4-1.8,4-4C16,9.8,14.2,8,12,8z"></path><path d="M12,4c-0.6,0-1-0.4-1-1V1c0-0.6,0.4-1,1-1s1,0.4,1,1v2C13,3.6,12.6,4,12,4z"></path><path d="M12,24c-0.6,0-1-0.4-1-1v-2c0-0.6,0.4-1,1-1s1,0.4,1,1v2C13,23.6,12.6,24,12,24z"></path><path d="M5.6,6.6c-0.3,0-0.5-0.1-0.7-0.3L3.5,4.9c-0.4-0.4-0.4-1,0-1.4s1-0.4,1.4,0l1.4,1.4c0.4,0.4,0.4,1,0,1.4C6.2,6.5,5.9,6.6,5.6,6.6z"></path><path d="M19.8,20.8c-0.3,0-0.5-0.1-0.7-0.3l-1.4-1.4c-0.4-0.4-0.4-1,0-1.4s1-0.4,1.4,0l1.4,1.4c0.4,0.4,0.4,1,0,1.4C20.3,20.7,20,20.8,19.8,20.8z"></path><path d="M3,13H1c-0.6,0-1-0.4-1-1s0.4-1,1-1h2c0.6,0,1,0.4,1,1S3.6,13,3,13z"></path><path d="M23,13h-2c-0.6,0-1-0.4-1-1s0.4-1,1-1h2c0.6,0,1,0.4,1,1S23.6,13,23,13z"></path><path d="M4.2,20.8c-0.3,0-0.5-0.1-0.7-0.3c-0.4-0.4-0.4-1,0-1.4l1.4-1.4c0.4-0.4,1-0.4,1.4,0s0.4,1,0,1.4l-1.4,1.4C4.7,20.7,4.5,20.8,4.2,20.8z"></path><path d="M18.4,6.6c-0.3,0-0.5-0.1-0.7-0.3c-0.4-0.4-0.4-1,0-1.4l1.4-1.4c0.4-0.4,1-0.4,1.4,0s0.4,1,0,1.4l-1.4,1.4C18.9,6.5,18.6,6.6,18.4,6.6z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="moon" data-v-ce54a7d1><path d="M12.1,22c-0.3,0-0.6,0-0.9,0c-5.5-0.5-9.5-5.4-9-10.9c0.4-4.8,4.2-8.6,9-9c0.4,0,0.8,0.2,1,0.5c0.2,0.3,0.2,0.8-0.1,1.1c-2,2.7-1.4,6.4,1.3,8.4c2.1,1.6,5,1.6,7.1,0c0.3-0.2,0.7-0.3,1.1-0.1c0.3,0.2,0.5,0.6,0.5,1c-0.2,2.7-1.5,5.1-3.6,6.8C16.6,21.2,14.4,22,12.1,22zM9.3,4.4c-2.9,1-5,3.6-5.2,6.8c-0.4,4.4,2.8,8.3,7.2,8.7c2.1,0.2,4.2-0.4,5.8-1.8c1.1-0.9,1.9-2.1,2.4-3.4c-2.5,0.9-5.3,0.5-7.5-1.1C9.2,11.4,8.1,7.7,9.3,4.4z"></path></svg><!--]--></span></span></button></div><!----><div class="VPFlyout VPNavBarExtra extra" data-v-a0fd61f4 data-v-40855f84 data-v-9c007e85><button type="button" class="button" aria-haspopup="true" aria-expanded="false" aria-label="extra navigation" data-v-9c007e85><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="icon" data-v-9c007e85><circle cx="12" cy="12" r="2"></circle><circle cx="19" cy="12" r="2"></circle><circle cx="5" cy="12" r="2"></circle></svg></button><div class="menu" data-v-9c007e85><div class="VPMenu" data-v-9c007e85 data-v-e7ea1737><!----><!--[--><!--[--><!----><div class="group" data-v-40855f84><div class="item appearance" data-v-40855f84><p class="label" data-v-40855f84>Appearance</p><div class="appearance-action" data-v-40855f84><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" title="toggle dark mode" aria-checked="false" data-v-40855f84 data-v-ce54a7d1 data-v-b1685198><span class="check" data-v-b1685198><span class="icon" data-v-b1685198><!--[--><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="sun" data-v-ce54a7d1><path d="M12,18c-3.3,0-6-2.7-6-6s2.7-6,6-6s6,2.7,6,6S15.3,18,12,18zM12,8c-2.2,0-4,1.8-4,4c0,2.2,1.8,4,4,4c2.2,0,4-1.8,4-4C16,9.8,14.2,8,12,8z"></path><path d="M12,4c-0.6,0-1-0.4-1-1V1c0-0.6,0.4-1,1-1s1,0.4,1,1v2C13,3.6,12.6,4,12,4z"></path><path d="M12,24c-0.6,0-1-0.4-1-1v-2c0-0.6,0.4-1,1-1s1,0.4,1,1v2C13,23.6,12.6,24,12,24z"></path><path d="M5.6,6.6c-0.3,0-0.5-0.1-0.7-0.3L3.5,4.9c-0.4-0.4-0.4-1,0-1.4s1-0.4,1.4,0l1.4,1.4c0.4,0.4,0.4,1,0,1.4C6.2,6.5,5.9,6.6,5.6,6.6z"></path><path d="M19.8,20.8c-0.3,0-0.5-0.1-0.7-0.3l-1.4-1.4c-0.4-0.4-0.4-1,0-1.4s1-0.4,1.4,0l1.4,1.4c0.4,0.4,0.4,1,0,1.4C20.3,20.7,20,20.8,19.8,20.8z"></path><path d="M3,13H1c-0.6,0-1-0.4-1-1s0.4-1,1-1h2c0.6,0,1,0.4,1,1S3.6,13,3,13z"></path><path d="M23,13h-2c-0.6,0-1-0.4-1-1s0.4-1,1-1h2c0.6,0,1,0.4,1,1S23.6,13,23,13z"></path><path d="M4.2,20.8c-0.3,0-0.5-0.1-0.7-0.3c-0.4-0.4-0.4-1,0-1.4l1.4-1.4c0.4-0.4,1-0.4,1.4,0s0.4,1,0,1.4l-1.4,1.4C4.7,20.7,4.5,20.8,4.2,20.8z"></path><path d="M18.4,6.6c-0.3,0-0.5-0.1-0.7-0.3c-0.4-0.4-0.4-1,0-1.4l1.4-1.4c0.4-0.4,1-0.4,1.4,0s0.4,1,0,1.4l-1.4,1.4C18.9,6.5,18.6,6.6,18.4,6.6z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="moon" data-v-ce54a7d1><path d="M12.1,22c-0.3,0-0.6,0-0.9,0c-5.5-0.5-9.5-5.4-9-10.9c0.4-4.8,4.2-8.6,9-9c0.4,0,0.8,0.2,1,0.5c0.2,0.3,0.2,0.8-0.1,1.1c-2,2.7-1.4,6.4,1.3,8.4c2.1,1.6,5,1.6,7.1,0c0.3-0.2,0.7-0.3,1.1-0.1c0.3,0.2,0.5,0.6,0.5,1c-0.2,2.7-1.5,5.1-3.6,6.8C16.6,21.2,14.4,22,12.1,22zM9.3,4.4c-2.9,1-5,3.6-5.2,6.8c-0.4,4.4,2.8,8.3,7.2,8.7c2.1,0.2,4.2-0.4,5.8-1.8c1.1-0.9,1.9-2.1,2.4-3.4c-2.5,0.9-5.3,0.5-7.5-1.1C9.2,11.4,8.1,7.7,9.3,4.4z"></path></svg><!--]--></span></span></button></div></div></div><!----><!--]--><!--]--></div></div></div><!--[--><!--]--><button type="button" class="VPNavBarHamburger hamburger" aria-label="mobile navigation" aria-expanded="false" aria-controls="VPNavScreen" data-v-a0fd61f4 data-v-e5dd9c1c><span class="container" data-v-e5dd9c1c><span class="top" data-v-e5dd9c1c></span><span class="middle" data-v-e5dd9c1c></span><span class="bottom" data-v-e5dd9c1c></span></span></button></div></div></div></div><!----></header><div class="VPLocalNav reached-top" data-v-5a346dfe data-v-79c8c1df><button class="menu" aria-expanded="false" aria-controls="VPSidebarNav" data-v-79c8c1df><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="menu-icon" data-v-79c8c1df><path d="M17,11H3c-0.6,0-1-0.4-1-1s0.4-1,1-1h14c0.6,0,1,0.4,1,1S17.6,11,17,11z"></path><path d="M21,7H3C2.4,7,2,6.6,2,6s0.4-1,1-1h18c0.6,0,1,0.4,1,1S21.6,7,21,7z"></path><path d="M21,15H3c-0.6,0-1-0.4-1-1s0.4-1,1-1h18c0.6,0,1,0.4,1,1S21.6,15,21,15z"></path><path d="M17,19H3c-0.6,0-1-0.4-1-1s0.4-1,1-1h14c0.6,0,1,0.4,1,1S17.6,19,17,19z"></path></svg><span class="menu-text" data-v-79c8c1df>Menu</span></button><div class="VPLocalNavOutlineDropdown" style="--vp-vh:0px;" data-v-79c8c1df data-v-1c15a60a><button data-v-1c15a60a>Return to top</button><!----></div></div><aside class="VPSidebar" data-v-5a346dfe data-v-b00e2fdd><div class="curtain" data-v-b00e2fdd></div><nav class="nav" id="VPSidebarNav" aria-labelledby="sidebar-aria-label" tabindex="-1" data-v-b00e2fdd><span class="visually-hidden" id="sidebar-aria-label" data-v-b00e2fdd> Sidebar Navigation </span><!--[--><!--]--><!--[--><div class="group" data-v-b00e2fdd><section class="VPSidebarItem level-0 has-active" data-v-b00e2fdd data-v-e31bd47b><div class="item" role="button" tabindex="0" data-v-e31bd47b><div class="indicator" data-v-e31bd47b></div><h2 class="text" data-v-e31bd47b>Blog</h2><!----></div><div class="items" data-v-e31bd47b><!--[--><div class="VPSidebarItem level-1 is-link" data-v-e31bd47b data-v-e31bd47b><div class="item" data-v-e31bd47b><div class="indicator" data-v-e31bd47b></div><a class="VPLink link link" href="/articles/Blog/01%E6%8C%87%E4%BB%A4.html" data-v-e31bd47b><!--[--><p class="text" data-v-e31bd47b>01指令</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-e31bd47b data-v-e31bd47b><div class="item" data-v-e31bd47b><div class="indicator" data-v-e31bd47b></div><a class="VPLink link link" href="/articles/Blog/02HuggingFace%E5%85%A5%E9%97%A8.html" data-v-e31bd47b><!--[--><p class="text" data-v-e31bd47b>02HuggingFace入门</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-e31bd47b data-v-e31bd47b><div class="item" data-v-e31bd47b><div class="indicator" data-v-e31bd47b></div><a class="VPLink link link" href="/articles/Blog/04%E5%88%86%E8%AF%8D%E6%96%B9%E6%B3%95.html" data-v-e31bd47b><!--[--><p class="text" data-v-e31bd47b>04分词方法</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-e31bd47b data-v-e31bd47b><div class="item" data-v-e31bd47b><div class="indicator" data-v-e31bd47b></div><a class="VPLink link link" href="/articles/Blog/05%E6%89%A9%E5%85%85%E8%AF%8D%E8%A1%A8.html" data-v-e31bd47b><!--[--><p class="text" data-v-e31bd47b>05扩充词表</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-e31bd47b data-v-e31bd47b><div class="item" data-v-e31bd47b><div class="indicator" data-v-e31bd47b></div><a class="VPLink link link" href="/articles/Blog/Untitled.html" data-v-e31bd47b><!--[--><p class="text" data-v-e31bd47b>Untitled</p><!--]--></a><!----></div><!----></div><!--]--></div></section></div><!--]--><!--[--><!--]--></nav></aside><div class="VPContent has-sidebar" id="VPContent" data-v-5a346dfe data-v-669faec9><div class="VPDoc has-sidebar has-aside" data-v-669faec9 data-v-6b87e69f><!--[--><!--]--><div class="container" data-v-6b87e69f><div class="aside" data-v-6b87e69f><div class="aside-curtain" data-v-6b87e69f></div><div class="aside-container" data-v-6b87e69f><div class="aside-content" data-v-6b87e69f><div class="VPDocAside" data-v-6b87e69f data-v-3f215769><!--[--><!--]--><!--[--><!--]--><div class="VPDocAsideOutline" role="navigation" data-v-3f215769 data-v-d330b1bb><div class="content" data-v-d330b1bb><div class="outline-marker" data-v-d330b1bb></div><div class="outline-title" role="heading" aria-level="2" data-v-d330b1bb>On this page</div><nav aria-labelledby="doc-outline-aria-label" data-v-d330b1bb><span class="visually-hidden" id="doc-outline-aria-label" data-v-d330b1bb> Table of Contents for current page </span><ul class="root" data-v-d330b1bb data-v-d0ee3533><!--[--><!--]--></ul></nav></div></div><!--[--><!--]--><div class="spacer" data-v-3f215769></div><!--[--><!--]--><!----><!--[--><!--]--><!--[--><!--]--></div></div></div></div><div class="content" data-v-6b87e69f><div class="content-container" data-v-6b87e69f><!--[--><!--]--><!----><main class="main" data-v-6b87e69f><div style="position:relative;" class="vp-doc _articles_Blog_04%E5%88%86%E8%AF%8D%E6%96%B9%E6%B3%95" data-v-6b87e69f><div><h1 id="引言" tabindex="-1">引言 <a class="header-anchor" href="#引言" aria-label="Permalink to &quot;引言&quot;">​</a></h1><p><strong>为什么分词？</strong></p><p>句子不能直接处理，句子要离散化处理。</p><p><strong>基于统计的分词</strong></p><p>先对各个句子进行分词，然后再统计并选出频数最高的前N个词组成词表。</p><p>缺点：</p><p>1、词的数量很多，词表不全，容易出现(Out Of Vocabulary, OOV)，模型无法处理；</p><p>2、低频词模型无法得到充分训练。</p><p><strong>字符粒度的分词</strong></p><p>如，英文单词最终是26个英文字母，虽然能够解决OOV问题，但单词被拆分成字符后，一方面丢失了词的语义信息，另一方面，模型输入会变得很长，这使得模型的训练更加复杂难以收敛。</p><p><strong>Subword(子词)分词</strong></p><p>它的划分粒度介于词与字符之间，比如可以将”looking”划分为”look”和”ing”两个子词，而划分出来的&quot;look&quot;，”ing”又能够用来构造其它词。</p><p>目前有三种主流的Subword算法，它们分别是：Byte Pair Encoding (BPE), WordPiece和Unigram Language Model。</p><h1 id="n-gram模型" tabindex="-1">N-gram模型 <a class="header-anchor" href="#n-gram模型" aria-label="Permalink to &quot;N-gram模型&quot;">​</a></h1><p><strong>什么是N-gram模型？</strong></p><p>如果我们有一个由 m 个字/词组成的序列（或者说一个句子），我们希望算得句子的概率，根据链式规则，可得</p><p><img src="/assets/image-20231214142426407.4496551e.png" alt="image-20231214142426407"></p><p>这个概率显然并不好算，不妨利用马尔科夫链的假设，即当前这个词仅仅跟前面几个有限的词相关。</p><p>当 n=1, 一个一元模型（unigram model)即为 ：</p><p><img src="/assets/v2-4e5bacfab55c6fc21990be89e3c84e14_720w.90a1f7a3.webp" alt="img"></p><p>当 n=2, 一个二元模型（bigram model)即为 ：</p><p><img src="/assets/v2-d45c62f548c4da5d47a904a09ba20159_720w.22ce88e2.webp" alt="img"></p><p>通过我们的标准语料库，我们可以近似的计算出所有的分词之间的二元条件概率，比如任意两个词，它们的条件概率分布可以近似的表示为</p><p><img src="/assets/image-20231214142846436.e8f3f85e.png" alt="image-20231214142846436"></p><p>利用语料库建立的统计概率，对于一个新的句子，我们就可以通过计算各种分词方法对应的联合分布概率，找到最大概率对应的分词方法，即为最优分词。</p><p>N元模型的分词方法虽然很好，但是要在实际中应用也有很多问题：</p><ul><li>某些生僻词，概率为0。（这种情况我们一般会使用拉普拉斯平滑，即给它一个较小的概率值）</li><li>句子长，分词有很多情况，计算量也非常大，这时我们可以用下一节维特比算法来优化算法时间复杂度。</li></ul><p><strong>上面只是根据分词计算出句子概率，那么如何分词？维特比算法。</strong></p><p>为了简化原理描述，我们本节的讨论都是以二元模型为基础。</p><p>维特比算法采用的是动态规划来解决这个最优分词问题的，动态规划要求局部路径也是最优路径的一部分，很显然我们的问题是成立的。首先我们看一个简单的分词例子：&quot;人生如梦境&quot;。它的可能分词可以用下面的概率图表示：</p><p>对于每个词，我们都可以计算出它的频率和二元条件概率。</p><p><img src="/assets/1042406-20170407134342035-1966704661.0af98029.png" alt="img"></p><p>从后往前推，最终的分词结果为&quot;人生/如/梦境&quot;。</p><h1 id="bpe" tabindex="-1">BPE <a class="header-anchor" href="#bpe" aria-label="Permalink to &quot;BPE&quot;">​</a></h1><p>BPE最早是一种数据压缩算法，由Sennrich等人于2015年在《Neural Machine Translation of Rare Words with Subword Units》中提出引入到NLP领域并很快得到推广。该算法简单有效，因而目前它是最流行的方法。</p><h2 id="词表构建" tabindex="-1">词表构建 <a class="header-anchor" href="#词表构建" aria-label="Permalink to &quot;词表构建&quot;">​</a></h2><p>BPE分词/词表构建步骤：</p><ol><li>准备足够大的训练语料，并确定期望的Subword词表大小；</li><li>将单词拆分为成最小单元。比如英文中26个字母加上各种符号，这些作为初始词表；</li><li>在语料上统计单词内相邻单元对的频数，选取频数最高的单元对合并成新的Subword单元；</li><li>重复第3步直到达到第1步设定的Subword词表大小或下一个最高频数为1.</li></ol><p>示例：</p><p>准备预料，初始词表。注意这里一定加终止符<code>&lt;/w&gt;</code>，中止符可以区分单词边界。</p><p><img src="/assets/v2-a2bec0a9fbc6f1c1dea9647a5cb4434f_720w.c0d9794a.webp" alt="img"></p><p>不断合并高频词，一直重复这个过程，直至达到词表大小。</p><p><img src="/assets/v2-fad89fd55ea5db62fcdd0bcadb4e00f7_720w.e6e10ac4.webp" alt="img"></p><h2 id="语料编码" tabindex="-1">语料编码 <a class="header-anchor" href="#语料编码" aria-label="Permalink to &quot;语料编码&quot;">​</a></h2><p>BPE分词后，编码步骤：</p><ol><li>词表从大到小排序；</li><li>遍历排序好的词表，寻找词表中的子词是否是该单词的子字符串。如果正好**「匹配」**，则输出当前子词，并对单词剩下的字符串继续匹配；</li><li>如果遍历完词表，单词中仍然有子字符串没有被匹配，那我们将其替换为一个特殊的子词，比如<code>unk</code>。</li></ol><p>示例：</p><p>假设有词表</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki github-dark vp-code-dark"><code><span class="line"><span style="color:#e1e4e8;">(“errrr&lt;/w&gt;”, </span></span>
<span class="line"><span style="color:#e1e4e8;">“tain&lt;/w&gt;”, </span></span>
<span class="line"><span style="color:#e1e4e8;">“moun”, </span></span>
<span class="line"><span style="color:#e1e4e8;">“est&lt;/w&gt;”, </span></span>
<span class="line"><span style="color:#e1e4e8;">“high”, </span></span>
<span class="line"><span style="color:#e1e4e8;">“the&lt;/w&gt;”, </span></span>
<span class="line"><span style="color:#e1e4e8;">“a&lt;/w&gt;”,</span></span>
<span class="line"><span style="color:#e1e4e8;">“ve&lt;/w&gt;”)</span></span></code></pre><pre class="shiki github-light vp-code-light"><code><span class="line"><span style="color:#24292e;">(“errrr&lt;/w&gt;”, </span></span>
<span class="line"><span style="color:#24292e;">“tain&lt;/w&gt;”, </span></span>
<span class="line"><span style="color:#24292e;">“moun”, </span></span>
<span class="line"><span style="color:#24292e;">“est&lt;/w&gt;”, </span></span>
<span class="line"><span style="color:#24292e;">“high”, </span></span>
<span class="line"><span style="color:#24292e;">“the&lt;/w&gt;”, </span></span>
<span class="line"><span style="color:#24292e;">“a&lt;/w&gt;”,</span></span>
<span class="line"><span style="color:#24292e;">“ve&lt;/w&gt;”)</span></span></code></pre></div><p>对于给定的单词<code>mountain</code>，其分词结果为：[<code>moun</code>, <code>tain&lt;/w&gt;</code>]。</p><p>对于给定的单词<code>love</code>, 其分词结果为：[<code>unk</code>, <code>unk</code>, <code>ve&lt;/w&gt;</code>]</p><h2 id="语料解码" tabindex="-1">语料解码 <a class="header-anchor" href="#语料解码" aria-label="Permalink to &quot;语料解码&quot;">​</a></h2><p>语料解码就是将所有的输出子词拼在一起，直到碰到结尾为<code>&lt;\w&gt;</code>。</p><p>假设模型输出为</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki github-dark vp-code-dark"><code><span class="line"><span style="color:#e1e4e8;">[&quot;moun&quot;, &quot;tain&lt;/w&gt;&quot;, &quot;high&quot;, &quot;the&lt;/w&gt;&quot;]</span></span></code></pre><pre class="shiki github-light vp-code-light"><code><span class="line"><span style="color:#24292e;">[&quot;moun&quot;, &quot;tain&lt;/w&gt;&quot;, &quot;high&quot;, &quot;the&lt;/w&gt;&quot;]</span></span></code></pre></div><p>解码的结果</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki github-dark vp-code-dark"><code><span class="line"><span style="color:#e1e4e8;">[&quot;mountain&lt;/w&gt;&quot;, &quot;highthe&lt;/w&gt;&quot;]</span></span></code></pre><pre class="shiki github-light vp-code-light"><code><span class="line"><span style="color:#24292e;">[&quot;mountain&lt;/w&gt;&quot;, &quot;highthe&lt;/w&gt;&quot;]</span></span></code></pre></div><h2 id="代码实现bpe" tabindex="-1">代码实现BPE <a class="header-anchor" href="#代码实现bpe" aria-label="Permalink to &quot;代码实现BPE&quot;">​</a></h2><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki github-dark vp-code-dark"><code><span class="line"><span style="color:#e1e4e8;">import re</span></span>
<span class="line"><span style="color:#e1e4e8;"></span></span>
<span class="line"><span style="color:#e1e4e8;"></span></span>
<span class="line"><span style="color:#e1e4e8;"># 语料库</span></span>
<span class="line"><span style="color:#e1e4e8;">def get_vocab(in_data_list):</span></span>
<span class="line"><span style="color:#e1e4e8;">    vocab = {}</span></span>
<span class="line"><span style="color:#e1e4e8;">    for line in in_data_list:</span></span>
<span class="line"><span style="color:#e1e4e8;">        words = line.strip().split()</span></span>
<span class="line"><span style="color:#e1e4e8;">        for word in words:</span></span>
<span class="line"><span style="color:#e1e4e8;">            vocab[&#39; &#39;.join(list(word)) + &#39; &lt;/w&gt;&#39;] = vocab.get(&#39; &#39;.join(list(word)) + &#39; &lt;/w&gt;&#39;, 0) + 1</span></span>
<span class="line"><span style="color:#e1e4e8;">    return vocab</span></span>
<span class="line"><span style="color:#e1e4e8;"></span></span>
<span class="line"><span style="color:#e1e4e8;"></span></span>
<span class="line"><span style="color:#e1e4e8;"># 统计当前语料库所有tokens以及其对应的数量</span></span>
<span class="line"><span style="color:#e1e4e8;">def get_tokens(vocab):</span></span>
<span class="line"><span style="color:#e1e4e8;">    tokens = {}</span></span>
<span class="line"><span style="color:#e1e4e8;">    for word, freq in vocab.items():</span></span>
<span class="line"><span style="color:#e1e4e8;">        cur_chars = word.split()</span></span>
<span class="line"><span style="color:#e1e4e8;">        for cur_char in cur_chars:</span></span>
<span class="line"><span style="color:#e1e4e8;">            tokens[cur_char] = tokens.get(cur_char, 0) + freq</span></span>
<span class="line"><span style="color:#e1e4e8;"></span></span>
<span class="line"><span style="color:#e1e4e8;">    return tokens</span></span>
<span class="line"><span style="color:#e1e4e8;"></span></span>
<span class="line"><span style="color:#e1e4e8;"></span></span>
<span class="line"><span style="color:#e1e4e8;"># 获取所有可能组成pair的token合体之后的频率</span></span>
<span class="line"><span style="color:#e1e4e8;">def get_stats(vocab):</span></span>
<span class="line"><span style="color:#e1e4e8;">    pairs = {}</span></span>
<span class="line"><span style="color:#e1e4e8;">    for word, freq in vocab.items():</span></span>
<span class="line"><span style="color:#e1e4e8;">        cur_tokens = word.split()</span></span>
<span class="line"><span style="color:#e1e4e8;">        for i in range(len(cur_tokens) - 1):</span></span>
<span class="line"><span style="color:#e1e4e8;">            pairs[(cur_tokens[i], cur_tokens[i + 1])] = pairs.get((cur_tokens[i], cur_tokens[i + 1]), 0) + freq</span></span>
<span class="line"><span style="color:#e1e4e8;">    return pairs</span></span>
<span class="line"><span style="color:#e1e4e8;"></span></span>
<span class="line"><span style="color:#e1e4e8;"></span></span>
<span class="line"><span style="color:#e1e4e8;"># 给定特定的pair 比如(e, s) 将vocab中所有(e,s)合并</span></span>
<span class="line"><span style="color:#e1e4e8;">def merge_vocab(pair, in_vocab):</span></span>
<span class="line"><span style="color:#e1e4e8;">    out_vocab = {}</span></span>
<span class="line"><span style="color:#e1e4e8;">    bigram = re.escape(&#39; &#39;.join(pair))</span></span>
<span class="line"><span style="color:#e1e4e8;">    p = re.compile(r&#39;(?&lt;!\S)&#39; + bigram + r&#39;(?!\S)&#39;)</span></span>
<span class="line"><span style="color:#e1e4e8;">    for word in in_vocab:</span></span>
<span class="line"><span style="color:#e1e4e8;">        w_out = p.sub(&#39;&#39;.join(pair), word)</span></span>
<span class="line"><span style="color:#e1e4e8;">        out_vocab[w_out] = in_vocab[word]</span></span>
<span class="line"><span style="color:#e1e4e8;">    return out_vocab</span></span>
<span class="line"><span style="color:#e1e4e8;"></span></span>
<span class="line"><span style="color:#e1e4e8;"></span></span>
<span class="line"><span style="color:#e1e4e8;">data_list = [&#39;i love dogs&#39;, &#39;i loved you&#39;]</span></span>
<span class="line"><span style="color:#e1e4e8;">cur_vocab = get_vocab(data_list)</span></span>
<span class="line"><span style="color:#e1e4e8;">print(f&#39;Begining vocab: {cur_vocab}&#39;)</span></span>
<span class="line"><span style="color:#e1e4e8;">cur_tokens = get_tokens(cur_vocab)</span></span>
<span class="line"><span style="color:#e1e4e8;">print(f&#39;Begining tokens: {cur_tokens}&#39;)</span></span>
<span class="line"><span style="color:#e1e4e8;"></span></span>
<span class="line"><span style="color:#e1e4e8;"># 合并到词表达到指定大小 size大于等于原vocab的大小</span></span>
<span class="line"><span style="color:#e1e4e8;">to_merged_vocab_size = 7</span></span>
<span class="line"><span style="color:#e1e4e8;">epoch = 0</span></span>
<span class="line"><span style="color:#e1e4e8;">while len(cur_tokens) &gt; to_merged_vocab_size:</span></span>
<span class="line"><span style="color:#e1e4e8;">    print(&#39;-&#39; * 50)</span></span>
<span class="line"><span style="color:#e1e4e8;">    pairs = get_stats(cur_vocab)</span></span>
<span class="line"><span style="color:#e1e4e8;">    if not pairs:</span></span>
<span class="line"><span style="color:#e1e4e8;">        break</span></span>
<span class="line"><span style="color:#e1e4e8;">    max_freq_pair = max(pairs, key=pairs.get)</span></span>
<span class="line"><span style="color:#e1e4e8;">    cur_vocab = merge_vocab(max_freq_pair, cur_vocab)</span></span>
<span class="line"><span style="color:#e1e4e8;">    print(f&#39;Iter: {epoch}&#39;)</span></span>
<span class="line"><span style="color:#e1e4e8;">    print(f&#39;max_freq_pair: {max_freq_pair}&#39;)</span></span>
<span class="line"><span style="color:#e1e4e8;">    print(f&#39;cur vocab: {cur_vocab}&#39;)</span></span>
<span class="line"><span style="color:#e1e4e8;">    cur_tokens = get_tokens(cur_vocab)</span></span>
<span class="line"><span style="color:#e1e4e8;">    print(&#39;Tokens: {}&#39;.format(cur_tokens))</span></span>
<span class="line"><span style="color:#e1e4e8;">    print(f&#39;len tokens:{len(cur_tokens)} &#39;)</span></span>
<span class="line"><span style="color:#e1e4e8;">    epoch += 1</span></span></code></pre><pre class="shiki github-light vp-code-light"><code><span class="line"><span style="color:#24292e;">import re</span></span>
<span class="line"><span style="color:#24292e;"></span></span>
<span class="line"><span style="color:#24292e;"></span></span>
<span class="line"><span style="color:#24292e;"># 语料库</span></span>
<span class="line"><span style="color:#24292e;">def get_vocab(in_data_list):</span></span>
<span class="line"><span style="color:#24292e;">    vocab = {}</span></span>
<span class="line"><span style="color:#24292e;">    for line in in_data_list:</span></span>
<span class="line"><span style="color:#24292e;">        words = line.strip().split()</span></span>
<span class="line"><span style="color:#24292e;">        for word in words:</span></span>
<span class="line"><span style="color:#24292e;">            vocab[&#39; &#39;.join(list(word)) + &#39; &lt;/w&gt;&#39;] = vocab.get(&#39; &#39;.join(list(word)) + &#39; &lt;/w&gt;&#39;, 0) + 1</span></span>
<span class="line"><span style="color:#24292e;">    return vocab</span></span>
<span class="line"><span style="color:#24292e;"></span></span>
<span class="line"><span style="color:#24292e;"></span></span>
<span class="line"><span style="color:#24292e;"># 统计当前语料库所有tokens以及其对应的数量</span></span>
<span class="line"><span style="color:#24292e;">def get_tokens(vocab):</span></span>
<span class="line"><span style="color:#24292e;">    tokens = {}</span></span>
<span class="line"><span style="color:#24292e;">    for word, freq in vocab.items():</span></span>
<span class="line"><span style="color:#24292e;">        cur_chars = word.split()</span></span>
<span class="line"><span style="color:#24292e;">        for cur_char in cur_chars:</span></span>
<span class="line"><span style="color:#24292e;">            tokens[cur_char] = tokens.get(cur_char, 0) + freq</span></span>
<span class="line"><span style="color:#24292e;"></span></span>
<span class="line"><span style="color:#24292e;">    return tokens</span></span>
<span class="line"><span style="color:#24292e;"></span></span>
<span class="line"><span style="color:#24292e;"></span></span>
<span class="line"><span style="color:#24292e;"># 获取所有可能组成pair的token合体之后的频率</span></span>
<span class="line"><span style="color:#24292e;">def get_stats(vocab):</span></span>
<span class="line"><span style="color:#24292e;">    pairs = {}</span></span>
<span class="line"><span style="color:#24292e;">    for word, freq in vocab.items():</span></span>
<span class="line"><span style="color:#24292e;">        cur_tokens = word.split()</span></span>
<span class="line"><span style="color:#24292e;">        for i in range(len(cur_tokens) - 1):</span></span>
<span class="line"><span style="color:#24292e;">            pairs[(cur_tokens[i], cur_tokens[i + 1])] = pairs.get((cur_tokens[i], cur_tokens[i + 1]), 0) + freq</span></span>
<span class="line"><span style="color:#24292e;">    return pairs</span></span>
<span class="line"><span style="color:#24292e;"></span></span>
<span class="line"><span style="color:#24292e;"></span></span>
<span class="line"><span style="color:#24292e;"># 给定特定的pair 比如(e, s) 将vocab中所有(e,s)合并</span></span>
<span class="line"><span style="color:#24292e;">def merge_vocab(pair, in_vocab):</span></span>
<span class="line"><span style="color:#24292e;">    out_vocab = {}</span></span>
<span class="line"><span style="color:#24292e;">    bigram = re.escape(&#39; &#39;.join(pair))</span></span>
<span class="line"><span style="color:#24292e;">    p = re.compile(r&#39;(?&lt;!\S)&#39; + bigram + r&#39;(?!\S)&#39;)</span></span>
<span class="line"><span style="color:#24292e;">    for word in in_vocab:</span></span>
<span class="line"><span style="color:#24292e;">        w_out = p.sub(&#39;&#39;.join(pair), word)</span></span>
<span class="line"><span style="color:#24292e;">        out_vocab[w_out] = in_vocab[word]</span></span>
<span class="line"><span style="color:#24292e;">    return out_vocab</span></span>
<span class="line"><span style="color:#24292e;"></span></span>
<span class="line"><span style="color:#24292e;"></span></span>
<span class="line"><span style="color:#24292e;">data_list = [&#39;i love dogs&#39;, &#39;i loved you&#39;]</span></span>
<span class="line"><span style="color:#24292e;">cur_vocab = get_vocab(data_list)</span></span>
<span class="line"><span style="color:#24292e;">print(f&#39;Begining vocab: {cur_vocab}&#39;)</span></span>
<span class="line"><span style="color:#24292e;">cur_tokens = get_tokens(cur_vocab)</span></span>
<span class="line"><span style="color:#24292e;">print(f&#39;Begining tokens: {cur_tokens}&#39;)</span></span>
<span class="line"><span style="color:#24292e;"></span></span>
<span class="line"><span style="color:#24292e;"># 合并到词表达到指定大小 size大于等于原vocab的大小</span></span>
<span class="line"><span style="color:#24292e;">to_merged_vocab_size = 7</span></span>
<span class="line"><span style="color:#24292e;">epoch = 0</span></span>
<span class="line"><span style="color:#24292e;">while len(cur_tokens) &gt; to_merged_vocab_size:</span></span>
<span class="line"><span style="color:#24292e;">    print(&#39;-&#39; * 50)</span></span>
<span class="line"><span style="color:#24292e;">    pairs = get_stats(cur_vocab)</span></span>
<span class="line"><span style="color:#24292e;">    if not pairs:</span></span>
<span class="line"><span style="color:#24292e;">        break</span></span>
<span class="line"><span style="color:#24292e;">    max_freq_pair = max(pairs, key=pairs.get)</span></span>
<span class="line"><span style="color:#24292e;">    cur_vocab = merge_vocab(max_freq_pair, cur_vocab)</span></span>
<span class="line"><span style="color:#24292e;">    print(f&#39;Iter: {epoch}&#39;)</span></span>
<span class="line"><span style="color:#24292e;">    print(f&#39;max_freq_pair: {max_freq_pair}&#39;)</span></span>
<span class="line"><span style="color:#24292e;">    print(f&#39;cur vocab: {cur_vocab}&#39;)</span></span>
<span class="line"><span style="color:#24292e;">    cur_tokens = get_tokens(cur_vocab)</span></span>
<span class="line"><span style="color:#24292e;">    print(&#39;Tokens: {}&#39;.format(cur_tokens))</span></span>
<span class="line"><span style="color:#24292e;">    print(f&#39;len tokens:{len(cur_tokens)} &#39;)</span></span>
<span class="line"><span style="color:#24292e;">    epoch += 1</span></span></code></pre></div><h1 id="byte-level-bpe" tabindex="-1">Byte-level BPE <a class="header-anchor" href="#byte-level-bpe" aria-label="Permalink to &quot;Byte-level BPE&quot;">​</a></h1><p>BPE的算法最开始的基础词表也可能会很大，比如如果将所有的unicode 字符都放入基础词表，一开始的词表大小就有十几万了。一种处理方法是我们<strong>以一个字节为一种“字符”</strong>，基础字符集的大小就锁定在了<strong>2^8=256</strong>。</p><p>例如，GPT-2的词汇表大小为50257 = 256 + <code>&lt;EOS&gt;</code> + 50000 mergers，<code>&lt;EOS&gt;</code>是句子结尾的特殊标记。</p><h1 id="wordpiece" tabindex="-1">WordPiece <a class="header-anchor" href="#wordpiece" aria-label="Permalink to &quot;WordPiece&quot;">​</a></h1><p>最早出现在2016年谷歌的论文Google’s Neural Machine Translation system。 2018 年 BERT 中也用了。</p><p>它可以被认为是 BPE 和 Unigram 算法的折中。WordPiece 也是一种贪婪算法，它利用似然性而不是计数频率来合并每次迭代中的最佳对，但配对字符的选择是基于计数频率的。因此，在选择要配对的字符方面，它与 BPE 类似；在选择要合并的最佳对方面，它与 Unigram 类似。</p><h2 id="原理" tabindex="-1">原理 <a class="header-anchor" href="#原理" aria-label="Permalink to &quot;原理&quot;">​</a></h2><p>BPE算法通过循环的方式不断将一些高频的pair进行合并，通过贪婪的方式，每一步都将将高频的组合进行合并。这种方法存在的一个主要问题是：一个词可能存在多种拆分方式，对于算法来说，难以评估使用那个拆分方式比较合理，可以组合的列表中的优先级无法确定，通常会直接取第一个，举个例子：</p><p><img src="/assets/v2-5ec8d534e62e24befd16865da662f197_r.84704bcf.jpg" alt="img"></p><p>例如：linear = <strong>li + near</strong> 或者 <strong>li + n + ea + r</strong>，这两种拆分方法哪个好坏，无法评价。所以，比BPE使用频次进行merge更好的方法是， 在merge的时候考虑merge前后的影响到底有多大。</p><p>WordPiece选择能够提升语言模型似然概率最大的相邻子词加入词表。例如，在考虑将&quot;e&quot;和&quot;s&quot;合并的时候除了会考虑&quot;es&quot;的概率值，还会考虑&quot;e&quot;和&quot;s&quot;的概率值。</p><p><strong>似然概率怎么计算？</strong></p><p>假设句子<mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative;"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:-0.566ex;" xmlns="http://www.w3.org/2000/svg" width="16.331ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 7218.2 1000" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D446" d="M308 24Q367 24 416 76T466 197Q466 260 414 284Q308 311 278 321T236 341Q176 383 176 462Q176 523 208 573T273 648Q302 673 343 688T407 704H418H425Q521 704 564 640Q565 640 577 653T603 682T623 704Q624 704 627 704T632 705Q645 705 645 698T617 577T585 459T569 456Q549 456 549 465Q549 471 550 475Q550 478 551 494T553 520Q553 554 544 579T526 616T501 641Q465 662 419 662Q362 662 313 616T263 510Q263 480 278 458T319 427Q323 425 389 408T456 390Q490 379 522 342T554 242Q554 216 546 186Q541 164 528 137T492 78T426 18T332 -20Q320 -22 298 -22Q199 -22 144 33L134 44L106 13Q83 -14 78 -18T65 -22Q52 -22 52 -14Q52 -11 110 221Q112 227 130 227H143Q149 221 149 216Q149 214 148 207T144 186T142 153Q144 114 160 87T203 47T255 29T308 24Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(922.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(1978.6,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(2367.6,0)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z" style="stroke-width:3;"></path></g><g data-mml-node="mn" transform="translate(2728.6,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(3228.6,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(3673.2,0)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z" style="stroke-width:3;"></path></g><g data-mml-node="mn" transform="translate(4034.2,0)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(4534.2,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(4978.9,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(5423.6,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(5868.2,0)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(6229.2,0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(6829.2,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" style="stroke-width:3;"></path></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0px;left:0px;clip:rect(1px, 1px, 1px, 1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0px 0px 0px;border:0px;display:block;width:auto;overflow:hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>S</mi><mo>=</mo><mo stretchy="false">(</mo><mi>t</mi><mn>1</mn><mo>,</mo><mi>t</mi><mn>2</mn><mo>,</mo><mo>,</mo><mo>,</mo><mi>t</mi><mi>n</mi><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container>，则句子S的语言模型似然值是：</p><p><img src="/assets/image-20231214084913459.213883c4.png" alt="image-20231214084913459"></p><p>假设把相邻位置的x和y两个子词进行合并，合并后产生的子词记为z，此时句子S似然值的变化可表示为：</p><p><img src="/assets/image-20231214084959106.bda39f2a.png" alt="image-20231214084959106"></p><p><img src="data:image/png;base64,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" alt="image-20231214195702259"></p><p>遍历所有的字符对，合并Loss最大的字符对。</p><p><strong>不太好理解？</strong></p><p>拆分前信息熵为-log(p(tz))，拆分后为-log(p(tx)) - log(p(ty)) ，拆分后-拆分前的信息增益为-log(p(tx)) - log(p(ty)) + log(p(tz))，可以变形为上式。信息增益越大越好。</p><h2 id="词表构建-1" tabindex="-1">词表构建 <a class="header-anchor" href="#词表构建-1" aria-label="Permalink to &quot;词表构建&quot;">​</a></h2><p><strong>WordPiece算法的主要步骤如下：</strong></p><ol><li>准备足够大的训练语料</li><li>确定期望的subword词表大小</li><li>将单词拆分成字符序列</li><li>基于第3步数据训练语言模型</li><li>从所有可能的subword单元中选择加入语言模型后能最大程度地增加训练数据概率的单元作为新的单元</li><li>重复第5步直到达到第2步设定的subword词表大小或概率增量低于某一阈值</li></ol><p>构建分词表后，编码解码步骤和BPE一样。</p><p>WordPiece有两种代码实现方式：bottom-up 和 top-down。最初的WordPiece和BPE一样基于bottom-up的，而BERT是基于 top-down。对于日语、中文和韩语，这种 top-down 的方法不起作用，因为没有明确的token units 可以开始。</p><h2 id="代码实现" tabindex="-1">代码实现 <a class="header-anchor" href="#代码实现" aria-label="Permalink to &quot;代码实现&quot;">​</a></h2><p><strong>这里是WordPiece的bottom-up算法实现</strong></p><p>top-down算法请参考<a href="https://www.tensorflow.org/text/guide/subwords_tokenizer#optional_the_algorithm" target="_blank" rel="noreferrer">https://www.tensorflow.org/text/guide/subwords_tokenizer#optional_the_algorithm</a></p><p>WordPiece算法分词时，会在非单词开头的字符上加上<code>##</code>前缀。如love，分词是</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki github-dark vp-code-dark"><code><span class="line"><span style="color:#e1e4e8;">l</span></span>
<span class="line"><span style="color:#e1e4e8;">##o</span></span>
<span class="line"><span style="color:#e1e4e8;">##v</span></span>
<span class="line"><span style="color:#e1e4e8;">##e</span></span></code></pre><pre class="shiki github-light vp-code-light"><code><span class="line"><span style="color:#24292e;">l</span></span>
<span class="line"><span style="color:#24292e;">##o</span></span>
<span class="line"><span style="color:#24292e;">##v</span></span>
<span class="line"><span style="color:#24292e;">##e</span></span></code></pre></div><p>代码示例</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki github-dark vp-code-dark"><code><span class="line"><span style="color:#e1e4e8;">from collections import defaultdict</span></span>
<span class="line"><span style="color:#e1e4e8;"></span></span>
<span class="line"><span style="color:#e1e4e8;"></span></span>
<span class="line"><span style="color:#e1e4e8;">def get_word_freqs(pre_tokenized_res):</span></span>
<span class="line"><span style="color:#e1e4e8;">    word_freqs = {}</span></span>
<span class="line"><span style="color:#e1e4e8;">    for word in pre_tokenized_res:</span></span>
<span class="line"><span style="color:#e1e4e8;">        word_freqs[word] = word_freqs.get(word, 0) + 1</span></span>
<span class="line"><span style="color:#e1e4e8;">    return word_freqs</span></span>
<span class="line"><span style="color:#e1e4e8;"></span></span>
<span class="line"><span style="color:#e1e4e8;"></span></span>
<span class="line"><span style="color:#e1e4e8;">def get_word_splits(word_freqs):</span></span>
<span class="line"><span style="color:#e1e4e8;">    word_splits = {}</span></span>
<span class="line"><span style="color:#e1e4e8;">    for word in word_freqs.keys():</span></span>
<span class="line"><span style="color:#e1e4e8;">        word_splits[word] = [c if i == 0 else f&quot;##{c}&quot; for i, c in enumerate(word)]</span></span>
<span class="line"><span style="color:#e1e4e8;">    return word_splits</span></span>
<span class="line"><span style="color:#e1e4e8;"></span></span>
<span class="line"><span style="color:#e1e4e8;"></span></span>
<span class="line"><span style="color:#e1e4e8;">def get_base_vocab(word_freqs, word_splits):</span></span>
<span class="line"><span style="color:#e1e4e8;">    vocab = {}</span></span>
<span class="line"><span style="color:#e1e4e8;">    for word, freq in word_freqs.items():</span></span>
<span class="line"><span style="color:#e1e4e8;">        for subword in word_splits.get(word):</span></span>
<span class="line"><span style="color:#e1e4e8;">            vocab[subword] = vocab.get(subword, 0) + freq</span></span>
<span class="line"><span style="color:#e1e4e8;">    return vocab</span></span>
<span class="line"><span style="color:#e1e4e8;"></span></span>
<span class="line"><span style="color:#e1e4e8;"></span></span>
<span class="line"><span style="color:#e1e4e8;">def get_pair_scores(word_freqs, word_splits, vocab):</span></span>
<span class="line"><span style="color:#e1e4e8;">    pair_freqs = {}</span></span>
<span class="line"><span style="color:#e1e4e8;">    for freq, split in zip(word_freqs.values(), word_splits.values()):</span></span>
<span class="line"><span style="color:#e1e4e8;">        for i in range(len(split) - 1):</span></span>
<span class="line"><span style="color:#e1e4e8;">            pair = (split[i], split[i + 1])</span></span>
<span class="line"><span style="color:#e1e4e8;">            pair_freqs[pair] = pair_freqs.get(pair, 0) + freq</span></span>
<span class="line"><span style="color:#e1e4e8;">    pair_scores = {}</span></span>
<span class="line"><span style="color:#e1e4e8;">    for pair, freq in pair_freqs.items():</span></span>
<span class="line"><span style="color:#e1e4e8;">        pair_scores[pair] = freq / (vocab[pair[0]] * vocab[pair[1]])</span></span>
<span class="line"><span style="color:#e1e4e8;">    return pair_scores</span></span>
<span class="line"><span style="color:#e1e4e8;"></span></span>
<span class="line"><span style="color:#e1e4e8;"></span></span>
<span class="line"><span style="color:#e1e4e8;">def merge_pair(pair, word_splits):</span></span>
<span class="line"><span style="color:#e1e4e8;">    a, b = pair</span></span>
<span class="line"><span style="color:#e1e4e8;">    for word, split in word_splits.items():</span></span>
<span class="line"><span style="color:#e1e4e8;">        if len(split) == 1:</span></span>
<span class="line"><span style="color:#e1e4e8;">            continue</span></span>
<span class="line"><span style="color:#e1e4e8;">        i = 0</span></span>
<span class="line"><span style="color:#e1e4e8;">        while i &lt; len(split) - 1:</span></span>
<span class="line"><span style="color:#e1e4e8;">            if split[i] == a and split[i + 1] == b:</span></span>
<span class="line"><span style="color:#e1e4e8;">                new_word = a + b[2:] if b.startswith(&#39;##&#39;) else a + b</span></span>
<span class="line"><span style="color:#e1e4e8;">                split = split[:i] + [new_word] + split[i + 2:]</span></span>
<span class="line"><span style="color:#e1e4e8;">            i += 1</span></span>
<span class="line"><span style="color:#e1e4e8;">        word_splits[word] = split</span></span>
<span class="line"><span style="color:#e1e4e8;">    return word_splits</span></span>
<span class="line"><span style="color:#e1e4e8;"></span></span>
<span class="line"><span style="color:#e1e4e8;"></span></span>
<span class="line"><span style="color:#e1e4e8;">def wordpiece_train(vocab_size, word_freqs, word_splits):</span></span>
<span class="line"><span style="color:#e1e4e8;">    vocab = get_base_vocab(word_splits)</span></span>
<span class="line"><span style="color:#e1e4e8;">    while len(vocab) &lt; vocab_size:</span></span>
<span class="line"><span style="color:#e1e4e8;">        pair_scores = get_pair_scores(word_freqs, word_splits)</span></span>
<span class="line"><span style="color:#e1e4e8;">        if not pair_scores:</span></span>
<span class="line"><span style="color:#e1e4e8;">            break</span></span>
<span class="line"><span style="color:#e1e4e8;">        max_score_pair = max(pair_scores, key=pair_scores.get)</span></span>
<span class="line"><span style="color:#e1e4e8;">        word_splits = merge_pair(max_score_pair, word_splits)</span></span>
<span class="line"><span style="color:#e1e4e8;">        a, b = max_score_pair</span></span>
<span class="line"><span style="color:#e1e4e8;">        new_word = a + b[2:] if b.startswith(&#39;##&#39;) else a + b</span></span>
<span class="line"><span style="color:#e1e4e8;">        vocab.append(new_word)</span></span>
<span class="line"><span style="color:#e1e4e8;">    return vocab</span></span>
<span class="line"><span style="color:#e1e4e8;"></span></span>
<span class="line"><span style="color:#e1e4e8;"></span></span>
<span class="line"><span style="color:#e1e4e8;">&quot;&quot;&quot;</span></span>
<span class="line"><span style="color:#e1e4e8;">I love dogs</span></span>
<span class="line"><span style="color:#e1e4e8;">I loved you</span></span>
<span class="line"><span style="color:#e1e4e8;">&quot;&quot;&quot;</span></span>
<span class="line"><span style="color:#e1e4e8;">corpus = [&#39;I&#39;, &#39;love&#39;, &#39;dogs&#39;, &#39;I&#39;, &#39;loved&#39;, &#39;you&#39;]</span></span>
<span class="line"><span style="color:#e1e4e8;">word_freqs = get_word_freqs(corpus)</span></span>
<span class="line"><span style="color:#e1e4e8;">print(f&quot;word_freqs: {word_freqs}&quot;)</span></span>
<span class="line"><span style="color:#e1e4e8;">word_splits = get_word_splits(word_freqs)</span></span>
<span class="line"><span style="color:#e1e4e8;">print(f&quot;word_splits: {word_splits}&quot;)</span></span>
<span class="line"><span style="color:#e1e4e8;">vocab = get_base_vocab(word_freqs, word_splits)</span></span>
<span class="line"><span style="color:#e1e4e8;">print(f&quot;vocab: {vocab}&quot;)</span></span>
<span class="line"><span style="color:#e1e4e8;"></span></span>
<span class="line"><span style="color:#e1e4e8;"># 合并到词表达到指定大小</span></span>
<span class="line"><span style="color:#e1e4e8;">to_merged_vocab_size = 7</span></span>
<span class="line"><span style="color:#e1e4e8;">epoch = 0</span></span>
<span class="line"><span style="color:#e1e4e8;">while len(vocab) &gt; to_merged_vocab_size:</span></span>
<span class="line"><span style="color:#e1e4e8;">    print(&#39;-&#39; * 50)</span></span>
<span class="line"><span style="color:#e1e4e8;">    print(f&#39;epoch: {epoch}&#39;)</span></span>
<span class="line"><span style="color:#e1e4e8;">    pair_scores = get_pair_scores(word_freqs, word_splits, vocab)</span></span>
<span class="line"><span style="color:#e1e4e8;">    if not pair_scores:</span></span>
<span class="line"><span style="color:#e1e4e8;">        break</span></span>
<span class="line"><span style="color:#e1e4e8;">    max_score_pair = max(pair_scores, key=pair_scores.get)</span></span>
<span class="line"><span style="color:#e1e4e8;">    print(f&#39;max_score_pair: {max_score_pair}&#39;)</span></span>
<span class="line"><span style="color:#e1e4e8;">    word_splits = merge_pair(max_score_pair, word_splits)</span></span>
<span class="line"><span style="color:#e1e4e8;">    print(f&#39;word_splits: {word_splits}&#39;)</span></span>
<span class="line"><span style="color:#e1e4e8;">    vocab = get_base_vocab(word_freqs, word_splits)</span></span>
<span class="line"><span style="color:#e1e4e8;">    print(f&#39;vocab: {vocab}&#39;)</span></span>
<span class="line"><span style="color:#e1e4e8;">print(&#39;-&#39; * 50)</span></span>
<span class="line"><span style="color:#e1e4e8;">print(f&#39;vocab: {list(vocab.keys())}&#39;)</span></span></code></pre><pre class="shiki github-light vp-code-light"><code><span class="line"><span style="color:#24292e;">from collections import defaultdict</span></span>
<span class="line"><span style="color:#24292e;"></span></span>
<span class="line"><span style="color:#24292e;"></span></span>
<span class="line"><span style="color:#24292e;">def get_word_freqs(pre_tokenized_res):</span></span>
<span class="line"><span style="color:#24292e;">    word_freqs = {}</span></span>
<span class="line"><span style="color:#24292e;">    for word in pre_tokenized_res:</span></span>
<span class="line"><span style="color:#24292e;">        word_freqs[word] = word_freqs.get(word, 0) + 1</span></span>
<span class="line"><span style="color:#24292e;">    return word_freqs</span></span>
<span class="line"><span style="color:#24292e;"></span></span>
<span class="line"><span style="color:#24292e;"></span></span>
<span class="line"><span style="color:#24292e;">def get_word_splits(word_freqs):</span></span>
<span class="line"><span style="color:#24292e;">    word_splits = {}</span></span>
<span class="line"><span style="color:#24292e;">    for word in word_freqs.keys():</span></span>
<span class="line"><span style="color:#24292e;">        word_splits[word] = [c if i == 0 else f&quot;##{c}&quot; for i, c in enumerate(word)]</span></span>
<span class="line"><span style="color:#24292e;">    return word_splits</span></span>
<span class="line"><span style="color:#24292e;"></span></span>
<span class="line"><span style="color:#24292e;"></span></span>
<span class="line"><span style="color:#24292e;">def get_base_vocab(word_freqs, word_splits):</span></span>
<span class="line"><span style="color:#24292e;">    vocab = {}</span></span>
<span class="line"><span style="color:#24292e;">    for word, freq in word_freqs.items():</span></span>
<span class="line"><span style="color:#24292e;">        for subword in word_splits.get(word):</span></span>
<span class="line"><span style="color:#24292e;">            vocab[subword] = vocab.get(subword, 0) + freq</span></span>
<span class="line"><span style="color:#24292e;">    return vocab</span></span>
<span class="line"><span style="color:#24292e;"></span></span>
<span class="line"><span style="color:#24292e;"></span></span>
<span class="line"><span style="color:#24292e;">def get_pair_scores(word_freqs, word_splits, vocab):</span></span>
<span class="line"><span style="color:#24292e;">    pair_freqs = {}</span></span>
<span class="line"><span style="color:#24292e;">    for freq, split in zip(word_freqs.values(), word_splits.values()):</span></span>
<span class="line"><span style="color:#24292e;">        for i in range(len(split) - 1):</span></span>
<span class="line"><span style="color:#24292e;">            pair = (split[i], split[i + 1])</span></span>
<span class="line"><span style="color:#24292e;">            pair_freqs[pair] = pair_freqs.get(pair, 0) + freq</span></span>
<span class="line"><span style="color:#24292e;">    pair_scores = {}</span></span>
<span class="line"><span style="color:#24292e;">    for pair, freq in pair_freqs.items():</span></span>
<span class="line"><span style="color:#24292e;">        pair_scores[pair] = freq / (vocab[pair[0]] * vocab[pair[1]])</span></span>
<span class="line"><span style="color:#24292e;">    return pair_scores</span></span>
<span class="line"><span style="color:#24292e;"></span></span>
<span class="line"><span style="color:#24292e;"></span></span>
<span class="line"><span style="color:#24292e;">def merge_pair(pair, word_splits):</span></span>
<span class="line"><span style="color:#24292e;">    a, b = pair</span></span>
<span class="line"><span style="color:#24292e;">    for word, split in word_splits.items():</span></span>
<span class="line"><span style="color:#24292e;">        if len(split) == 1:</span></span>
<span class="line"><span style="color:#24292e;">            continue</span></span>
<span class="line"><span style="color:#24292e;">        i = 0</span></span>
<span class="line"><span style="color:#24292e;">        while i &lt; len(split) - 1:</span></span>
<span class="line"><span style="color:#24292e;">            if split[i] == a and split[i + 1] == b:</span></span>
<span class="line"><span style="color:#24292e;">                new_word = a + b[2:] if b.startswith(&#39;##&#39;) else a + b</span></span>
<span class="line"><span style="color:#24292e;">                split = split[:i] + [new_word] + split[i + 2:]</span></span>
<span class="line"><span style="color:#24292e;">            i += 1</span></span>
<span class="line"><span style="color:#24292e;">        word_splits[word] = split</span></span>
<span class="line"><span style="color:#24292e;">    return word_splits</span></span>
<span class="line"><span style="color:#24292e;"></span></span>
<span class="line"><span style="color:#24292e;"></span></span>
<span class="line"><span style="color:#24292e;">def wordpiece_train(vocab_size, word_freqs, word_splits):</span></span>
<span class="line"><span style="color:#24292e;">    vocab = get_base_vocab(word_splits)</span></span>
<span class="line"><span style="color:#24292e;">    while len(vocab) &lt; vocab_size:</span></span>
<span class="line"><span style="color:#24292e;">        pair_scores = get_pair_scores(word_freqs, word_splits)</span></span>
<span class="line"><span style="color:#24292e;">        if not pair_scores:</span></span>
<span class="line"><span style="color:#24292e;">            break</span></span>
<span class="line"><span style="color:#24292e;">        max_score_pair = max(pair_scores, key=pair_scores.get)</span></span>
<span class="line"><span style="color:#24292e;">        word_splits = merge_pair(max_score_pair, word_splits)</span></span>
<span class="line"><span style="color:#24292e;">        a, b = max_score_pair</span></span>
<span class="line"><span style="color:#24292e;">        new_word = a + b[2:] if b.startswith(&#39;##&#39;) else a + b</span></span>
<span class="line"><span style="color:#24292e;">        vocab.append(new_word)</span></span>
<span class="line"><span style="color:#24292e;">    return vocab</span></span>
<span class="line"><span style="color:#24292e;"></span></span>
<span class="line"><span style="color:#24292e;"></span></span>
<span class="line"><span style="color:#24292e;">&quot;&quot;&quot;</span></span>
<span class="line"><span style="color:#24292e;">I love dogs</span></span>
<span class="line"><span style="color:#24292e;">I loved you</span></span>
<span class="line"><span style="color:#24292e;">&quot;&quot;&quot;</span></span>
<span class="line"><span style="color:#24292e;">corpus = [&#39;I&#39;, &#39;love&#39;, &#39;dogs&#39;, &#39;I&#39;, &#39;loved&#39;, &#39;you&#39;]</span></span>
<span class="line"><span style="color:#24292e;">word_freqs = get_word_freqs(corpus)</span></span>
<span class="line"><span style="color:#24292e;">print(f&quot;word_freqs: {word_freqs}&quot;)</span></span>
<span class="line"><span style="color:#24292e;">word_splits = get_word_splits(word_freqs)</span></span>
<span class="line"><span style="color:#24292e;">print(f&quot;word_splits: {word_splits}&quot;)</span></span>
<span class="line"><span style="color:#24292e;">vocab = get_base_vocab(word_freqs, word_splits)</span></span>
<span class="line"><span style="color:#24292e;">print(f&quot;vocab: {vocab}&quot;)</span></span>
<span class="line"><span style="color:#24292e;"></span></span>
<span class="line"><span style="color:#24292e;"># 合并到词表达到指定大小</span></span>
<span class="line"><span style="color:#24292e;">to_merged_vocab_size = 7</span></span>
<span class="line"><span style="color:#24292e;">epoch = 0</span></span>
<span class="line"><span style="color:#24292e;">while len(vocab) &gt; to_merged_vocab_size:</span></span>
<span class="line"><span style="color:#24292e;">    print(&#39;-&#39; * 50)</span></span>
<span class="line"><span style="color:#24292e;">    print(f&#39;epoch: {epoch}&#39;)</span></span>
<span class="line"><span style="color:#24292e;">    pair_scores = get_pair_scores(word_freqs, word_splits, vocab)</span></span>
<span class="line"><span style="color:#24292e;">    if not pair_scores:</span></span>
<span class="line"><span style="color:#24292e;">        break</span></span>
<span class="line"><span style="color:#24292e;">    max_score_pair = max(pair_scores, key=pair_scores.get)</span></span>
<span class="line"><span style="color:#24292e;">    print(f&#39;max_score_pair: {max_score_pair}&#39;)</span></span>
<span class="line"><span style="color:#24292e;">    word_splits = merge_pair(max_score_pair, word_splits)</span></span>
<span class="line"><span style="color:#24292e;">    print(f&#39;word_splits: {word_splits}&#39;)</span></span>
<span class="line"><span style="color:#24292e;">    vocab = get_base_vocab(word_freqs, word_splits)</span></span>
<span class="line"><span style="color:#24292e;">    print(f&#39;vocab: {vocab}&#39;)</span></span>
<span class="line"><span style="color:#24292e;">print(&#39;-&#39; * 50)</span></span>
<span class="line"><span style="color:#24292e;">print(f&#39;vocab: {list(vocab.keys())}&#39;)</span></span></code></pre></div><h1 id="unigram" tabindex="-1">Unigram <a class="header-anchor" href="#unigram" aria-label="Permalink to &quot;Unigram&quot;">​</a></h1><h2 id="原理-1" tabindex="-1">原理 <a class="header-anchor" href="#原理-1" aria-label="Permalink to &quot;原理&quot;">​</a></h2><p>Unigram 语言建模2018在 《 Improving neural network translation models with multiple subword candidates》 中提出。与WordPiece一样，Unigram Language Model(ULM)同样使用语言模型来挑选子词。不同之处在于，BPE和WordPiece算法的词表大小都是从小到大变化，属于增量法。而Unigram Language Model则是减量法,即先初始化一个大词表，根据评估准则不断丢弃词表，直到满足限定条件。UniLM算法考虑了句子的不同分词的可能行，因而能够获得不同分词结果所对应的概率。</p><p>我们接下来看看UniLM是如何操作的。</p><p>因为Unigram是先有一个大词表，不断丢弃分词，直到满足条件。这个大词表，比如先用BPE算法求得。</p><p>已知词表是V，xi是句子的分词结果，p(xi)该分词出现的频率，当前分词下句子X的似然值可以表示为P(X)：</p><p><img src="/assets/image-20231215110753917.63682d73.png" alt="image-20231215110753917"></p><blockquote><p>似然常常被用作概率的同义词。但是在<a href="https://so.csdn.net/so/search?q=%E7%BB%9F%E8%AE%A1%E5%AD%A6&amp;spm=1001.2101.3001.7020" target="_blank" rel="noreferrer">统计学</a>中，二者有截然不同的用法，那在统计学中：</p><p>​ 1.概率描述的是：指定参数后，预测即将发生事件的可能性。</p><p>​ 2.似然描述的是：在一直某些观测所得到的结果时，对有关实物的性质的参数进行估计。</p></blockquote><p>假设|D|是语料库中语料数量，s表示句子索引，X是句子， S(x)是句子x的所有候选分割，将语料库中所有句子的所有分词组合形成的概率相加为（似然）：</p><p><img src="/assets/image-20231215111123517.7afca90b.png" alt="image-20231215111123517"></p><blockquote><p>有样本，及样本服从的分布模型。求模型参数，是极大似然估计。</p><p>有样本，及多个分布模型；求样本属于哪个分布、求模型参数，是EM估计。</p></blockquote><p>求解<strong>分布的参数</strong>和<strong>分布(概率)</strong>，即求解最优的<strong>词表</strong>和<strong>分词的概率</strong>，用EM算法。EM算法就是初始化参数+迭代，随意最后的分词过程是：</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki github-dark vp-code-dark"><code><span class="line"><span style="color:#e1e4e8;">1.初始时，建立一个足够大的词表(种子词表)。一般，可用语料中的所有字符加上常见的子字符串初始化词表，也可以通过BPE算法初始化。</span></span>
<span class="line"><span style="color:#e1e4e8;">2.重复这一步直到词表的大小符合预期</span></span>
<span class="line"><span style="color:#e1e4e8;">a.针对当前这一轮的词表，估计每个子词在语料上的概率P(xi)。</span></span>
<span class="line"><span style="color:#e1e4e8;">b.对于每个子词，计算当该子词被从词表中移除时，总的loss降低了多少，记为该子词的loss(xi)。</span></span>
<span class="line"><span style="color:#e1e4e8;">c.将子词按照loss大小进行排序，丢弃一定比例loss最小的子词(比如20%)，保留下来的子词生成新的词表。这里需要注意的是，单字符不能被丢弃，这是为了避免OOV情况。</span></span></code></pre><pre class="shiki github-light vp-code-light"><code><span class="line"><span style="color:#24292e;">1.初始时，建立一个足够大的词表(种子词表)。一般，可用语料中的所有字符加上常见的子字符串初始化词表，也可以通过BPE算法初始化。</span></span>
<span class="line"><span style="color:#24292e;">2.重复这一步直到词表的大小符合预期</span></span>
<span class="line"><span style="color:#24292e;">a.针对当前这一轮的词表，估计每个子词在语料上的概率P(xi)。</span></span>
<span class="line"><span style="color:#24292e;">b.对于每个子词，计算当该子词被从词表中移除时，总的loss降低了多少，记为该子词的loss(xi)。</span></span>
<span class="line"><span style="color:#24292e;">c.将子词按照loss大小进行排序，丢弃一定比例loss最小的子词(比如20%)，保留下来的子词生成新的词表。这里需要注意的是，单字符不能被丢弃，这是为了避免OOV情况。</span></span></code></pre></div><p>可以看出，ULM会保留那些以较高频率出现在很多句子的分词结果中的子词，因为这些子词如果被丢弃，其损失会很大。</p><p>按照EM算法描述是：</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki github-dark vp-code-dark"><code><span class="line"><span style="color:#e1e4e8;">1.初始化模型参数（词表）</span></span>
<span class="line"><span style="color:#e1e4e8;">2.迭代</span></span>
<span class="line"><span style="color:#e1e4e8;">a.计算联合分布的条件概率期望（计算概率）</span></span>
<span class="line"><span style="color:#e1e4e8;">b.c.极大化求解，得到新的参数（词表）</span></span></code></pre><pre class="shiki github-light vp-code-light"><code><span class="line"><span style="color:#24292e;">1.初始化模型参数（词表）</span></span>
<span class="line"><span style="color:#24292e;">2.迭代</span></span>
<span class="line"><span style="color:#24292e;">a.计算联合分布的条件概率期望（计算概率）</span></span>
<span class="line"><span style="color:#24292e;">b.c.极大化求解，得到新的参数（词表）</span></span></code></pre></div><h2 id="代码实现-1" tabindex="-1">代码实现 <a class="header-anchor" href="#代码实现-1" aria-label="Permalink to &quot;代码实现&quot;">​</a></h2><p><strong>如何计算语料库Loss？</strong></p><p>语料库的Loss，就是所有句子Loss，就是所有字的Loss乘以频率</p><mjx-container class="MathJax" jax="SVG" display="true" style="direction:ltr;display:block;text-align:center;margin:1em 0;position:relative;"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:-1.018ex;" xmlns="http://www.w3.org/2000/svg" width="54.116ex" height="3.167ex" role="img" focusable="false" viewBox="0 -950 23919.1 1400" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D43F" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 217 683Q271 680 344 680Q485 680 506 683H518Q524 677 524 674T522 656Q517 641 513 637H475Q406 636 394 628Q387 624 380 600T313 336Q297 271 279 198T252 88L243 52Q243 48 252 48T311 46H328Q360 46 379 47T428 54T478 72T522 106T564 161Q580 191 594 228T611 270Q616 273 628 273H641Q647 264 647 262T627 203T583 83T557 9Q555 4 553 3T537 0T494 -1Q483 -1 418 -1T294 0H116Q32 0 32 10Q32 17 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(681,0)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(1166,0)"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(1635,0)"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(2104,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(2493,0)"><path data-c="1D450" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(2926,0)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(3411,0)"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(3862,0)"><path data-c="1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(4365,0)"><path data-c="1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(4937,0)"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(5406,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(6072.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(7128.6,0)"><path data-c="2211" d="M60 948Q63 950 665 950H1267L1325 815Q1384 677 1388 669H1348L1341 683Q1320 724 1285 761Q1235 809 1174 838T1033 881T882 898T699 902H574H543H251L259 891Q722 258 724 252Q725 250 724 246Q721 243 460 -56L196 -356Q196 -357 407 -357Q459 -357 548 -357T676 -358Q812 -358 896 -353T1063 -332T1204 -283T1307 -196Q1328 -170 1348 -124H1388Q1388 -125 1381 -145T1356 -210T1325 -294L1267 -449L666 -450Q64 -450 61 -448Q55 -446 55 -439Q55 -437 57 -433L590 177Q590 178 557 222T452 366T322 544L56 909L55 924Q55 945 60 948Z" style="stroke-width:3;"></path></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(8739.2,0)"><g data-mml-node="mi"><path data-c="1D43F" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 217 683Q271 680 344 680Q485 680 506 683H518Q524 677 524 674T522 656Q517 641 513 637H475Q406 636 394 628Q387 624 380 600T313 336Q297 271 279 198T252 88L243 52Q243 48 252 48T311 46H328Q360 46 379 47T428 54T478 72T522 106T564 161Q580 191 594 228T611 270Q616 273 628 273H641Q647 264 647 262T627 203T583 83T557 9Q555 4 553 3T537 0T494 -1Q483 -1 418 -1T294 0H116Q32 0 32 10Q32 17 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(681,0)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(1166,0)"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(1635,0)"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(2104,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" style="stroke-width:3;"></path></g><g data-mml-node="msub" transform="translate(2493,0)"><g data-mml-node="mi"><path data-c="1D44B" d="M42 0H40Q26 0 26 11Q26 15 29 27Q33 41 36 43T55 46Q141 49 190 98Q200 108 306 224T411 342Q302 620 297 625Q288 636 234 637H206Q200 643 200 645T202 664Q206 677 212 683H226Q260 681 347 681Q380 681 408 681T453 682T473 682Q490 682 490 671Q490 670 488 658Q484 643 481 640T465 637Q434 634 411 620L488 426L541 485Q646 598 646 610Q646 628 622 635Q617 635 609 637Q594 637 594 648Q594 650 596 664Q600 677 606 683H618Q619 683 643 683T697 681T738 680Q828 680 837 683H845Q852 676 852 672Q850 647 840 637H824Q790 636 763 628T722 611T698 593L687 584Q687 585 592 480L505 384Q505 383 536 304T601 142T638 56Q648 47 699 46Q734 46 734 37Q734 35 732 23Q728 7 725 4T711 1Q708 1 678 1T589 2Q528 2 496 2T461 1Q444 1 444 10Q444 11 446 25Q448 35 450 39T455 44T464 46T480 47T506 54Q523 62 523 64Q522 64 476 181L429 299Q241 95 236 84Q232 76 232 72Q232 53 261 47Q262 47 267 47T273 46Q276 46 277 46T280 45T283 42T284 35Q284 26 282 19Q279 6 276 4T261 1Q258 1 243 1T201 2T142 2Q64 2 42 0Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(861,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z" style="stroke-width:3;"></path></g></g><g data-mml-node="mo" transform="translate(3648,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" style="stroke-width:3;"></path></g></g><g data-mml-node="mo" transform="translate(13054,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(14109.7,0)"><path data-c="2211" d="M60 948Q63 950 665 950H1267L1325 815Q1384 677 1388 669H1348L1341 683Q1320 724 1285 761Q1235 809 1174 838T1033 881T882 898T699 902H574H543H251L259 891Q722 258 724 252Q725 250 724 246Q721 243 460 -56L196 -356Q196 -357 407 -357Q459 -357 548 -357T676 -358Q812 -358 896 -353T1063 -332T1204 -283T1307 -196Q1328 -170 1348 -124H1388Q1388 -125 1381 -145T1356 -210T1325 -294L1267 -449L666 -450Q64 -450 61 -448Q55 -446 55 -439Q55 -437 57 -433L590 177Q590 178 557 222T452 366T322 544L56 909L55 924Q55 945 60 948Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(15720.4,0)"><path data-c="1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(16270.4,0)"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(16721.4,0)"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(17187.4,0)"><path data-c="1D45E" d="M33 157Q33 258 109 349T280 441Q340 441 372 389Q373 390 377 395T388 406T404 418Q438 442 450 442Q454 442 457 439T460 434Q460 425 391 149Q320 -135 320 -139Q320 -147 365 -148H390Q396 -156 396 -157T393 -175Q389 -188 383 -194H370Q339 -192 262 -192Q234 -192 211 -192T174 -192T157 -193Q143 -193 143 -185Q143 -182 145 -170Q149 -154 152 -151T172 -148Q220 -148 230 -141Q238 -136 258 -53T279 32Q279 33 272 29Q224 -10 172 -10Q117 -10 75 30T33 157ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z" style="stroke-width:3;"></path></g><g data-mml-node="msub" transform="translate(17647.4,0)"><g data-mml-node="mi"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(502,-150) scale(0.707)"><path data-c="1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z" style="stroke-width:3;"></path></g></g><g data-mml-node="mi" transform="translate(18490.7,0)"><path data-c="1D43F" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 217 683Q271 680 344 680Q485 680 506 683H518Q524 677 524 674T522 656Q517 641 513 637H475Q406 636 394 628Q387 624 380 600T313 336Q297 271 279 198T252 88L243 52Q243 48 252 48T311 46H328Q360 46 379 47T428 54T478 72T522 106T564 161Q580 191 594 228T611 270Q616 273 628 273H641Q647 264 647 262T627 203T583 83T557 9Q555 4 553 3T537 0T494 -1Q483 -1 418 -1T294 0H116Q32 0 32 10Q32 17 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(19171.7,0)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(19656.7,0)"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(20125.7,0)"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(20594.7,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(20983.7,0)"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(21699.7,0)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(22184.7,0)"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z" style="stroke-width:3;"></path></g><g data-mml-node="msub" transform="translate(22635.7,0)"><g data-mml-node="mi"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(553,-150) scale(0.707)"><path data-c="1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z" style="stroke-width:3;"></path></g></g><g data-mml-node="mo" transform="translate(23530.1,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" style="stroke-width:3;"></path></g></g></g></svg><mjx-assistive-mml unselectable="on" display="block" style="top:0px;left:0px;clip:rect(1px, 1px, 1px, 1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0px 0px 0px;border:0px;display:block;overflow:hidden;width:100%;"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mi>L</mi><mi>o</mi><mi>s</mi><mi>s</mi><mo stretchy="false">(</mo><mi>c</mi><mi>o</mi><mi>r</mi><mi>p</mi><mi>u</mi><mi>s</mi><mo stretchy="false">)</mo><mo>=</mo><mo data-mjx-texclass="OP">∑</mo><mrow data-mjx-texclass="ORD"><mi>L</mi><mi>o</mi><mi>s</mi><mi>s</mi><mo stretchy="false">(</mo><msub><mi>X</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow><mo>=</mo><mo data-mjx-texclass="OP">∑</mo><mi>f</mi><mi>r</mi><mi>e</mi><mi>q</mi><msub><mi>s</mi><mi>j</mi></msub><mi>L</mi><mi>o</mi><mi>s</mi><mi>s</mi><mo stretchy="false">(</mo><mi>w</mi><mi>o</mi><mi>r</mi><msub><mi>d</mi><mi>j</mi></msub><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container><p><strong>如何计算字的最大Loss？</strong></p><p>就是字的分词的概率</p><mjx-container class="MathJax" jax="SVG" display="true" style="direction:ltr;display:block;text-align:center;margin:1em 0;position:relative;"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:-0.566ex;" xmlns="http://www.w3.org/2000/svg" width="34.456ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 15229.5 1000" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D43F" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 217 683Q271 680 344 680Q485 680 506 683H518Q524 677 524 674T522 656Q517 641 513 637H475Q406 636 394 628Q387 624 380 600T313 336Q297 271 279 198T252 88L243 52Q243 48 252 48T311 46H328Q360 46 379 47T428 54T478 72T522 106T564 161Q580 191 594 228T611 270Q616 273 628 273H641Q647 264 647 262T627 203T583 83T557 9Q555 4 553 3T537 0T494 -1Q483 -1 418 -1T294 0H116Q32 0 32 10Q32 17 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(681,0)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(1166,0)"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(1635,0)"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(2104,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(2493,0)"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(3209,0)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(3694,0)"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z" style="stroke-width:3;"></path></g><g data-mml-node="msub" transform="translate(4145,0)"><g data-mml-node="mi"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(553,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z" style="stroke-width:3;"></path></g></g><g data-mml-node="mo" transform="translate(4992,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(5658.7,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(6714.5,0)"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(7465.5,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(7854.5,0)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(8215.5,0)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(8700.5,0)"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(9221.5,0)"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(9687.5,0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(10287.5,0)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(10632.5,0)"><path data-c="1D467" d="M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346T162 335T155 324L153 320Q150 317 138 317Q117 317 117 325Q117 330 120 339Q133 378 163 406T229 440Q241 442 246 442Q271 442 291 425T329 392T367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434Q468 430 463 420T449 399T432 377T418 358L411 349Q368 298 275 214T160 106L148 94L163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143Q443 138 442 134Q425 72 376 31T278 -11Q252 -11 232 6T193 40T155 57Q111 57 76 -3Q70 -11 59 -11H54H41Q35 -5 35 -2Q35 13 93 84Q132 129 225 214T340 322Q352 338 347 338Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(11097.5,0)"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(11563.5,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(11952.5,0)"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(12668.5,0)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(13153.5,0)"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z" style="stroke-width:3;"></path></g><g data-mml-node="msub" transform="translate(13604.5,0)"><g data-mml-node="mi"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(553,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z" style="stroke-width:3;"></path></g></g><g data-mml-node="mo" transform="translate(14451.5,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(14840.5,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" style="stroke-width:3;"></path></g></g></g></svg><mjx-assistive-mml unselectable="on" display="block" style="top:0px;left:0px;clip:rect(1px, 1px, 1px, 1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0px 0px 0px;border:0px;display:block;overflow:hidden;width:100%;"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mi>L</mi><mi>o</mi><mi>s</mi><mi>s</mi><mo stretchy="false">(</mo><mi>w</mi><mi>o</mi><mi>r</mi><msub><mi>d</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo>=</mo><mi>P</mi><mo stretchy="false">(</mo><mi>t</mi><mi>o</mi><mi>k</mi><mi>e</mi><mi>n</mi><mi>i</mi><mi>z</mi><mi>e</mi><mo stretchy="false">(</mo><mi>w</mi><mi>o</mi><mi>r</mi><msub><mi>d</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container><p><strong>如何分词？如何计算分词的概率？</strong></p><p>已知分词表，及分词的概率，可以使用维特比算法，求出字的分词。</p><p><strong>代码：</strong></p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki github-dark vp-code-dark"><code><span class="line"><span style="color:#e1e4e8;">from typing import Dict, List, Tuple</span></span>
<span class="line"><span style="color:#e1e4e8;">import copy</span></span>
<span class="line"><span style="color:#e1e4e8;">import math</span></span>
<span class="line"><span style="color:#e1e4e8;"></span></span>
<span class="line"><span style="color:#e1e4e8;"></span></span>
<span class="line"><span style="color:#e1e4e8;">def get_word_freqs(pre_tokenized_res):</span></span>
<span class="line"><span style="color:#e1e4e8;">    word_freqs = {}</span></span>
<span class="line"><span style="color:#e1e4e8;">    for word in pre_tokenized_res:</span></span>
<span class="line"><span style="color:#e1e4e8;">        word_freqs[word] = word_freqs.get(word, 0) + 1</span></span>
<span class="line"><span style="color:#e1e4e8;">    return word_freqs</span></span>
<span class="line"><span style="color:#e1e4e8;"></span></span>
<span class="line"><span style="color:#e1e4e8;"></span></span>
<span class="line"><span style="color:#e1e4e8;">def init_vocab(word_freqs, init_vocab_size):</span></span>
<span class="line"><span style="color:#e1e4e8;">    char_freqs = {}</span></span>
<span class="line"><span style="color:#e1e4e8;">    subword_freqs = {}</span></span>
<span class="line"><span style="color:#e1e4e8;">    for word, freq in word_freqs.items():</span></span>
<span class="line"><span style="color:#e1e4e8;">        for i in range(len(word)):</span></span>
<span class="line"><span style="color:#e1e4e8;">            char_freqs[word[i]] = char_freqs.get(word[i], 0) + freq</span></span>
<span class="line"><span style="color:#e1e4e8;">            # 子词的长度至少是2，并且word[i:j]是左闭右开，因此j最多要取到len(word)</span></span>
<span class="line"><span style="color:#e1e4e8;">            for j in range(i + 2, len(word) + 1):</span></span>
<span class="line"><span style="color:#e1e4e8;">                # 必须是严格子词，即不能包含自身</span></span>
<span class="line"><span style="color:#e1e4e8;">                if j - i &lt; len(word):</span></span>
<span class="line"><span style="color:#e1e4e8;">                    subword_freqs[word[i:j]] = subword_freqs.get(word[i:j], 0) + freq</span></span>
<span class="line"><span style="color:#e1e4e8;">    char_freqs = sorted(char_freqs.items())</span></span>
<span class="line"><span style="color:#e1e4e8;">    subword_freqs = sorted(subword_freqs.items(), key=lambda x: x[1], reverse=True)</span></span>
<span class="line"><span style="color:#e1e4e8;">    assert init_vocab_size &gt; len(char_freqs)</span></span>
<span class="line"><span style="color:#e1e4e8;">    vocab_with_freqs = char_freqs + subword_freqs[:init_vocab_size - len(char_freqs)]</span></span>
<span class="line"><span style="color:#e1e4e8;">    return dict(vocab_with_freqs)</span></span>
<span class="line"><span style="color:#e1e4e8;"></span></span>
<span class="line"><span style="color:#e1e4e8;"></span></span>
<span class="line"><span style="color:#e1e4e8;"># 计算分词loss</span></span>
<span class="line"><span style="color:#e1e4e8;">def process_vocab(vocab_with_freqs: Dict[str, int]) -&gt; Dict[str, float]:</span></span>
<span class="line"><span style="color:#e1e4e8;">    total_sum = sum(vocab_with_freqs.values())</span></span>
<span class="line"><span style="color:#e1e4e8;">    vocab_with_loss = {token: -math.log(freq / total_sum) for token, freq in vocab_with_freqs.items()}</span></span>
<span class="line"><span style="color:#e1e4e8;">    return vocab_with_loss</span></span>
<span class="line"><span style="color:#e1e4e8;"></span></span>
<span class="line"><span style="color:#e1e4e8;"></span></span>
<span class="line"><span style="color:#e1e4e8;"># 已知分词表+分词频率，可以用维特比算法分词</span></span>
<span class="line"><span style="color:#e1e4e8;">def tokenize_word(word: str, vocab_with_loss: Dict[str, float]):</span></span>
<span class="line"><span style="color:#e1e4e8;">    dp = [{&#39;start&#39;: None, &#39;loss&#39;: None} for _ in range(len(word) + 1)]</span></span>
<span class="line"><span style="color:#e1e4e8;">    dp[0] = {&#39;start&#39;: -1, &#39;loss&#39;: 0}</span></span>
<span class="line"><span style="color:#e1e4e8;"></span></span>
<span class="line"><span style="color:#e1e4e8;">    for i in range(len(word)):</span></span>
<span class="line"><span style="color:#e1e4e8;">        for j in range(i + 1, len(word) + 1):</span></span>
<span class="line"><span style="color:#e1e4e8;">            token = word[i:j]</span></span>
<span class="line"><span style="color:#e1e4e8;">            if token in vocab_with_loss and dp[i][&#39;loss&#39;] is not None:</span></span>
<span class="line"><span style="color:#e1e4e8;">                new_loss = dp[i][&#39;loss&#39;] + vocab_with_loss[token]</span></span>
<span class="line"><span style="color:#e1e4e8;">                if dp[j][&#39;loss&#39;] is None or new_loss &lt; dp[j][&#39;loss&#39;]:</span></span>
<span class="line"><span style="color:#e1e4e8;">                    dp[j] = {&#39;start&#39;: i, &#39;loss&#39;: new_loss}</span></span>
<span class="line"><span style="color:#e1e4e8;"></span></span>
<span class="line"><span style="color:#e1e4e8;">    word_loss = dp[-1][&#39;loss&#39;]</span></span>
<span class="line"><span style="color:#e1e4e8;">    if word_loss is None:</span></span>
<span class="line"><span style="color:#e1e4e8;">        return [&#39;[UNK]&#39;], word_loss</span></span>
<span class="line"><span style="color:#e1e4e8;"></span></span>
<span class="line"><span style="color:#e1e4e8;">    start, end = dp[-1][&#39;start&#39;], len(word)</span></span>
<span class="line"><span style="color:#e1e4e8;">    res = []</span></span>
<span class="line"><span style="color:#e1e4e8;">    while ~start:</span></span>
<span class="line"><span style="color:#e1e4e8;">        res.append(word[start:end])</span></span>
<span class="line"><span style="color:#e1e4e8;">        end = start</span></span>
<span class="line"><span style="color:#e1e4e8;">        start = dp[start][&#39;start&#39;]</span></span>
<span class="line"><span style="color:#e1e4e8;"></span></span>
<span class="line"><span style="color:#e1e4e8;">    res.reverse()</span></span>
<span class="line"><span style="color:#e1e4e8;">    return res, word_loss</span></span>
<span class="line"><span style="color:#e1e4e8;"></span></span>
<span class="line"><span style="color:#e1e4e8;"></span></span>
<span class="line"><span style="color:#e1e4e8;"># 计算语料库Loss = sum(freq(x) * tokenize_word(x))</span></span>
<span class="line"><span style="color:#e1e4e8;">def corpus_loss(word_freqs, vocab_with_loss):</span></span>
<span class="line"><span style="color:#e1e4e8;">    loss = 0</span></span>
<span class="line"><span style="color:#e1e4e8;">    for word, freq in word_freqs.items():</span></span>
<span class="line"><span style="color:#e1e4e8;">        loss += freq * tokenize_word(word, vocab_with_loss)[1]</span></span>
<span class="line"><span style="color:#e1e4e8;">    return loss</span></span>
<span class="line"><span style="color:#e1e4e8;"></span></span>
<span class="line"><span style="color:#e1e4e8;"></span></span>
<span class="line"><span style="color:#e1e4e8;"># 计算每个分词的score = Loss_i - Loss</span></span>
<span class="line"><span style="color:#e1e4e8;">def compute_scores(word_freqs, vocab_with_loss) -&gt; List[str]:</span></span>
<span class="line"><span style="color:#e1e4e8;">    scores = {}</span></span>
<span class="line"><span style="color:#e1e4e8;">    pre_loss = corpus_loss(word_freqs, vocab_with_loss)</span></span>
<span class="line"><span style="color:#e1e4e8;">    for token in vocab_with_loss:</span></span>
<span class="line"><span style="color:#e1e4e8;">        if len(token) == 1: continue</span></span>
<span class="line"><span style="color:#e1e4e8;">        vocab_with_loss_ = copy.deepcopy(vocab_with_loss)</span></span>
<span class="line"><span style="color:#e1e4e8;">        vocab_with_loss_.pop(token)</span></span>
<span class="line"><span style="color:#e1e4e8;">        cur_loss = corpus_loss(word_freqs, vocab_with_loss_)</span></span>
<span class="line"><span style="color:#e1e4e8;">        scores[token] = cur_loss - pre_loss</span></span>
<span class="line"><span style="color:#e1e4e8;">    return sorted(scores, key=lambda x: x[1])</span></span>
<span class="line"><span style="color:#e1e4e8;"></span></span>
<span class="line"><span style="color:#e1e4e8;"></span></span>
<span class="line"><span style="color:#e1e4e8;">&quot;&quot;&quot;</span></span>
<span class="line"><span style="color:#e1e4e8;">I love you very much</span></span>
<span class="line"><span style="color:#e1e4e8;">I am very hungry</span></span>
<span class="line"><span style="color:#e1e4e8;">&quot;&quot;&quot;</span></span>
<span class="line"><span style="color:#e1e4e8;">corpus = [&#39;I&#39;, &#39;love&#39;, &#39;you&#39;, &#39;very&#39;, &#39;much&#39;, &#39;I&#39;, &#39;am&#39;, &#39;very&#39;, &#39;hungry&#39;]</span></span>
<span class="line"><span style="color:#e1e4e8;">word_freqs = get_word_freqs(corpus)</span></span>
<span class="line"><span style="color:#e1e4e8;">print(f&#39;word_freqs: {word_freqs}&#39;)</span></span>
<span class="line"><span style="color:#e1e4e8;"></span></span>
<span class="line"><span style="color:#e1e4e8;"># 初始化词表大小</span></span>
<span class="line"><span style="color:#e1e4e8;">init_vocab_size = 50</span></span>
<span class="line"><span style="color:#e1e4e8;">vocab_with_freqs = init_vocab(word_freqs, init_vocab_size)</span></span>
<span class="line"><span style="color:#e1e4e8;">print(f&#39;vocab_with_freqs: {vocab_with_freqs}&#39;)</span></span>
<span class="line"><span style="color:#e1e4e8;">vocab_with_loss = process_vocab(vocab_with_freqs)</span></span>
<span class="line"><span style="color:#e1e4e8;">print(f&#39;vocab_with_loss: {vocab_with_loss}&#39;)</span></span>
<span class="line"><span style="color:#e1e4e8;"></span></span>
<span class="line"><span style="color:#e1e4e8;">vocab_size = 15 # 最终词表大小</span></span>
<span class="line"><span style="color:#e1e4e8;">p = 0.1         # 舍弃比率</span></span>
<span class="line"><span style="color:#e1e4e8;">epoch = 0</span></span>
<span class="line"><span style="color:#e1e4e8;">while len(vocab_with_loss) &gt; vocab_size:</span></span>
<span class="line"><span style="color:#e1e4e8;">    print(&#39;-----------&#39;)</span></span>
<span class="line"><span style="color:#e1e4e8;">    print(f&#39;epoch: {epoch}&#39;)</span></span>
<span class="line"><span style="color:#e1e4e8;">    epoch += 1</span></span>
<span class="line"><span style="color:#e1e4e8;">    scores = compute_scores(word_freqs, vocab_with_loss)</span></span>
<span class="line"><span style="color:#e1e4e8;">    print(f&#39;scores: {scores}&#39;)</span></span>
<span class="line"><span style="color:#e1e4e8;">    num_to_remove = min(int(len(vocab_with_loss) * p), len(scores))</span></span>
<span class="line"><span style="color:#e1e4e8;">    if num_to_remove == 0:</span></span>
<span class="line"><span style="color:#e1e4e8;">        break</span></span>
<span class="line"><span style="color:#e1e4e8;">    for i in range(num_to_remove):</span></span>
<span class="line"><span style="color:#e1e4e8;">        vocab_with_freqs.pop(scores[i])</span></span>
<span class="line"><span style="color:#e1e4e8;">    vocab_with_loss = process_vocab(vocab_with_freqs)</span></span>
<span class="line"><span style="color:#e1e4e8;">    print(f&#39;vocab_with_freqs: {vocab_with_freqs}&#39;)</span></span>
<span class="line"><span style="color:#e1e4e8;">    print(f&#39;vocab_with_loss: {vocab_with_loss}&#39;)</span></span>
<span class="line"><span style="color:#e1e4e8;"></span></span>
<span class="line"><span style="color:#e1e4e8;">print(&#39;-----------&#39;)</span></span>
<span class="line"><span style="color:#e1e4e8;">print(f&#39;vocab_with_loss: {vocab_with_loss}&#39;)</span></span>
<span class="line"><span style="color:#e1e4e8;">print(len(vocab_with_loss))</span></span></code></pre><pre class="shiki github-light vp-code-light"><code><span class="line"><span style="color:#24292e;">from typing import Dict, List, Tuple</span></span>
<span class="line"><span style="color:#24292e;">import copy</span></span>
<span class="line"><span style="color:#24292e;">import math</span></span>
<span class="line"><span style="color:#24292e;"></span></span>
<span class="line"><span style="color:#24292e;"></span></span>
<span class="line"><span style="color:#24292e;">def get_word_freqs(pre_tokenized_res):</span></span>
<span class="line"><span style="color:#24292e;">    word_freqs = {}</span></span>
<span class="line"><span style="color:#24292e;">    for word in pre_tokenized_res:</span></span>
<span class="line"><span style="color:#24292e;">        word_freqs[word] = word_freqs.get(word, 0) + 1</span></span>
<span class="line"><span style="color:#24292e;">    return word_freqs</span></span>
<span class="line"><span style="color:#24292e;"></span></span>
<span class="line"><span style="color:#24292e;"></span></span>
<span class="line"><span style="color:#24292e;">def init_vocab(word_freqs, init_vocab_size):</span></span>
<span class="line"><span style="color:#24292e;">    char_freqs = {}</span></span>
<span class="line"><span style="color:#24292e;">    subword_freqs = {}</span></span>
<span class="line"><span style="color:#24292e;">    for word, freq in word_freqs.items():</span></span>
<span class="line"><span style="color:#24292e;">        for i in range(len(word)):</span></span>
<span class="line"><span style="color:#24292e;">            char_freqs[word[i]] = char_freqs.get(word[i], 0) + freq</span></span>
<span class="line"><span style="color:#24292e;">            # 子词的长度至少是2，并且word[i:j]是左闭右开，因此j最多要取到len(word)</span></span>
<span class="line"><span style="color:#24292e;">            for j in range(i + 2, len(word) + 1):</span></span>
<span class="line"><span style="color:#24292e;">                # 必须是严格子词，即不能包含自身</span></span>
<span class="line"><span style="color:#24292e;">                if j - i &lt; len(word):</span></span>
<span class="line"><span style="color:#24292e;">                    subword_freqs[word[i:j]] = subword_freqs.get(word[i:j], 0) + freq</span></span>
<span class="line"><span style="color:#24292e;">    char_freqs = sorted(char_freqs.items())</span></span>
<span class="line"><span style="color:#24292e;">    subword_freqs = sorted(subword_freqs.items(), key=lambda x: x[1], reverse=True)</span></span>
<span class="line"><span style="color:#24292e;">    assert init_vocab_size &gt; len(char_freqs)</span></span>
<span class="line"><span style="color:#24292e;">    vocab_with_freqs = char_freqs + subword_freqs[:init_vocab_size - len(char_freqs)]</span></span>
<span class="line"><span style="color:#24292e;">    return dict(vocab_with_freqs)</span></span>
<span class="line"><span style="color:#24292e;"></span></span>
<span class="line"><span style="color:#24292e;"></span></span>
<span class="line"><span style="color:#24292e;"># 计算分词loss</span></span>
<span class="line"><span style="color:#24292e;">def process_vocab(vocab_with_freqs: Dict[str, int]) -&gt; Dict[str, float]:</span></span>
<span class="line"><span style="color:#24292e;">    total_sum = sum(vocab_with_freqs.values())</span></span>
<span class="line"><span style="color:#24292e;">    vocab_with_loss = {token: -math.log(freq / total_sum) for token, freq in vocab_with_freqs.items()}</span></span>
<span class="line"><span style="color:#24292e;">    return vocab_with_loss</span></span>
<span class="line"><span style="color:#24292e;"></span></span>
<span class="line"><span style="color:#24292e;"></span></span>
<span class="line"><span style="color:#24292e;"># 已知分词表+分词频率，可以用维特比算法分词</span></span>
<span class="line"><span style="color:#24292e;">def tokenize_word(word: str, vocab_with_loss: Dict[str, float]):</span></span>
<span class="line"><span style="color:#24292e;">    dp = [{&#39;start&#39;: None, &#39;loss&#39;: None} for _ in range(len(word) + 1)]</span></span>
<span class="line"><span style="color:#24292e;">    dp[0] = {&#39;start&#39;: -1, &#39;loss&#39;: 0}</span></span>
<span class="line"><span style="color:#24292e;"></span></span>
<span class="line"><span style="color:#24292e;">    for i in range(len(word)):</span></span>
<span class="line"><span style="color:#24292e;">        for j in range(i + 1, len(word) + 1):</span></span>
<span class="line"><span style="color:#24292e;">            token = word[i:j]</span></span>
<span class="line"><span style="color:#24292e;">            if token in vocab_with_loss and dp[i][&#39;loss&#39;] is not None:</span></span>
<span class="line"><span style="color:#24292e;">                new_loss = dp[i][&#39;loss&#39;] + vocab_with_loss[token]</span></span>
<span class="line"><span style="color:#24292e;">                if dp[j][&#39;loss&#39;] is None or new_loss &lt; dp[j][&#39;loss&#39;]:</span></span>
<span class="line"><span style="color:#24292e;">                    dp[j] = {&#39;start&#39;: i, &#39;loss&#39;: new_loss}</span></span>
<span class="line"><span style="color:#24292e;"></span></span>
<span class="line"><span style="color:#24292e;">    word_loss = dp[-1][&#39;loss&#39;]</span></span>
<span class="line"><span style="color:#24292e;">    if word_loss is None:</span></span>
<span class="line"><span style="color:#24292e;">        return [&#39;[UNK]&#39;], word_loss</span></span>
<span class="line"><span style="color:#24292e;"></span></span>
<span class="line"><span style="color:#24292e;">    start, end = dp[-1][&#39;start&#39;], len(word)</span></span>
<span class="line"><span style="color:#24292e;">    res = []</span></span>
<span class="line"><span style="color:#24292e;">    while ~start:</span></span>
<span class="line"><span style="color:#24292e;">        res.append(word[start:end])</span></span>
<span class="line"><span style="color:#24292e;">        end = start</span></span>
<span class="line"><span style="color:#24292e;">        start = dp[start][&#39;start&#39;]</span></span>
<span class="line"><span style="color:#24292e;"></span></span>
<span class="line"><span style="color:#24292e;">    res.reverse()</span></span>
<span class="line"><span style="color:#24292e;">    return res, word_loss</span></span>
<span class="line"><span style="color:#24292e;"></span></span>
<span class="line"><span style="color:#24292e;"></span></span>
<span class="line"><span style="color:#24292e;"># 计算语料库Loss = sum(freq(x) * tokenize_word(x))</span></span>
<span class="line"><span style="color:#24292e;">def corpus_loss(word_freqs, vocab_with_loss):</span></span>
<span class="line"><span style="color:#24292e;">    loss = 0</span></span>
<span class="line"><span style="color:#24292e;">    for word, freq in word_freqs.items():</span></span>
<span class="line"><span style="color:#24292e;">        loss += freq * tokenize_word(word, vocab_with_loss)[1]</span></span>
<span class="line"><span style="color:#24292e;">    return loss</span></span>
<span class="line"><span style="color:#24292e;"></span></span>
<span class="line"><span style="color:#24292e;"></span></span>
<span class="line"><span style="color:#24292e;"># 计算每个分词的score = Loss_i - Loss</span></span>
<span class="line"><span style="color:#24292e;">def compute_scores(word_freqs, vocab_with_loss) -&gt; List[str]:</span></span>
<span class="line"><span style="color:#24292e;">    scores = {}</span></span>
<span class="line"><span style="color:#24292e;">    pre_loss = corpus_loss(word_freqs, vocab_with_loss)</span></span>
<span class="line"><span style="color:#24292e;">    for token in vocab_with_loss:</span></span>
<span class="line"><span style="color:#24292e;">        if len(token) == 1: continue</span></span>
<span class="line"><span style="color:#24292e;">        vocab_with_loss_ = copy.deepcopy(vocab_with_loss)</span></span>
<span class="line"><span style="color:#24292e;">        vocab_with_loss_.pop(token)</span></span>
<span class="line"><span style="color:#24292e;">        cur_loss = corpus_loss(word_freqs, vocab_with_loss_)</span></span>
<span class="line"><span style="color:#24292e;">        scores[token] = cur_loss - pre_loss</span></span>
<span class="line"><span style="color:#24292e;">    return sorted(scores, key=lambda x: x[1])</span></span>
<span class="line"><span style="color:#24292e;"></span></span>
<span class="line"><span style="color:#24292e;"></span></span>
<span class="line"><span style="color:#24292e;">&quot;&quot;&quot;</span></span>
<span class="line"><span style="color:#24292e;">I love you very much</span></span>
<span class="line"><span style="color:#24292e;">I am very hungry</span></span>
<span class="line"><span style="color:#24292e;">&quot;&quot;&quot;</span></span>
<span class="line"><span style="color:#24292e;">corpus = [&#39;I&#39;, &#39;love&#39;, &#39;you&#39;, &#39;very&#39;, &#39;much&#39;, &#39;I&#39;, &#39;am&#39;, &#39;very&#39;, &#39;hungry&#39;]</span></span>
<span class="line"><span style="color:#24292e;">word_freqs = get_word_freqs(corpus)</span></span>
<span class="line"><span style="color:#24292e;">print(f&#39;word_freqs: {word_freqs}&#39;)</span></span>
<span class="line"><span style="color:#24292e;"></span></span>
<span class="line"><span style="color:#24292e;"># 初始化词表大小</span></span>
<span class="line"><span style="color:#24292e;">init_vocab_size = 50</span></span>
<span class="line"><span style="color:#24292e;">vocab_with_freqs = init_vocab(word_freqs, init_vocab_size)</span></span>
<span class="line"><span style="color:#24292e;">print(f&#39;vocab_with_freqs: {vocab_with_freqs}&#39;)</span></span>
<span class="line"><span style="color:#24292e;">vocab_with_loss = process_vocab(vocab_with_freqs)</span></span>
<span class="line"><span style="color:#24292e;">print(f&#39;vocab_with_loss: {vocab_with_loss}&#39;)</span></span>
<span class="line"><span style="color:#24292e;"></span></span>
<span class="line"><span style="color:#24292e;">vocab_size = 15 # 最终词表大小</span></span>
<span class="line"><span style="color:#24292e;">p = 0.1         # 舍弃比率</span></span>
<span class="line"><span style="color:#24292e;">epoch = 0</span></span>
<span class="line"><span style="color:#24292e;">while len(vocab_with_loss) &gt; vocab_size:</span></span>
<span class="line"><span style="color:#24292e;">    print(&#39;-----------&#39;)</span></span>
<span class="line"><span style="color:#24292e;">    print(f&#39;epoch: {epoch}&#39;)</span></span>
<span class="line"><span style="color:#24292e;">    epoch += 1</span></span>
<span class="line"><span style="color:#24292e;">    scores = compute_scores(word_freqs, vocab_with_loss)</span></span>
<span class="line"><span style="color:#24292e;">    print(f&#39;scores: {scores}&#39;)</span></span>
<span class="line"><span style="color:#24292e;">    num_to_remove = min(int(len(vocab_with_loss) * p), len(scores))</span></span>
<span class="line"><span style="color:#24292e;">    if num_to_remove == 0:</span></span>
<span class="line"><span style="color:#24292e;">        break</span></span>
<span class="line"><span style="color:#24292e;">    for i in range(num_to_remove):</span></span>
<span class="line"><span style="color:#24292e;">        vocab_with_freqs.pop(scores[i])</span></span>
<span class="line"><span style="color:#24292e;">    vocab_with_loss = process_vocab(vocab_with_freqs)</span></span>
<span class="line"><span style="color:#24292e;">    print(f&#39;vocab_with_freqs: {vocab_with_freqs}&#39;)</span></span>
<span class="line"><span style="color:#24292e;">    print(f&#39;vocab_with_loss: {vocab_with_loss}&#39;)</span></span>
<span class="line"><span style="color:#24292e;"></span></span>
<span class="line"><span style="color:#24292e;">print(&#39;-----------&#39;)</span></span>
<span class="line"><span style="color:#24292e;">print(f&#39;vocab_with_loss: {vocab_with_loss}&#39;)</span></span>
<span class="line"><span style="color:#24292e;">print(len(vocab_with_loss))</span></span></code></pre></div></div></div></main><footer class="VPDocFooter" data-v-6b87e69f data-v-ef5dee53><!--[--><!--]--><div class="edit-info" data-v-ef5dee53><div class="edit-link" data-v-ef5dee53><a class="VPLink link vp-external-link-icon no-icon edit-link-button" href="https://github.com/mingriyingying/mingriyingying.github.io/blob/master/docs/articles/Blog/04分词方法.md" target="_blank" rel="noreferrer" data-v-ef5dee53><!--[--><svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 24 24" class="edit-link-icon" aria-label="edit icon" data-v-ef5dee53><path d="M18,23H4c-1.7,0-3-1.3-3-3V6c0-1.7,1.3-3,3-3h7c0.6,0,1,0.4,1,1s-0.4,1-1,1H4C3.4,5,3,5.4,3,6v14c0,0.6,0.4,1,1,1h14c0.6,0,1-0.4,1-1v-7c0-0.6,0.4-1,1-1s1,0.4,1,1v7C21,21.7,19.7,23,18,23z"></path><path d="M8,17c-0.3,0-0.5-0.1-0.7-0.3C7,16.5,6.9,16.1,7,15.8l1-4c0-0.2,0.1-0.3,0.3-0.5l9.5-9.5c1.2-1.2,3.2-1.2,4.4,0c1.2,1.2,1.2,3.2,0,4.4l-9.5,9.5c-0.1,0.1-0.3,0.2-0.5,0.3l-4,1C8.2,17,8.1,17,8,17zM9.9,12.5l-0.5,2.1l2.1-0.5l9.3-9.3c0.4-0.4,0.4-1.1,0-1.6c-0.4-0.4-1.2-0.4-1.6,0l0,0L9.9,12.5z M18.5,2.5L18.5,2.5L18.5,2.5z"></path></svg> Edit this page on GitHub<!--]--></a></div><!----></div><!----></footer><!--[--><!--]--></div></div></div><!--[--><!--]--></div></div><!----><!--[--><!--]--></div></div>
    <script>window.__VP_HASH_MAP__=JSON.parse("{\"articles_algorithm_00数学基础_01线性代数的本质.md\":\"23bedfb0\",\"about_index.md\":\"60b46048\",\"articles_algorithm_00数学基础_index.md\":\"1f342d0d\",\"articles_algorithm_01python语法_工具_index.md\":\"8ad0cc38\",\"articles_algorithm_10白板推导系列_01介绍.md\":\"6ae311a1\",\"articles_algorithm_01python语法_工具_01python3语法.md\":\"6a181f84\",\"articles_algorithm_01python语法_工具_03python常用库.md\":\"8065203a\",\"articles_algorithm_12机器学习笔记_00算法性能度量.md\":\"7e1b17b0\",\"articles_algorithm_12机器学习笔记_01概述.md\":\"8303897d\",\"articles_algorithm_10白板推导系列_06支持向量机.md\":\"f6f8da89\",\"articles_algorithm_00数学基础_03统计学基础.md\":\"40fc88d1\",\"articles_algorithm_10白板推导系列_22nn.md\":\"22f28813\",\"articles_algorithm_10白板推导系列_index.md\":\"7e03b24d\",\"articles_algorithm_11强化学习_01强化学习环境.md\":\"c31ab779\",\"articles_algorithm_11强化学习_01深度强化学习-李宏毅.md\":\"a341f12d\",\"articles_algorithm_12机器学习笔记_index.md\":\"f281a514\",\"articles_algorithm_12机器学习笔记_16lightgbm.md\":\"b9ec8433\",\"articles_algorithm_12机器学习笔记_15xgboost.md\":\"1926e6b4\",\"articles_algorithm_12机器学习笔记_14lightgbm.md\":\"f59e6c64\",\"articles_algorithm_13llm_00llm发展2017-2013.md\":\"638c1e7b\",\"articles_algorithm_13llm_00综述.md\":\"908399be\",\"articles_algorithm_12机器学习笔记_17catboost.md\":\"22bab25d\",\"articles_algorithm_13llm_00多模态.md\":\"8f3dfb00\",\"articles_algorithm_13llm_01prtnet.md\":\"6ef4eb8b\",\"articles_algorithm_13llm_01seq2seq和attention.md\":\"39e8f779\",\"articles_algorithm_13llm_02word2vec.md\":\"4c41a321\",\"articles_algorithm_13llm_03transformerxl.md\":\"1bdd1c9e\",\"articles_algorithm_13llm_03transformer详解.md\":\"bd4e0b95\",\"articles_algorithm_13llm_04bert1进化史.md\":\"89a60318\",\"articles_algorithm_13llm_05gpt02模型.md\":\"4fbc5006\",\"articles_algorithm_13llm_06mtdnn.md\":\"8e857fc8\",\"articles_algorithm_13llm_04bert2详解.md\":\"cf590553\",\"articles_algorithm_13llm_06xlnet.md\":\"4ffbcf8e\",\"articles_algorithm_13llm_08albert.md\":\"26b974a1\",\"articles_algorithm_13llm_09electra.md\":\"296753e6\",\"articles_algorithm_13llm_10t5.md\":\"082bb6dd\",\"articles_algorithm_13llm_11t5.md\":\"3dd0f536\",\"articles_algorithm_13llm_11学习attention mask.md\":\"141f3813\",\"articles_algorithm_13llm_13flan.md\":\"87f87797\",\"articles_algorithm_13llm_15lamda.md\":\"9662b3db\",\"articles_algorithm_10白板推导系列_18贝叶斯线性回归.md\":\"93509b41\",\"articles_algorithm_12机器学习笔记_02梯度消失.md\":\"629ea8c1\",\"articles_algorithm_12机器学习笔记_03逻辑回归.md\":\"3ea2e8bf\",\"articles_algorithm_12机器学习笔记_05adam.md\":\"6fc614d2\",\"articles_algorithm_12机器学习笔记_11过拟合问题.md\":\"1b068c55\",\"articles_algorithm_12机器学习笔记_13adaboost.md\":\"e052cc8e\",\"articles_algorithm_13llm_16instructgpt.md\":\"9cdd551d\",\"articles_algorithm_13llm_16opt.md\":\"9d355205\",\"articles_algorithm_13llm_17gpt-neox-20b.md\":\"8ac0fa4d\",\"articles_algorithm_13llm_22opt-iml.md\":\"9340804b\",\"articles_algorithm_13llm_22mt0.md\":\"da08f63e\",\"articles_algorithm_13llm_23gpt4.md\":\"f5856165\",\"articles_algorithm_13llm_23llama.md\":\"bf1d4734\",\"articles_algorithm_13llm_25stanford alpaca.md\":\"10edecc5\",\"articles_algorithm_13llm_25vicuna.md\":\"fc6ee813\",\"articles_algorithm_13llm_26llama2.md\":\"7bfdfec1\",\"articles_algorithm_13llm_39qwen.md\":\"b76aa8fe\",\"articles_algorithm_13llm_44mistral-moe.md\":\"0d4785a7\",\"articles_algorithm_13llm_45.md\":\"a8560241\",\"articles_algorithm_13llm_index.md\":\"e0020df1\",\"articles_algorithm_14lmm_01dall-e.md\":\"8325dfd6\",\"articles_algorithm_14lmm_03目标检测rcnn_yolo_ssd.md\":\"167e6fe2\",\"articles_algorithm_14lmm_04blip_cogvlm_llava_qwenvl.md\":\"0384b60f\",\"articles_algorithm_14lmm_05ipadaptor_photomaker_instantid.md\":\"c73fc8e8\",\"articles_algorithm_14lmm_06vid2densepose_magicanimate.md\":\"236e67a5\",\"articles_algorithm_14lmm_index.md\":\"2730851a\",\"articles_algorithm_16rag_01langchain入门.md\":\"6dffa91a\",\"articles_algorithm_16rag_02工具总结.md\":\"b4a26916\",\"articles_algorithm_16rag_100基于大语言模型知识问答应用落地实践.md\":\"43385309\",\"articles_algorithm_16rag_101qanything开源rag项目.md\":\"46d52bff\",\"articles_algorithm_16rag_102提升rag性能综述.md\":\"54ab66df\",\"articles_algorithm_16rag_103嵌入模型介绍.md\":\"71b0913b\",\"articles_algorithm_16rag_index.md\":\"c5186700\",\"articles_algorithm_17agent_01babyagi.md\":\"dcdc0b56\",\"articles_algorithm_17agent_index.md\":\"05fe7f50\",\"articles_algorithm_18emodied_01rt2.md\":\"b39ddb31\",\"articles_algorithm_18emodied_index.md\":\"291e6872\",\"articles_algorithm_21模型部署_00调研模型工业化部署.md\":\"5f1fa797\",\"articles_algorithm_21模型部署_02fastapi.md\":\"77179108\",\"articles_algorithm_21模型部署_02gradio.md\":\"8a053f52\",\"articles_algorithm_21模型部署_04中兴llm加速.md\":\"9f1e4c09\",\"articles_algorithm_21模型部署_05推理加速总结.md\":\"ce7680d8\",\"articles_algorithm_21模型部署_index.md\":\"945f856d\",\"articles_algorithm_22模型训练和微调_01并行训练.md\":\"1816afc0\",\"articles_algorithm_22模型训练和微调_02模型微调.md\":\"a343f965\",\"articles_algorithm_22模型训练和微调_03chatglm微调学习.md\":\"e385334f\",\"articles_algorithm_22模型训练和微调_04deepseepd rlhf.md\":\"78fab5b2\",\"articles_algorithm_22模型训练和微调_10各种包如何用.md\":\"ed1c9aa1\",\"articles_algorithm_22模型训练和微调_index.md\":\"6a29de37\",\"articles_algorithm_99deeplab_01提取语音.md\":\"fcda4e9a\",\"articles_algorithm_99deeplab_02trafficclassification.md\":\"9a7813f5\",\"articles_algorithm_99deeplab_03vall-e_x.md\":\"6a396ed8\",\"articles_algorithm_99deeplab_index.md\":\"4e708434\",\"articles_blog_01指令.md\":\"3e35021a\",\"articles_blog_02huggingface入门.md\":\"f84c50bc\",\"articles_blog_04分词方法.md\":\"081c20f8\",\"articles_blog_05扩充词表.md\":\"47600ee3\",\"articles_blog_untitled.md\":\"b099848d\",\"articles_java_01java语法_02java8实战.md\":\"466ba27a\",\"articles_algorithm_13llm_26llama2中的技术.md\":\"7a23239f\",\"articles_algorithm_13llm_21flan-palm.md\":\"838412c1\",\"articles_algorithm_13llm_04学习分词技术subword.md\":\"9e10718a\",\"articles_algorithm_22模型训练和微调_11扩充词表.md\":\"9e319fe0\",\"articles_blog_index.md\":\"9250c279\",\"articles_algorithm_11强化学习_index.md\":\"61535b70\",\"articles_algorithm_13llm_21flan-t5.md\":\"c421e943\",\"articles_java_01java语法_10java读大数据量csv.md\":\"30452cef\",\"articles_java_01java语法_10java读大数据量excel.md\":\"1a64b300\",\"articles_algorithm_10白板推导系列_03线性回归.md\":\"e23a1c36\",\"articles_algorithm_13llm_00ai前期发展2016.md\":\"26249a16\",\"articles_algorithm_10白板推导系列_02高斯分布.md\":\"41ad3dcb\",\"articles_algorithm_13llm_22bloom.md\":\"00f28f91\",\"articles_algorithm_13llm_05gpt01论文.md\":\"586c5492\",\"articles_java_01java语法_10web前后端大文件上传和接收.md\":\"c9393c1d\",\"articles_algorithm_13llm_22chatgpt.md\":\"630f5044\",\"articles_algorithm_13llm_14t0.md\":\"c554e2f2\",\"articles_algorithm_13llm_12gpt3.md\":\"e57f36c0\",\"articles_algorithm_12机器学习笔记_04gbdt.md\":\"a2f54a46\",\"articles_algorithm_13llm_18ul2.md\":\"6c2b9028\",\"articles_algorithm_10白板推导系列_05降维.md\":\"33c05daa\",\"articles_java_01java语法_index.md\":\"7829dfc3\",\"articles_algorithm_10白板推导系列_24approinference.md\":\"813ae68b\",\"articles_algorithm_14lmm_01stablediffusion.md\":\"24a70893\",\"articles_algorithm_12机器学习笔记_14adboost.md\":\"8df17e03\",\"articles_algorithm_12机器学习笔记_10调参指南.md\":\"8cfbae47\",\"articles_algorithm_14lmm_03vae_ddpm_vit_dit.md\":\"31f265e8\",\"articles_algorithm_13llm_07roberta.md\":\"863b793e\",\"articles_algorithm_12机器学习笔记_12bagging和随机森林.md\":\"25cdf836\",\"articles_algorithm_01python语法_工具_02np.array()和np.mat()区别.md\":\"20f996fb\",\"articles_algorithm_21模型部署_12qwenllm.md\":\"e5e76ed7\",\"articles_algorithm_22模型训练和微调_00项目学习.md\":\"a303eabc\",\"articles_algorithm_13llm_40分词和扩充词表.md\":\"c252de17\",\"articles_algorithm_00数学基础_02概率论.md\":\"9954b3e6\",\"articles_algorithm_10白板推导系列_17高斯网络.md\":\"b629dbd5\",\"articles_algorithm_10白板推导系列_07指数族分布.md\":\"7e7e9f72\",\"articles_algorithm_10白板推导系列_15particlefilter粒子滤波.md\":\"7cb194f0\",\"articles_algorithm_10白板推导系列_04线性分类.md\":\"19875900\",\"articles_algorithm_10白板推导系列_14lds线性动态系统.md\":\"fb943bcc\",\"articles_algorithm_13llm_19palm.md\":\"8bc63126\",\"articles_algorithm_10白板推导系列_08概率图模型.md\":\"1a606efa\",\"articles_algorithm_10白板推导系列_20rbm.md\":\"1ee68cc8\",\"articles_java_01java语法_11java线程池的使用.md\":\"e5940b78\",\"articles_java_05maven_index.md\":\"0fc81f60\",\"articles_java_20mysql_index.md\":\"e283be17\",\"articles_java_22janusgraph_index.md\":\"82fee626\",\"articles_java_30设计模式_index.md\":\"2036be1e\",\"articles_java_31java并发编程_index.md\":\"29d2febb\",\"articles_java_51springboot_00springboot.md\":\"9446f56a\",\"articles_java_51springboot_01freemarker模板.md\":\"35b979d5\",\"articles_java_51springboot_02i18n国际化.md\":\"14b59b8c\",\"articles_java_51springboot_03日志slf4j.md\":\"03ae1162\",\"articles_java_51springboot_04缓存redis.md\":\"2503e11d\",\"articles_java_03javaweb基础tomcat servlet jsp_index.md\":\"a423dc3e\",\"articles_java_51springboot_05安全shiro.md\":\"b9311a8e\",\"articles_java_51springboot_05安全springsecurity.md\":\"830cd08b\",\"articles_java_51springboot_index.md\":\"197072b0\",\"articles_algorithm_10白板推导系列_12mcmc.md\":\"ceebe04f\",\"articles_java_51springboot_10读文件.md\":\"d839210a\",\"articles_java_51springboot_11日志.md\":\"9b4f3776\",\"articles_algorithm_10白板推导系列_16crf.md\":\"606f5a36\",\"articles_java_51springboot_07任务 之 异步 定时 邮件.md\":\"ee4a3800\",\"articles_java_51springboot_06分页pagehelper.md\":\"29b69393\",\"articles_java_01java语法_12java正则化.md\":\"d96fb728\",\"articles_algorithm_10白板推导系列_11vi变分推断.md\":\"7521e131\",\"articles_algorithm_10白板推导系列_21spectral谱聚类.md\":\"06fd562a\",\"articles_java_52springcloud_index.md\":\"f8f5dd71\",\"articles_java_53arthus_index.md\":\"cff324b3\",\"articles_java_60kafka_index.md\":\"c1e239b3\",\"articles_algorithm_10白板推导系列_10gmm.md\":\"16b4ef85\",\"articles_java_65vue_01vue3_ts语法.md\":\"0b446e12\",\"articles_algorithm_10白板推导系列_23partitionfunction.md\":\"5076e96d\",\"articles_java_65vue_02vue3实战.md\":\"7edccd0e\",\"articles_java_65vue_index.md\":\"41626696\",\"articles_java_66jenkins_index.md\":\"0874217a\",\"articles_algorithm_10白板推导系列_19高斯过程回归.md\":\"d1425ccf\",\"articles_ops_docker_index.md\":\"3df6eb09\",\"articles_ops_git_index.md\":\"0f8e763f\",\"articles_java_02html_css_js_xml_index.md\":\"94de74a0\",\"articles_windows_10idea.md\":\"9f3f4681\",\"articles_windows_11net.md\":\"1ec98538\",\"articles_ops_linux_04linux性能监控.md\":\"d53800ef\",\"articles_windows_14typora.md\":\"5403befa\",\"articles_windows_13tvbox.md\":\"6be49e05\",\"articles_windows_15miniconda_pycharm_cuda.md\":\"31b7e997\",\"articles_windows_16欧拉系统安装gpu驱动.md\":\"166785a3\",\"articles_windows_index.md\":\"1f3bd6a8\",\"index.md\":\"50f44e84\",\"articles_ops_linux_linux开发环境准备.md\":\"b613d8f2\",\"articles_ops_nginx_index.md\":\"b404ecb4\",\"articles_ops_k8s_index.md\":\"bb4b7c97\",\"articles_ops_linux_01linux教程.md\":\"417340bc\",\"articles_ops_linux_index.md\":\"8dd7462a\",\"articles_ops_k8s_k8s教程.md\":\"7510ec7f\",\"articles_python_01环境.md\":\"0f1ad157\",\"articles_java_61elasticsearch_index.md\":\"4386ad02\",\"articles_python_10python操作docx文件.md\":\"a4990092\",\"articles_java_65redis_index.md\":\"1a5188c6\",\"articles_python_11sqlalchemy.md\":\"64e60703\",\"articles_ops_linux_02shell脚本.md\":\"39572f23\",\"articles_ops_linux_02shell脚本之服务启停脚本.md\":\"a67f47dd\",\"articles_python_20fastapi什么时候用async.md\":\"d7802c92\",\"articles_ops_linux_03重要命令.md\":\"f2bfd1d2\",\"articles_python_21fastapi开发请求.md\":\"9b942595\",\"articles_python_index.md\":\"d83303df\",\"articles_ops_k8s_k8s安装文档.md\":\"26fd6f3a\",\"articles_windows_01常用软件.md\":\"c9842742\",\"articles_windows_02latex与文献检索.md\":\"7e7d3a86\",\"articles_vitepress_index.md\":\"eb84204e\",\"articles_algorithm_10白板推导系列_09em.md\":\"21c1515b\",\"articles_algorithm_10白板推导系列_13hmm.md\":\"61763bcd\"}");window.__VP_SITE_DATA__=JSON.parse("{\"lang\":\"en-US\",\"dir\":\"ltr\",\"title\":\"明日盈盈\",\"titleTemplate\":\"Make each day count, Make learning a habit.\",\"description\":\"一只程序猿\",\"base\":\"/\",\"head\":[],\"appearance\":true,\"themeConfig\":{\"siteTitle\":\"Home\",\"nav\":[{\"text\":\"Algorithm\",\"items\":[{\"text\":\"00数学基础\",\"link\":\"/articles/Algorithm/00数学基础/\"},{\"text\":\"01python语法&工具\",\"link\":\"/articles/Algorithm/01python语法&工具/\"},{\"text\":\"10白板推导系列\",\"link\":\"/articles/Algorithm/10白板推导系列/\"},{\"text\":\"11强化学习\",\"link\":\"/articles/Algorithm/11强化学习/\"},{\"text\":\"12机器学习笔记\",\"link\":\"/articles/Algorithm/12机器学习笔记/\"},{\"text\":\"13LLM\",\"link\":\"/articles/Algorithm/13LLM/\"},{\"text\":\"14LMM\",\"link\":\"/articles/Algorithm/14LMM/\"},{\"text\":\"16RAG\",\"link\":\"/articles/Algorithm/16RAG/\"},{\"text\":\"17Agent\",\"link\":\"/articles/Algorithm/17Agent/\"},{\"text\":\"18Emodied\",\"link\":\"/articles/Algorithm/18Emodied/\"},{\"text\":\"21模型部署\",\"link\":\"/articles/Algorithm/21模型部署/\"},{\"text\":\"22模型训练和微调\",\"link\":\"/articles/Algorithm/22模型训练和微调/\"},{\"text\":\"99deeplab\",\"link\":\"/articles/Algorithm/99deeplab/\"}]},{\"text\":\"Blog\",\"link\":\"/articles/Blog/\"},{\"text\":\"Java\",\"items\":[{\"text\":\"01Java语法\",\"link\":\"/articles/Java/01Java语法/\"},{\"text\":\"02HTML+CSS+JS+XML\",\"link\":\"/articles/Java/02HTML+CSS+JS+XML/\"},{\"text\":\"03JavaWeb基础tomcat servlet jsp\",\"link\":\"/articles/Java/03JavaWeb基础tomcat servlet jsp/\"},{\"text\":\"05Maven\",\"link\":\"/articles/Java/05Maven/\"},{\"text\":\"20mysql\",\"link\":\"/articles/Java/20mysql/\"},{\"text\":\"22JanusGraph\",\"link\":\"/articles/Java/22JanusGraph/\"},{\"text\":\"30设计模式\",\"link\":\"/articles/Java/30设计模式/\"},{\"text\":\"31Java并发编程\",\"link\":\"/articles/Java/31Java并发编程/\"},{\"text\":\"51SpringBoot\",\"link\":\"/articles/Java/51SpringBoot/\"},{\"text\":\"52SpringCloud\",\"link\":\"/articles/Java/52SpringCloud/\"},{\"text\":\"53Arthus\",\"link\":\"/articles/Java/53Arthus/\"},{\"text\":\"60kafka\",\"link\":\"/articles/Java/60kafka/\"},{\"text\":\"61ElasticSearch\",\"link\":\"/articles/Java/61ElasticSearch/\"},{\"text\":\"65redis\",\"link\":\"/articles/Java/65redis/\"},{\"text\":\"65vue\",\"link\":\"/articles/Java/65vue/\"},{\"text\":\"66jenkins\",\"link\":\"/articles/Java/66jenkins/\"}]},{\"text\":\"Ops\",\"items\":[{\"text\":\"Docker\",\"link\":\"/articles/Ops/Docker/\"},{\"text\":\"Git\",\"link\":\"/articles/Ops/Git/\"},{\"text\":\"Linux\",\"link\":\"/articles/Ops/Linux/\"},{\"text\":\"Nginx\",\"link\":\"/articles/Ops/Nginx/\"},{\"text\":\"k8s\",\"link\":\"/articles/Ops/k8s/\"}]},{\"text\":\"Python\",\"link\":\"/articles/Python/\"},{\"text\":\"VitePress\",\"link\":\"/articles/VitePress/\"},{\"text\":\"windows\",\"link\":\"/articles/windows/\"}],\"sidebar\":{\"/articles/Algorithm/00数学基础\":[{\"text\":\"00数学基础\",\"items\":[{\"text\":\"01线性代数的本质\",\"link\":\"/articles/Algorithm/00数学基础/01线性代数的本质.md\"},{\"text\":\"02概率论\",\"link\":\"/articles/Algorithm/00数学基础/02概率论.md\"},{\"text\":\"03统计学基础\",\"link\":\"/articles/Algorithm/00数学基础/03统计学基础.md\"}]}],\"/articles/Algorithm/01python语法&工具\":[{\"text\":\"01python语法&工具\",\"items\":[{\"text\":\"01python3语法\",\"link\":\"/articles/Algorithm/01python语法&工具/01python3语法.md\"},{\"text\":\"02np.array()和np.mat()区别\",\"link\":\"/articles/Algorithm/01python语法&工具/02np.array()和np.mat()区别.md\"},{\"text\":\"03python常用库\",\"link\":\"/articles/Algorithm/01python语法&工具/03python常用库.md\"}]}],\"/articles/Algorithm/10白板推导系列\":[{\"text\":\"10白板推导系列\",\"items\":[{\"text\":\"01介绍\",\"link\":\"/articles/Algorithm/10白板推导系列/01介绍.md\"},{\"text\":\"02高斯分布\",\"link\":\"/articles/Algorithm/10白板推导系列/02高斯分布.md\"},{\"text\":\"03线性回归\",\"link\":\"/articles/Algorithm/10白板推导系列/03线性回归.md\"},{\"text\":\"04线性分类\",\"link\":\"/articles/Algorithm/10白板推导系列/04线性分类.md\"},{\"text\":\"05降维\",\"link\":\"/articles/Algorithm/10白板推导系列/05降维.md\"},{\"text\":\"06支持向量机\",\"link\":\"/articles/Algorithm/10白板推导系列/06支持向量机.md\"},{\"text\":\"07指数族分布\",\"link\":\"/articles/Algorithm/10白板推导系列/07指数族分布.md\"},{\"text\":\"08概率图模型\",\"link\":\"/articles/Algorithm/10白板推导系列/08概率图模型.md\"},{\"text\":\"09EM\",\"link\":\"/articles/Algorithm/10白板推导系列/09EM.md\"},{\"text\":\"10GMM\",\"link\":\"/articles/Algorithm/10白板推导系列/10GMM.md\"},{\"text\":\"11VI变分推断\",\"link\":\"/articles/Algorithm/10白板推导系列/11VI变分推断.md\"},{\"text\":\"12MCMC\",\"link\":\"/articles/Algorithm/10白板推导系列/12MCMC.md\"},{\"text\":\"13HMM\",\"link\":\"/articles/Algorithm/10白板推导系列/13HMM.md\"},{\"text\":\"14LDS线性动态系统\",\"link\":\"/articles/Algorithm/10白板推导系列/14LDS线性动态系统.md\"},{\"text\":\"15particleFilter粒子滤波\",\"link\":\"/articles/Algorithm/10白板推导系列/15particleFilter粒子滤波.md\"},{\"text\":\"16CRF\",\"link\":\"/articles/Algorithm/10白板推导系列/16CRF.md\"},{\"text\":\"17高斯网络\",\"link\":\"/articles/Algorithm/10白板推导系列/17高斯网络.md\"},{\"text\":\"18贝叶斯线性回归\",\"link\":\"/articles/Algorithm/10白板推导系列/18贝叶斯线性回归.md\"},{\"text\":\"19高斯过程回归\",\"link\":\"/articles/Algorithm/10白板推导系列/19高斯过程回归.md\"},{\"text\":\"20RBM\",\"link\":\"/articles/Algorithm/10白板推导系列/20RBM.md\"},{\"text\":\"21Spectral谱聚类\",\"link\":\"/articles/Algorithm/10白板推导系列/21Spectral谱聚类.md\"},{\"text\":\"22NN\",\"link\":\"/articles/Algorithm/10白板推导系列/22NN.md\"},{\"text\":\"23PartitionFunction\",\"link\":\"/articles/Algorithm/10白板推导系列/23PartitionFunction.md\"},{\"text\":\"24ApproInference\",\"link\":\"/articles/Algorithm/10白板推导系列/24ApproInference.md\"}]}],\"/articles/Algorithm/11强化学习\":[{\"text\":\"11强化学习\",\"items\":[{\"text\":\"01强化学习环境\",\"link\":\"/articles/Algorithm/11强化学习/01强化学习环境.md\"},{\"text\":\"01深度强化学习-李宏毅\",\"link\":\"/articles/Algorithm/11强化学习/01深度强化学习-李宏毅.md\"}]}],\"/articles/Algorithm/12机器学习笔记\":[{\"text\":\"12机器学习笔记\",\"items\":[{\"text\":\"00算法性能度量\",\"link\":\"/articles/Algorithm/12机器学习笔记/00算法性能度量.md\"},{\"text\":\"01概述\",\"link\":\"/articles/Algorithm/12机器学习笔记/01概述.md\"},{\"text\":\"02梯度消失\",\"link\":\"/articles/Algorithm/12机器学习笔记/02梯度消失.md\"},{\"text\":\"03逻辑回归\",\"link\":\"/articles/Algorithm/12机器学习笔记/03逻辑回归.md\"},{\"text\":\"04GBDT\",\"link\":\"/articles/Algorithm/12机器学习笔记/04GBDT.md\"},{\"text\":\"05Adam\",\"link\":\"/articles/Algorithm/12机器学习笔记/05Adam.md\"},{\"text\":\"10调参指南\",\"link\":\"/articles/Algorithm/12机器学习笔记/10调参指南.md\"},{\"text\":\"11过拟合问题\",\"link\":\"/articles/Algorithm/12机器学习笔记/11过拟合问题.md\"},{\"text\":\"12bagging和随机森林\",\"link\":\"/articles/Algorithm/12机器学习笔记/12bagging和随机森林.md\"},{\"text\":\"13Adaboost\",\"link\":\"/articles/Algorithm/12机器学习笔记/13Adaboost.md\"},{\"text\":\"14Adboost\",\"link\":\"/articles/Algorithm/12机器学习笔记/14Adboost.md\"},{\"text\":\"14LightGBM\",\"link\":\"/articles/Algorithm/12机器学习笔记/14LightGBM.md\"},{\"text\":\"15XGBoost\",\"link\":\"/articles/Algorithm/12机器学习笔记/15XGBoost.md\"},{\"text\":\"16Lightgbm\",\"link\":\"/articles/Algorithm/12机器学习笔记/16Lightgbm.md\"},{\"text\":\"17catboost\",\"link\":\"/articles/Algorithm/12机器学习笔记/17catboost.md\"}]}],\"/articles/Algorithm/13LLM\":[{\"text\":\"13LLM\",\"items\":[{\"text\":\"00AI前期发展2016\",\"link\":\"/articles/Algorithm/13LLM/00AI前期发展2016.md\"},{\"text\":\"00LLM发展2017-2013\",\"link\":\"/articles/Algorithm/13LLM/00LLM发展2017-2013.md\"},{\"text\":\"00多模态\",\"link\":\"/articles/Algorithm/13LLM/00多模态.md\"},{\"text\":\"00综述\",\"link\":\"/articles/Algorithm/13LLM/00综述.md\"},{\"text\":\"01PrtNet\",\"link\":\"/articles/Algorithm/13LLM/01PrtNet.md\"},{\"text\":\"01seq2seq和Attention\",\"link\":\"/articles/Algorithm/13LLM/01seq2seq和Attention.md\"},{\"text\":\"02word2Vec\",\"link\":\"/articles/Algorithm/13LLM/02word2Vec.md\"},{\"text\":\"03TransformerXL\",\"link\":\"/articles/Algorithm/13LLM/03TransformerXL.md\"},{\"text\":\"03Transformer详解\",\"link\":\"/articles/Algorithm/13LLM/03Transformer详解.md\"},{\"text\":\"04bert1进化史\",\"link\":\"/articles/Algorithm/13LLM/04bert1进化史.md\"},{\"text\":\"04bert2详解\",\"link\":\"/articles/Algorithm/13LLM/04bert2详解.md\"},{\"text\":\"04学习分词技术Subword\",\"link\":\"/articles/Algorithm/13LLM/04学习分词技术Subword.md\"},{\"text\":\"05GPT01论文\",\"link\":\"/articles/Algorithm/13LLM/05GPT01论文.md\"},{\"text\":\"05GPT02模型\",\"link\":\"/articles/Algorithm/13LLM/05GPT02模型.md\"},{\"text\":\"06MTDNN\",\"link\":\"/articles/Algorithm/13LLM/06MTDNN.md\"},{\"text\":\"06XLNet\",\"link\":\"/articles/Algorithm/13LLM/06XLNet.md\"},{\"text\":\"07RoBERTa\",\"link\":\"/articles/Algorithm/13LLM/07RoBERTa.md\"},{\"text\":\"08ALBERT\",\"link\":\"/articles/Algorithm/13LLM/08ALBERT.md\"},{\"text\":\"09ELECTRA\",\"link\":\"/articles/Algorithm/13LLM/09ELECTRA.md\"},{\"text\":\"10T5\",\"link\":\"/articles/Algorithm/13LLM/10T5.md\"},{\"text\":\"11T5\",\"link\":\"/articles/Algorithm/13LLM/11T5.md\"},{\"text\":\"11学习attention mask\",\"link\":\"/articles/Algorithm/13LLM/11学习attention mask.md\"},{\"text\":\"12GPT3\",\"link\":\"/articles/Algorithm/13LLM/12GPT3.md\"},{\"text\":\"13FLAN\",\"link\":\"/articles/Algorithm/13LLM/13FLAN.md\"},{\"text\":\"14T0\",\"link\":\"/articles/Algorithm/13LLM/14T0.md\"},{\"text\":\"15LaMDA\",\"link\":\"/articles/Algorithm/13LLM/15LaMDA.md\"},{\"text\":\"16InstructGPT\",\"link\":\"/articles/Algorithm/13LLM/16InstructGPT.md\"},{\"text\":\"16OPT\",\"link\":\"/articles/Algorithm/13LLM/16OPT.md\"},{\"text\":\"17GPT-NeoX-20B\",\"link\":\"/articles/Algorithm/13LLM/17GPT-NeoX-20B.md\"},{\"text\":\"18UL2\",\"link\":\"/articles/Algorithm/13LLM/18UL2.md\"},{\"text\":\"19PaLM\",\"link\":\"/articles/Algorithm/13LLM/19PaLM.md\"},{\"text\":\"21Flan-PaLM\",\"link\":\"/articles/Algorithm/13LLM/21Flan-PaLM.md\"},{\"text\":\"21Flan-T5\",\"link\":\"/articles/Algorithm/13LLM/21Flan-T5.md\"},{\"text\":\"22BLOOM\",\"link\":\"/articles/Algorithm/13LLM/22BLOOM.md\"},{\"text\":\"22ChatGPT\",\"link\":\"/articles/Algorithm/13LLM/22ChatGPT.md\"},{\"text\":\"22OPT-IML\",\"link\":\"/articles/Algorithm/13LLM/22OPT-IML.md\"},{\"text\":\"22mT0\",\"link\":\"/articles/Algorithm/13LLM/22mT0.md\"},{\"text\":\"23GPT4\",\"link\":\"/articles/Algorithm/13LLM/23GPT4.md\"},{\"text\":\"23LLaMA\",\"link\":\"/articles/Algorithm/13LLM/23LLaMA.md\"},{\"text\":\"25Stanford Alpaca\",\"link\":\"/articles/Algorithm/13LLM/25Stanford Alpaca.md\"},{\"text\":\"25Vicuna\",\"link\":\"/articles/Algorithm/13LLM/25Vicuna.md\"},{\"text\":\"26LLaMA2\",\"link\":\"/articles/Algorithm/13LLM/26LLaMA2.md\"},{\"text\":\"26LLaMA2中的技术\",\"link\":\"/articles/Algorithm/13LLM/26LLaMA2中的技术.md\"},{\"text\":\"39Qwen\",\"link\":\"/articles/Algorithm/13LLM/39Qwen.md\"},{\"text\":\"40分词和扩充词表\",\"link\":\"/articles/Algorithm/13LLM/40分词和扩充词表.md\"},{\"text\":\"44Mistral-MoE\",\"link\":\"/articles/Algorithm/13LLM/44Mistral-MoE.md\"},{\"text\":\"45\",\"link\":\"/articles/Algorithm/13LLM/45.md\"}]}],\"/articles/Algorithm/14LMM\":[{\"text\":\"14LMM\",\"items\":[{\"text\":\"01DALL-E\",\"link\":\"/articles/Algorithm/14LMM/01DALL-E.md\"},{\"text\":\"01StableDiffusion\",\"link\":\"/articles/Algorithm/14LMM/01StableDiffusion.md\"},{\"text\":\"03VAE_DDPM_ViT_DiT\",\"link\":\"/articles/Algorithm/14LMM/03VAE_DDPM_ViT_DiT.md\"},{\"text\":\"03目标检测RCNN_YOLO_SSD\",\"link\":\"/articles/Algorithm/14LMM/03目标检测RCNN_YOLO_SSD.md\"},{\"text\":\"04BLIP_CogVLM_LLaVA_QwenVL\",\"link\":\"/articles/Algorithm/14LMM/04BLIP_CogVLM_LLaVA_QwenVL.md\"},{\"text\":\"05IPAdaptor_PhotoMaker_InstantID\",\"link\":\"/articles/Algorithm/14LMM/05IPAdaptor_PhotoMaker_InstantID.md\"},{\"text\":\"06Vid2DensePose_MagicAnimate\",\"link\":\"/articles/Algorithm/14LMM/06Vid2DensePose_MagicAnimate.md\"}]}],\"/articles/Algorithm/16RAG\":[{\"text\":\"16RAG\",\"items\":[{\"text\":\"01LangChain入门\",\"link\":\"/articles/Algorithm/16RAG/01LangChain入门.md\"},{\"text\":\"02工具总结\",\"link\":\"/articles/Algorithm/16RAG/02工具总结.md\"},{\"text\":\"100基于大语言模型知识问答应用落地实践\",\"link\":\"/articles/Algorithm/16RAG/100基于大语言模型知识问答应用落地实践.md\"},{\"text\":\"101QAnything开源RAG项目\",\"link\":\"/articles/Algorithm/16RAG/101QAnything开源RAG项目.md\"},{\"text\":\"102提升RAG性能综述\",\"link\":\"/articles/Algorithm/16RAG/102提升RAG性能综述.md\"},{\"text\":\"103嵌入模型介绍\",\"link\":\"/articles/Algorithm/16RAG/103嵌入模型介绍.md\"}]}],\"/articles/Algorithm/17Agent\":[{\"text\":\"17Agent\",\"items\":[{\"text\":\"01babyAGI\",\"link\":\"/articles/Algorithm/17Agent/01babyAGI.md\"}]}],\"/articles/Algorithm/18Emodied\":[{\"text\":\"18Emodied\",\"items\":[{\"text\":\"01RT2\",\"link\":\"/articles/Algorithm/18Emodied/01RT2.md\"}]}],\"/articles/Algorithm/21模型部署\":[{\"text\":\"21模型部署\",\"items\":[{\"text\":\"00调研模型工业化部署\",\"link\":\"/articles/Algorithm/21模型部署/00调研模型工业化部署.md\"},{\"text\":\"02FastApi\",\"link\":\"/articles/Algorithm/21模型部署/02FastApi.md\"},{\"text\":\"02Gradio\",\"link\":\"/articles/Algorithm/21模型部署/02Gradio.md\"},{\"text\":\"04中兴LLM加速\",\"link\":\"/articles/Algorithm/21模型部署/04中兴LLM加速.md\"},{\"text\":\"05推理加速总结\",\"link\":\"/articles/Algorithm/21模型部署/05推理加速总结.md\"},{\"text\":\"12QwenLLM\",\"link\":\"/articles/Algorithm/21模型部署/12QwenLLM.md\"}]}],\"/articles/Algorithm/22模型训练和微调\":[{\"text\":\"22模型训练和微调\",\"items\":[{\"text\":\"00项目学习\",\"link\":\"/articles/Algorithm/22模型训练和微调/00项目学习.md\"},{\"text\":\"01并行训练\",\"link\":\"/articles/Algorithm/22模型训练和微调/01并行训练.md\"},{\"text\":\"02模型微调\",\"link\":\"/articles/Algorithm/22模型训练和微调/02模型微调.md\"},{\"text\":\"03ChatGLM微调学习\",\"link\":\"/articles/Algorithm/22模型训练和微调/03ChatGLM微调学习.md\"},{\"text\":\"04DeepSeepd RLHF\",\"link\":\"/articles/Algorithm/22模型训练和微调/04DeepSeepd RLHF.md\"},{\"text\":\"10各种包如何用\",\"link\":\"/articles/Algorithm/22模型训练和微调/10各种包如何用.md\"},{\"text\":\"11扩充词表\",\"link\":\"/articles/Algorithm/22模型训练和微调/11扩充词表.md\"}]}],\"/articles/Algorithm/99deeplab\":[{\"text\":\"99deeplab\",\"items\":[{\"text\":\"01提取语音\",\"link\":\"/articles/Algorithm/99deeplab/01提取语音.md\"},{\"text\":\"02TrafficClassification\",\"link\":\"/articles/Algorithm/99deeplab/02TrafficClassification.md\"},{\"text\":\"03VALL-E_X\",\"link\":\"/articles/Algorithm/99deeplab/03VALL-E_X.md\"}]}],\"/articles/Blog\":[{\"text\":\"Blog\",\"items\":[{\"text\":\"01指令\",\"link\":\"/articles/Blog/01指令.md\"},{\"text\":\"02HuggingFace入门\",\"link\":\"/articles/Blog/02HuggingFace入门.md\"},{\"text\":\"04分词方法\",\"link\":\"/articles/Blog/04分词方法.md\"},{\"text\":\"05扩充词表\",\"link\":\"/articles/Blog/05扩充词表.md\"},{\"text\":\"Untitled\",\"link\":\"/articles/Blog/Untitled.md\"}]}],\"/articles/Java/01Java语法\":[{\"text\":\"01Java语法\",\"items\":[{\"text\":\"02Java8实战\",\"link\":\"/articles/Java/01Java语法/02Java8实战.md\"},{\"text\":\"10Java读大数据量csv\",\"link\":\"/articles/Java/01Java语法/10Java读大数据量csv.md\"},{\"text\":\"10Java读大数据量excel\",\"link\":\"/articles/Java/01Java语法/10Java读大数据量excel.md\"},{\"text\":\"10Web前后端大文件上传和接收\",\"link\":\"/articles/Java/01Java语法/10Web前后端大文件上传和接收.md\"},{\"text\":\"11Java线程池的使用\",\"link\":\"/articles/Java/01Java语法/11Java线程池的使用.md\"},{\"text\":\"12Java正则化\",\"link\":\"/articles/Java/01Java语法/12Java正则化.md\"}]}],\"/articles/Java/51SpringBoot\":[{\"text\":\"51SpringBoot\",\"items\":[{\"text\":\"00SpringBoot\",\"link\":\"/articles/Java/51SpringBoot/00SpringBoot.md\"},{\"text\":\"01Freemarker模板\",\"link\":\"/articles/Java/51SpringBoot/01Freemarker模板.md\"},{\"text\":\"02i18n国际化\",\"link\":\"/articles/Java/51SpringBoot/02i18n国际化.md\"},{\"text\":\"03日志SLF4J\",\"link\":\"/articles/Java/51SpringBoot/03日志SLF4J.md\"},{\"text\":\"04缓存Redis\",\"link\":\"/articles/Java/51SpringBoot/04缓存Redis.md\"},{\"text\":\"05安全Shiro\",\"link\":\"/articles/Java/51SpringBoot/05安全Shiro.md\"},{\"text\":\"05安全SpringSecurity\",\"link\":\"/articles/Java/51SpringBoot/05安全SpringSecurity.md\"},{\"text\":\"06分页PageHelper\",\"link\":\"/articles/Java/51SpringBoot/06分页PageHelper.md\"},{\"text\":\"07任务 之 异步 定时 邮件\",\"link\":\"/articles/Java/51SpringBoot/07任务 之 异步 定时 邮件.md\"},{\"text\":\"10读文件\",\"link\":\"/articles/Java/51SpringBoot/10读文件.md\"},{\"text\":\"11日志\",\"link\":\"/articles/Java/51SpringBoot/11日志.md\"}]}],\"/articles/Java/65vue\":[{\"text\":\"65vue\",\"items\":[{\"text\":\"01vue3+ts语法\",\"link\":\"/articles/Java/65vue/01vue3+ts语法.md\"},{\"text\":\"02vue3实战\",\"link\":\"/articles/Java/65vue/02vue3实战.md\"}]}],\"/articles/Ops/Linux\":[{\"text\":\"Linux\",\"items\":[{\"text\":\"01linux教程\",\"link\":\"/articles/Ops/Linux/01linux教程.md\"},{\"text\":\"02Shell脚本\",\"link\":\"/articles/Ops/Linux/02Shell脚本.md\"},{\"text\":\"02Shell脚本之服务启停脚本\",\"link\":\"/articles/Ops/Linux/02Shell脚本之服务启停脚本.md\"},{\"text\":\"03重要命令\",\"link\":\"/articles/Ops/Linux/03重要命令.md\"},{\"text\":\"04Linux性能监控\",\"link\":\"/articles/Ops/Linux/04Linux性能监控.md\"},{\"text\":\"linux开发环境准备\",\"link\":\"/articles/Ops/Linux/linux开发环境准备.md\"}]}],\"/articles/Ops/k8s\":[{\"text\":\"k8s\",\"items\":[{\"text\":\"k8s安装文档\",\"link\":\"/articles/Ops/k8s/k8s安装文档.md\"},{\"text\":\"k8s教程\",\"link\":\"/articles/Ops/k8s/k8s教程.md\"}]}],\"/articles/Python\":[{\"text\":\"Python\",\"items\":[{\"text\":\"01环境\",\"link\":\"/articles/Python/01环境.md\"},{\"text\":\"10python操作docx文件\",\"link\":\"/articles/Python/10python操作docx文件.md\"},{\"text\":\"11SQLAlchemy\",\"link\":\"/articles/Python/11SQLAlchemy.md\"},{\"text\":\"20FastAPI什么时候用async\",\"link\":\"/articles/Python/20FastAPI什么时候用async.md\"},{\"text\":\"21FastAPI开发请求\",\"link\":\"/articles/Python/21FastAPI开发请求.md\"}]}],\"/articles/windows\":[{\"text\":\"windows\",\"items\":[{\"text\":\"01常用软件\",\"link\":\"/articles/windows/01常用软件.md\"},{\"text\":\"02Latex与文献检索\",\"link\":\"/articles/windows/02Latex与文献检索.md\"},{\"text\":\"10IDEA\",\"link\":\"/articles/windows/10IDEA.md\"},{\"text\":\"11net\",\"link\":\"/articles/windows/11net.md\"},{\"text\":\"13tvbox\",\"link\":\"/articles/windows/13tvbox.md\"},{\"text\":\"14Typora\",\"link\":\"/articles/windows/14Typora.md\"},{\"text\":\"15miniconda+pycharm+CUDA\",\"link\":\"/articles/windows/15miniconda+pycharm+CUDA.md\"},{\"text\":\"16欧拉系统安装GPU驱动\",\"link\":\"/articles/windows/16欧拉系统安装GPU驱动.md\"}]}]},\"outline\":{\"level\":[1,3]},\"editLink\":{\"pattern\":\"https://github.com/mingriyingying/mingriyingying.github.io/blob/master/docs/:path\",\"text\":\"Edit this page on GitHub\"},\"lastUpdated\":{\"text\":\"Updated at\",\"formatOptions\":{\"dateStyle\":\"short\",\"timeStyle\":\"medium\"}},\"docFooter\":{\"prev\":false,\"next\":false}},\"locales\":{},\"scrollOffset\":90,\"cleanUrls\":false}");</script>
    
  </body>
</html>